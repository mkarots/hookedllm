{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"HookedLLM","text":"<p>Async-first, scoped hook system for LLM observability with SOLID/DI architecture</p> <p>HookedLLM provides transparent observability for LLM calls through a powerful hook system. Add evaluation, logging, metrics, and custom behaviors to your LLM applications without modifying core application logic.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Config or Code: Define hooks programmatically or via YAML</li> <li>Scoped Isolation: Named scopes prevent hook interference across application contexts</li> <li>Conditional Execution: Run hooks only when rules match (model, tags, metadata)</li> <li>Async-First: Built for modern async LLM SDKs</li> <li>Resilient: Hook failures never break your LLM calls</li> <li>Type-Safe: Full type hints and IDE autocomplete support</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import hookedllm\nfrom openai import AsyncOpenAI\n\n# Define a simple hook\nasync def log_usage(call_input, call_output, context):\n    print(f\"Model: {call_input.model}\")\n    print(f\"Tokens: {call_output.usage.get('total_tokens', 0)}\")\n\n# Register hook to a scope\nhookedllm.scope(\"evaluation\").after(log_usage)\n\n# Wrap your client with the scope\nclient = hookedllm.wrap(AsyncOpenAI(), scope=\"evaluation\")\n\n# Use normally - hooks execute automatically!\nresponse = await client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code># Core package (zero dependencies)\npip install hookedllm\n\n# With OpenAI support\npip install hookedllm[openai]\n\n# With all optional dependencies\npip install hookedllm[all]\n</code></pre>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li>Quick Start Guide - Get up and running in minutes</li> <li>Advanced Setup Guide - Complete setup with logging, error handling, and metrics</li> <li>Core Concepts - Understand scopes, hooks, and rules</li> <li>Examples - See real-world usage patterns</li> <li>Advanced Guides - Performance, observability, and more</li> <li>API Reference - Complete API documentation</li> <li>Architecture Guide - Deep dive into design</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE file for details.</p>"},{"location":"advanced-setup/","title":"Advanced Setup Guide","text":"<p>Complete setup with logging, error handling, and metrics.</p>"},{"location":"advanced-setup/#installation","title":"Installation","text":"<pre><code>pip install hookedllm[openai]\n# Or: pip install hookedllm[all]\n</code></pre>"},{"location":"advanced-setup/#setup","title":"Setup","text":""},{"location":"advanced-setup/#1-environment-configuration","title":"1. Environment Configuration","text":"<pre><code>import os\nfrom openai import AsyncOpenAI\nimport hookedllm\n\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"OPENAI_API_KEY environment variable not set\")\n\nclient = hookedllm.wrap(\n    AsyncOpenAI(api_key=api_key),\n    scope=os.getenv(\"HOOKEDLLM_SCOPE\", \"default\")\n)\n</code></pre>"},{"location":"advanced-setup/#2-structured-logging","title":"2. Structured Logging","text":"<pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\nasync def logger_hook(call_input, call_output, context):\n    logger.info(\"llm_call_completed\", extra={\n        \"call_id\": context.call_id,\n        \"model\": call_input.model,\n        \"tokens\": call_output.usage.get(\"total_tokens\", 0) if call_output.usage else 0,\n        \"duration_ms\": context.elapsed_ms,\n    })\n\nhookedllm.scope(\"default\").after(logger_hook)\n</code></pre>"},{"location":"advanced-setup/#3-error-handling","title":"3. Error Handling","text":"<pre><code>async def error_handler(call_input, error, context):\n    logger.error(\"llm_call_failed\", extra={\n        \"call_id\": context.call_id,\n        \"model\": call_input.model,\n        \"error_type\": type(error).__name__,\n    }, exc_info=True)\n\nhookedllm.scope(\"default\").error(error_handler)\n</code></pre>"},{"location":"advanced-setup/#4-metrics","title":"4. Metrics","text":"<pre><code>from prometheus_client import Counter, Histogram\n\nllm_calls_total = Counter(\"llm_calls_total\", \"Total LLM calls\", [\"model\"])\nllm_call_duration = Histogram(\"llm_call_duration_seconds\", \"LLM call duration\", [\"model\"])\n\nasync def metrics_hook(result):\n    model = result.input.model\n    llm_calls_total.labels(model=model).inc()\n    if result.output:\n        llm_call_duration.labels(model=model).observe(result.elapsed_ms / 1000.0)\n\nhookedllm.finally_(metrics_hook)\n</code></pre>"},{"location":"advanced-setup/#application-structure","title":"Application Structure","text":"<pre><code>your_app/\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 hooks/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py      # Register hooks\n\u2502   \u2502   \u251c\u2500\u2500 logging.py       # Logging hooks\n\u2502   \u2502   \u2514\u2500\u2500 metrics.py       # Metrics hooks\n\u2502   \u2514\u2500\u2500 llm.py               # LLM client setup\n\u2514\u2500\u2500 main.py                  # Application entry point\n</code></pre>"},{"location":"advanced-setup/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>OPENAI_API_KEY</code>: OpenAI API key (required)</li> <li><code>HOOKEDLLM_SCOPE</code>: Default scope (default: \"default\")</li> <li><code>LOG_LEVEL</code>: Logging level (default: \"INFO\")</li> </ul> <p>See examples/advanced.md for complete examples.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to HookedLLM!</p> <p>Please see the Contributing Guidelines for detailed information on how to contribute.</p>"},{"location":"contributing/#quick-links","title":"Quick Links","text":"<ul> <li>Code of Conduct</li> <li>Security Policy</li> <li>Architecture Guide</li> </ul>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Fork the repository</li> <li>Clone your fork</li> <li>Install in development mode:    <pre><code>pip install -e \".[dev]\"\n</code></pre></li> <li>Run tests:    <pre><code>pytest\n</code></pre></li> </ol>"},{"location":"contributing/#code-standards","title":"Code Standards","text":"<ul> <li>Use type hints throughout</li> <li>Follow PEP 8 (enforced by <code>black</code> and <code>ruff</code>)</li> <li>Write docstrings for public APIs</li> <li>Keep functions focused (Single Responsibility)</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Write tests for new functionality</li> <li>Ensure all tests pass</li> <li>Maintain or improve code coverage</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Update README for user-facing changes</li> <li>Add docstrings to new functions/classes</li> <li>Update examples if applicable</li> </ul>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>Open an issue for questions or discussions!</p>"},{"location":"hooks/","title":"Hooks","text":"<p>Hooks are functions that execute at specific points in the LLM call lifecycle. HookedLLM supports four hook types: <code>before</code>, <code>after</code>, <code>error</code>, and <code>finally</code>.</p>"},{"location":"hooks/#hook-types","title":"Hook Types","text":""},{"location":"hooks/#before-hooks","title":"Before Hooks","text":"<p>Run before the LLM call executes. Use them to: - Modify the call input - Add metadata to the context - Validate inputs - Set up tracking</p> <pre><code>async def before_hook(call_input, context):\n    \"\"\"Run before LLM call.\"\"\"\n    # Add user ID to metadata\n    context.metadata[\"user_id\"] = get_current_user_id()\n\n    # Modify call parameters\n    call_input.params[\"temperature\"] = 0.7\n\n    # Log the request\n    logger.info(f\"Starting call {context.call_id}\")\n\nhookedllm.before(before_hook)\n</code></pre>"},{"location":"hooks/#after-hooks","title":"After Hooks","text":"<p>Run after a successful LLM call. Use them to: - Process the response - Log results - Evaluate quality - Store metrics</p> <pre><code>async def after_hook(call_input, call_output, context):\n    \"\"\"Run after successful LLM call.\"\"\"\n    # Log the response\n    print(f\"Response: {call_output.text}\")\n\n    # Track token usage\n    tokens = call_output.usage.get(\"total_tokens\", 0)\n    metrics.record_tokens(tokens)\n\n    # Store evaluation\n    context.metadata[\"response_length\"] = len(call_output.text)\n\nhookedllm.after(after_hook)\n</code></pre>"},{"location":"hooks/#error-hooks","title":"Error Hooks","text":"<p>Run when an error occurs during the LLM call. Use them to: - Log errors - Send alerts - Track error rates - Handle failures gracefully</p> <pre><code>async def error_hook(call_input, error, context):\n    \"\"\"Run when LLM call fails.\"\"\"\n    # Log the error\n    logger.error(f\"Call {context.call_id} failed: {error}\")\n\n    # Send alert for critical errors\n    if isinstance(error, RateLimitError):\n        alerting.send_alert(\"Rate limit exceeded\")\n\n    # Track error metrics\n    metrics.record_error(type(error).__name__)\n\nhookedllm.error(error_hook)\n</code></pre>"},{"location":"hooks/#finally-hooks","title":"Finally Hooks","text":"<p>Run always, regardless of success or failure. Use them to: - Track metrics - Clean up resources - Log completion - Record timing</p> <pre><code>async def finally_hook(result):\n    \"\"\"Run after call completes (success or failure).\"\"\"\n    # Track timing\n    metrics.record_duration(result.elapsed_ms)\n\n    # Log completion\n    logger.info(f\"Call {result.context.call_id} completed in {result.elapsed_ms}ms\")\n\n    # Clean up\n    cleanup_resources(result.context.call_id)\n\nhookedllm.finally_(finally_hook)\n</code></pre>"},{"location":"hooks/#hook-execution-order","title":"Hook Execution Order","text":"<p>For a single LLM call, hooks execute in this order:</p> <ol> <li>Before hooks (in registration order)</li> <li>LLM call executes</li> <li>After hooks (if successful) OR Error hooks (if failed)</li> <li>Finally hooks (always)</li> </ol> <pre><code># Registration order matters within each hook type\nhookedllm.before(hook1)  # Runs first\nhookedllm.before(hook2)  # Runs second\n\nhookedllm.after(hook3)   # Runs first (if successful)\nhookedllm.after(hook4)   # Runs second (if successful)\n</code></pre>"},{"location":"hooks/#hook-signatures","title":"Hook Signatures","text":"<p>Each hook type has a specific signature:</p> <pre><code># Before hook\nasync def before_hook(call_input: CallInput, context: CallContext) -&gt; None:\n    ...\n\n# After hook\nasync def after_hook(\n    call_input: CallInput,\n    call_output: CallOutput,\n    context: CallContext\n) -&gt; None:\n    ...\n\n# Error hook\nasync def error_hook(\n    call_input: CallInput,\n    error: BaseException,\n    context: CallContext\n) -&gt; None:\n    ...\n\n# Finally hook\nasync def finally_hook(result: CallResult) -&gt; None:\n    ...\n</code></pre>"},{"location":"hooks/#hook-failures","title":"Hook Failures","text":"<p>Hook failures never break your LLM calls. If a hook raises an exception, it's caught and logged, but the LLM call continues normally.</p> <pre><code>async def buggy_hook(call_input, call_output, context):\n    raise ValueError(\"Something went wrong!\")\n\n# This hook failure won't affect the LLM call\nhookedllm.after(buggy_hook)\n\n# The call still succeeds\nresponse = await client.chat.completions.create(...)\n</code></pre>"},{"location":"hooks/#examples","title":"Examples","text":"<p>See Basic Usage Examples for complete hook examples, or Advanced Examples for complex patterns.</p>"},{"location":"quick-start/","title":"Quick Start Guide","text":"<p>Get started with HookedLLM in minutes. This guide will walk you through installing and using HookedLLM to add observability to your LLM calls.</p>"},{"location":"quick-start/#installation","title":"Installation","text":"<p>Install HookedLLM using pip:</p> <pre><code># Core package (zero dependencies)\npip install hookedllm\n\n# With OpenAI support\npip install hookedllm[openai]\n\n# With all optional dependencies\npip install hookedllm[all]\n</code></pre>"},{"location":"quick-start/#your-first-hook","title":"Your First Hook","text":"<p>Let's create a simple hook that logs every LLM call:</p> <pre><code>import hookedllm\nfrom openai import AsyncOpenAI\n\n# Define a hook function\nasync def log_call(call_input, call_output, context):\n    \"\"\"Log information about each LLM call.\"\"\"\n    print(f\"Model: {call_input.model}\")\n    print(f\"Tokens used: {call_output.usage.get('total_tokens', 0)}\")\n    print(f\"Call ID: {context.call_id}\")\n\n# Register the hook globally (runs for all clients)\nhookedllm.after(log_call)\n\n# Wrap your OpenAI client\nclient = hookedllm.wrap(AsyncOpenAI(api_key=\"your-api-key\"))\n\n# Use the client normally - hooks execute automatically!\nresponse = await client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"quick-start/#using-scopes","title":"Using Scopes","text":"<p>Scopes let you isolate hooks to specific parts of your application:</p> <pre><code># Register hooks to different scopes\nhookedllm.scope(\"evaluation\").after(evaluate_response)\nhookedllm.scope(\"default\").after(log_to_default)\n\n# Create clients with specific scopes\neval_client = hookedllm.wrap(AsyncOpenAI(), scope=\"evaluation\")\ndefault_client = hookedllm.wrap(AsyncOpenAI(), scope=\"default\")\n\n# Each client only runs hooks from its scope\n</code></pre>"},{"location":"quick-start/#hook-types","title":"Hook Types","text":"<p>HookedLLM supports four hook types:</p> <pre><code># Before: runs before LLM call\nasync def before_hook(call_input, context):\n    context.metadata[\"user_id\"] = \"abc123\"\n\n# After: runs after successful call\nasync def after_hook(call_input, call_output, context):\n    print(f\"Response: {call_output.text}\")\n\n# Error: runs on failure\nasync def error_hook(call_input, error, context):\n    print(f\"Error: {error}\")\n\n# Finally: always runs with complete result\nasync def finally_hook(result):\n    print(f\"Took {result.elapsed_ms}ms\")\n\n# Register hooks\nhookedllm.before(before_hook)\nhookedllm.after(after_hook)\nhookedllm.error(error_hook)\nhookedllm.finally_(finally_hook)\n</code></pre>"},{"location":"quick-start/#conditional-hooks","title":"Conditional Hooks","text":"<p>Run hooks only when conditions match:</p> <pre><code># Only for GPT-4\nhookedllm.after(\n    expensive_eval,\n    when=hookedllm.when.model(\"gpt-4\")\n)\n\n# Only for tagged calls\nhookedllm.after(\n    logger_hook,\n    when=hookedllm.when.tag(\"monitoring\")\n)\n\n# Complex rules\nhookedllm.after(\n    my_hook,\n    when=(\n        hookedllm.when.model(\"gpt-4\") &amp;\n        hookedllm.when.tag(\"monitoring\") &amp;\n        ~hookedllm.when.tag(\"test\")\n    )\n)\n</code></pre>"},{"location":"quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Scopes for isolating hooks</li> <li>Explore Hook Types in detail</li> <li>See Complete Examples for full working examples</li> <li>Check out Advanced Setup Guide for complete setup with logging, error handling, and metrics</li> <li>Read Advanced Guides for testing and customization</li> </ul>"},{"location":"rules/","title":"Rules","text":"<p>Rules let you conditionally execute hooks based on call properties like model, tags, or metadata.</p>"},{"location":"rules/#basic-rules","title":"Basic Rules","text":""},{"location":"rules/#model-rules","title":"Model Rules","text":"<p>Run hooks only for specific models:</p> <pre><code># Only for GPT-4\nhookedllm.after(\n    expensive_eval,\n    when=hookedllm.when.model(\"gpt-4\")\n)\n\n# Only for GPT-4o-mini\nhookedllm.after(\n    quick_eval,\n    when=hookedllm.when.model(\"gpt-4o-mini\")\n)\n</code></pre>"},{"location":"rules/#tag-rules","title":"Tag Rules","text":"<p>Run hooks only when specific tags are present:</p> <pre><code># Only for tagged calls\nhookedllm.after(\n    logger_hook,\n    when=hookedllm.when.tag(\"monitoring\")\n)\n\n# Only for critical calls\nhookedllm.error(\n    alert_on_error,\n    when=hookedllm.when.tag(\"critical\")\n)\n</code></pre> <p>To use tags, pass them in <code>extra_body</code>:</p> <pre><code>response = await client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[...],\n    extra_body={\n        \"hookedllm_tags\": [\"monitoring\", \"critical\"]\n    }\n)\n</code></pre>"},{"location":"rules/#metadata-rules","title":"Metadata Rules","text":"<p>Run hooks based on metadata values:</p> <pre><code># Only for premium users\nhookedllm.after(\n    premium_hook,\n    when=lambda call_input, ctx: ctx.metadata.get(\"tier\") == \"premium\"\n)\n\n# Only for specific user IDs\nhookedllm.after(\n    user_tracking,\n    when=lambda call_input, ctx: ctx.metadata.get(\"user_id\") == \"abc123\"\n)\n</code></pre>"},{"location":"rules/#rule-composition","title":"Rule Composition","text":"<p>Combine rules with <code>&amp;</code> (AND), <code>|</code> (OR), and <code>~</code> (NOT):</p> <pre><code># GPT-4 AND monitoring tag\nhookedllm.after(\n    my_hook,\n    when=(\n        hookedllm.when.model(\"gpt-4\") &amp;\n        hookedllm.when.tag(\"monitoring\")\n    )\n)\n\n# Monitoring tag AND NOT test tag\nhookedllm.after(\n    monitoring_hook,\n    when=(\n        hookedllm.when.tag(\"monitoring\") &amp;\n        ~hookedllm.when.tag(\"test\")\n    )\n)\n\n# GPT-4 OR GPT-3.5\nhookedllm.after(\n    gpt_hook,\n    when=(\n        hookedllm.when.model(\"gpt-4\") |\n        hookedllm.when.model(\"gpt-3.5-turbo\")\n    )\n)\n\n# Complex: GPT-4 AND monitoring AND NOT test\nhookedllm.after(\n    complex_hook,\n    when=(\n        hookedllm.when.model(\"gpt-4\") &amp;\n        hookedllm.when.tag(\"monitoring\") &amp;\n        ~hookedllm.when.tag(\"test\")\n    )\n)\n</code></pre>"},{"location":"rules/#custom-predicates","title":"Custom Predicates","text":"<p>Create custom rules with lambda functions:</p> <pre><code># Custom rule: only for calls with &gt; 100 tokens\nhookedllm.after(\n    large_call_hook,\n    when=lambda call_input, ctx: len(str(call_input.messages)) &gt; 100\n)\n\n# Custom rule: only for specific models and premium users\nhookedllm.after(\n    premium_gpt4_hook,\n    when=lambda call_input, ctx: (\n        call_input.model == \"gpt-4\" and\n        ctx.metadata.get(\"tier\") == \"premium\"\n    )\n)\n</code></pre>"},{"location":"rules/#rule-examples","title":"Rule Examples","text":""},{"location":"rules/#conditional-evaluation","title":"Conditional Evaluation","text":"<pre><code># Only evaluate expensive models\nhookedllm.scope(\"evaluation\").after(\n    expensive_evaluation,\n    when=hookedllm.when.model(\"gpt-4\")\n)\n\n# Quick evaluation for cheaper models\nhookedllm.scope(\"evaluation\").after(\n    quick_evaluation,\n    when=hookedllm.when.model(\"gpt-4o-mini\")\n)\n</code></pre>"},{"location":"rules/#environment-specific-hooks","title":"Environment-Specific Hooks","text":"<pre><code># Conditional logging\nhookedllm.after(\n    logger_hook,\n    when=hookedllm.when.tag(\"monitoring\")\n)\n\n# Development debugging\nhookedllm.after(\n    debug_logger,\n    when=hookedllm.when.tag(\"development\")\n)\n</code></pre>"},{"location":"rules/#user-tier-hooks","title":"User Tier Hooks","text":"<pre><code># Premium user features\nhookedllm.after(\n    premium_features,\n    when=lambda call_input, ctx: ctx.metadata.get(\"tier\") == \"premium\"\n)\n\n# Free tier limitations\nhookedllm.before(\n    enforce_limits,\n    when=lambda call_input, ctx: ctx.metadata.get(\"tier\") == \"free\"\n)\n</code></pre>"},{"location":"rules/#best-practices","title":"Best Practices","text":"<ol> <li>Use rules for conditional logic: Don't check conditions inside hooks - use rules instead</li> <li>Combine with scopes: Use scopes for isolation, rules for conditions</li> <li>Keep rules simple: Complex logic is better in hook functions</li> <li>Document custom rules: Add comments explaining complex predicates</li> </ol>"},{"location":"rules/#examples","title":"Examples","text":"<p>See Advanced Examples for more rule usage patterns.</p>"},{"location":"scopes/","title":"Scopes","text":"<p>Scopes are named isolation boundaries for hooks. They prevent hook interference across different parts of your application.</p>"},{"location":"scopes/#why-scopes","title":"Why Scopes?","text":"<p>Imagine you have two parts of your application:</p> <ol> <li>Evaluation: Runs expensive evaluation hooks on every call</li> <li>Default: Runs lightweight logging hooks</li> </ol> <p>Without scopes, evaluation hooks would run on default calls (wasteful) and default hooks would run on evaluation calls (noisy). Scopes solve this by isolating hooks to specific contexts.</p>"},{"location":"scopes/#basic-usage","title":"Basic Usage","text":"<pre><code>import hookedllm\n\n# Register hooks to different scopes\nhookedllm.scope(\"evaluation\").after(evaluate_response)\nhookedllm.scope(\"evaluation\").after(calculate_metrics)\n\nhookedllm.scope(\"default\").after(default_logger)\nhookedllm.scope(\"default\").error(alert_on_error)\n\n# Create clients with specific scopes\neval_client = hookedllm.wrap(AsyncOpenAI(), scope=\"evaluation\")\ndefault_client = hookedllm.wrap(AsyncOpenAI(), scope=\"default\")\n\n# Each client only runs hooks from its scope\n# eval_client runs: evaluate_response + calculate_metrics\n# default_client runs: default_logger + alert_on_error\n</code></pre>"},{"location":"scopes/#multiple-scopes","title":"Multiple Scopes","text":"<p>A client can use multiple scopes:</p> <pre><code>hookedllm.scope(\"logging\").finally_(log_call)\nhookedllm.scope(\"metrics\").finally_(track_metrics)\nhookedllm.scope(\"evaluation\").after(evaluate)\n\n# Client with all three scopes\nclient = hookedllm.wrap(\n    AsyncOpenAI(),\n    scope=[\"logging\", \"metrics\", \"evaluation\"]\n)\n\n# Runs hooks from all three scopes:\n# log_call + track_metrics + evaluate\n</code></pre>"},{"location":"scopes/#global-vs-scoped-hooks","title":"Global vs Scoped Hooks","text":"<p>Global hooks run for all clients, regardless of scope:</p> <pre><code># Global hook - runs for ALL clients\nhookedllm.finally_(track_all_metrics)\n\n# Scoped hooks - only for specific clients\nhookedllm.scope(\"evaluation\").after(evaluate)\nhookedllm.scope(\"default\").error(alert)\n\n# Evaluation client gets: track_all_metrics + evaluate\neval_client = hookedllm.wrap(AsyncOpenAI(), scope=\"evaluation\")\n\n# Default client gets: track_all_metrics + alert\ndefault_client = hookedllm.wrap(AsyncOpenAI(), scope=\"default\")\n</code></pre>"},{"location":"scopes/#scope-best-practices","title":"Scope Best Practices","text":"<ol> <li> <p>Use descriptive names: <code>\"evaluation\"</code>, <code>\"default\"</code>, <code>\"testing\"</code> are better than <code>\"scope1\"</code>, <code>\"scope2\"</code></p> </li> <li> <p>Keep scopes focused: Each scope should have a clear purpose</p> </li> <li> <p>Combine scopes when needed: Use multiple scopes for cross-cutting concerns</p> </li> <li> <p>Use global hooks sparingly: Only for hooks that truly need to run everywhere</p> </li> </ol>"},{"location":"scopes/#examples","title":"Examples","text":"<p>See Scopes Examples for real-world scope usage patterns.</p>"},{"location":"api/","title":"API Reference","text":"<p>Complete API reference for HookedLLM.</p>"},{"location":"api/#main-module","title":"Main Module","text":""},{"location":"api/#hookedllm","title":"<code>hookedllm</code>","text":"<p>HookedLLM - Scoped observability for LLM calls with SOLID/DI architecture.</p> Simple usage <p>import hookedllm from openai import AsyncOpenAI</p> <p>Advanced usage (custom DI):     ctx = hookedllm.create_context(         executor=CustomExecutor(logger=my_logger)     )     client = ctx.wrap(AsyncOpenAI(), scope=\"test\")</p>"},{"location":"api/#hookedllm--register-hooks-to-scopes","title":"Register hooks to scopes","text":"<p>hookedllm.scope(\"evaluation\").after(evaluate_hook)</p>"},{"location":"api/#hookedllm--wrap-client-with-scope","title":"Wrap client with scope","text":"<p>client = hookedllm.wrap(AsyncOpenAI(), scope=\"evaluation\")</p>"},{"location":"api/#hookedllm--use-normally-hooks-auto-execute","title":"Use normally - hooks auto-execute!","text":"<p>response = await client.chat.completions.create(...)</p>"},{"location":"api/#hookedllm-classes","title":"Classes","text":""},{"location":"api/#hookedllm.DefaultHookExecutor","title":"<code>DefaultHookExecutor(error_handler: Callable[[Exception, str], None] | None = None, logger: Any | None = None)</code>","text":"<p>Concrete hook executor with dependency injection.</p> <p>Single Responsibility: Only executes hooks, doesn't store them. Dependencies injected: error_handler, logger</p> <p>Guarantees: - Hook failures never break the main LLM call - Hooks execute in order - Rules are evaluated before execution</p> <p>Initialize executor with optional dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>error_handler</code> <code>Callable[[Exception, str], None] | None</code> <p>Called when a hook fails. Signature: (error, context_str)</p> <code>None</code> <code>logger</code> <code>Any | None</code> <p>Logger instance (must have .error() method)</p> <code>None</code> Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>def __init__(\n    self,\n    error_handler: Callable[[Exception, str], None] | None = None,\n    logger: Any | None = None,\n):\n    \"\"\"\n    Initialize executor with optional dependencies.\n\n    Args:\n        error_handler: Called when a hook fails. Signature: (error, context_str)\n        logger: Logger instance (must have .error() method)\n    \"\"\"\n    self._error_handler = error_handler or self._default_error_handler\n    self._logger = logger\n</code></pre>"},{"location":"api/#hookedllm.DefaultHookExecutor-functions","title":"Functions","text":""},{"location":"api/#hookedllm.DefaultHookExecutor.execute_after","title":"<code>execute_after(hooks: list[tuple[AfterHook, Rule | None]], call_input: CallInput, call_output: CallOutput, context: CallContext) -&gt; None</code>  <code>async</code>","text":"<p>Execute after hooks with rule matching.</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[tuple[AfterHook, Rule | None]]</code> <p>List of (hook, rule) tuples</p> required <code>call_input</code> <code>CallInput</code> <p>The LLM call input</p> required <code>call_output</code> <code>CallOutput</code> <p>The LLM call output</p> required <code>context</code> <code>CallContext</code> <p>The call context</p> required Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>async def execute_after(\n    self,\n    hooks: list[tuple[AfterHook, Rule | None]],\n    call_input: CallInput,\n    call_output: CallOutput,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"\n    Execute after hooks with rule matching.\n\n    Args:\n        hooks: List of (hook, rule) tuples\n        call_input: The LLM call input\n        call_output: The LLM call output\n        context: The call context\n    \"\"\"\n    for hook, rule in hooks:\n        # Check if rule matches\n        if rule is None or rule.matches(call_input, context):\n            try:\n                await hook(call_input, call_output, context)\n            except Exception as e:\n                hook_name = getattr(hook, \"__name__\", str(hook))\n                self._error_handler(e, f\"After hook {hook_name}\")\n                if self._logger:\n                    self._logger.error(f\"After hook {hook_name} failed: {e}\")\n</code></pre>"},{"location":"api/#hookedllm.DefaultHookExecutor.execute_before","title":"<code>execute_before(hooks: list[tuple[BeforeHook, Rule | None]], call_input: CallInput, context: CallContext) -&gt; None</code>  <code>async</code>","text":"<p>Execute before hooks with rule matching.</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[tuple[BeforeHook, Rule | None]]</code> <p>List of (hook, rule) tuples</p> required <code>call_input</code> <code>CallInput</code> <p>The LLM call input</p> required <code>context</code> <code>CallContext</code> <p>The call context</p> required Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>async def execute_before(\n    self,\n    hooks: list[tuple[BeforeHook, Rule | None]],\n    call_input: CallInput,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"\n    Execute before hooks with rule matching.\n\n    Args:\n        hooks: List of (hook, rule) tuples\n        call_input: The LLM call input\n        context: The call context\n    \"\"\"\n    for hook, rule in hooks:\n        # Check if rule matches\n        if rule is None or rule.matches(call_input, context):\n            try:\n                await hook(call_input, context)\n            except Exception as e:\n                hook_name = getattr(hook, \"__name__\", str(hook))\n                self._error_handler(e, f\"Before hook {hook_name}\")\n                if self._logger:\n                    self._logger.error(f\"Before hook {hook_name} failed: {e}\")\n</code></pre>"},{"location":"api/#hookedllm.DefaultHookExecutor.execute_error","title":"<code>execute_error(hooks: list[tuple[ErrorHook, Rule | None]], call_input: CallInput, error: BaseException, context: CallContext) -&gt; None</code>  <code>async</code>","text":"<p>Execute error hooks with rule matching.</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[tuple[ErrorHook, Rule | None]]</code> <p>List of (hook, rule) tuples</p> required <code>call_input</code> <code>CallInput</code> <p>The LLM call input</p> required <code>error</code> <code>BaseException</code> <p>The error that occurred</p> required <code>context</code> <code>CallContext</code> <p>The call context</p> required Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>async def execute_error(\n    self,\n    hooks: list[tuple[ErrorHook, Rule | None]],\n    call_input: CallInput,\n    error: BaseException,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"\n    Execute error hooks with rule matching.\n\n    Args:\n        hooks: List of (hook, rule) tuples\n        call_input: The LLM call input\n        error: The error that occurred\n        context: The call context\n    \"\"\"\n    for hook, rule in hooks:\n        # Check if rule matches\n        if rule is None or rule.matches(call_input, context):\n            try:\n                await hook(call_input, error, context)\n            except Exception as e:\n                hook_name = getattr(hook, \"__name__\", str(hook))\n                self._error_handler(e, f\"Error hook {hook_name}\")\n                if self._logger:\n                    self._logger.error(f\"Error hook {hook_name} failed: {e}\")\n</code></pre>"},{"location":"api/#hookedllm.DefaultHookExecutor.execute_finally","title":"<code>execute_finally(hooks: list[tuple[FinallyHook, Rule | None]], result: CallResult) -&gt; None</code>  <code>async</code>","text":"<p>Execute finally hooks.</p> <p>Note: Finally hooks don't use rule matching - they always run.</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[tuple[FinallyHook, Rule | None]]</code> <p>List of (hook, rule) tuples (rule is ignored for finally hooks)</p> required <code>result</code> <code>CallResult</code> <p>The complete call result</p> required Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>async def execute_finally(\n    self, hooks: list[tuple[FinallyHook, Rule | None]], result: CallResult\n) -&gt; None:\n    \"\"\"\n    Execute finally hooks.\n\n    Note: Finally hooks don't use rule matching - they always run.\n\n    Args:\n        hooks: List of (hook, rule) tuples (rule is ignored for finally hooks)\n        result: The complete call result\n    \"\"\"\n    for hook, _rule in hooks:\n        # Finally hooks always run, ignore rule\n        try:\n            await hook(result)\n        except Exception as e:\n            hook_name = getattr(hook, \"__name__\", str(hook))\n            self._error_handler(e, f\"Finally hook {hook_name}\")\n            if self._logger:\n                self._logger.error(f\"Finally hook {hook_name} failed: {e}\")\n</code></pre>"},{"location":"api/#hookedllm.HookedLLMContext","title":"<code>HookedLLMContext(registry: ScopeRegistry | None = None, executor: HookExecutor | None = None)</code>","text":"<p>Dependency Injection container for hookedllm.</p> <p>Holds all dependencies (registry, executor) and provides factory methods for creating wrapped clients and accessing scopes.</p> <p>Benefits: - Testable: inject mock dependencies - Flexible: swap implementations - Explicit: dependencies are clear</p> <p>Initialize context with optional dependency injection.</p> <p>Parameters:</p> Name Type Description Default <code>registry</code> <code>ScopeRegistry | None</code> <p>Custom scope registry (default: InMemoryScopeRegistry)</p> <code>None</code> <code>executor</code> <code>HookExecutor | None</code> <p>Custom hook executor (default: DefaultHookExecutor)</p> <code>None</code> Source code in <code>src/hookedllm/__init__.py</code> <pre><code>def __init__(self, registry: ScopeRegistry | None = None, executor: HookExecutor | None = None):\n    \"\"\"\n    Initialize context with optional dependency injection.\n\n    Args:\n        registry: Custom scope registry (default: InMemoryScopeRegistry)\n        executor: Custom hook executor (default: DefaultHookExecutor)\n    \"\"\"\n    # Allow injection of custom implementations (DIP)\n    self.registry = registry or InMemoryScopeRegistry()\n    self.executor = executor or DefaultHookExecutor()\n</code></pre>"},{"location":"api/#hookedllm.HookedLLMContext-functions","title":"Functions","text":""},{"location":"api/#hookedllm.HookedLLMContext.after","title":"<code>after(hook: AfterHook, *, when: Rule | None = None) -&gt; None</code>","text":"<p>Register a global after hook.</p> Source code in <code>src/hookedllm/__init__.py</code> <pre><code>def after(self, hook: AfterHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"Register a global after hook.\"\"\"\n    self.global_scope().add_after(hook, when)\n</code></pre>"},{"location":"api/#hookedllm.HookedLLMContext.before","title":"<code>before(hook: BeforeHook, *, when: Rule | None = None) -&gt; None</code>","text":"<p>Register a global before hook.</p> Source code in <code>src/hookedllm/__init__.py</code> <pre><code>def before(self, hook: BeforeHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"Register a global before hook.\"\"\"\n    self.global_scope().add_before(hook, when)\n</code></pre>"},{"location":"api/#hookedllm.HookedLLMContext.error","title":"<code>error(hook: ErrorHook, *, when: Rule | None = None) -&gt; None</code>","text":"<p>Register a global error hook.</p> Source code in <code>src/hookedllm/__init__.py</code> <pre><code>def error(self, hook: ErrorHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"Register a global error hook.\"\"\"\n    self.global_scope().add_error(hook, when)\n</code></pre>"},{"location":"api/#hookedllm.HookedLLMContext.finally_","title":"<code>finally_(hook: FinallyHook, *, when: Rule | None = None) -&gt; None</code>","text":"<p>Register a global finally hook.</p> Source code in <code>src/hookedllm/__init__.py</code> <pre><code>def finally_(self, hook: FinallyHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"Register a global finally hook.\"\"\"\n    self.global_scope().add_finally(hook, when)\n</code></pre>"},{"location":"api/#hookedllm.HookedLLMContext.global_scope","title":"<code>global_scope() -&gt; ScopeHookStore</code>","text":"<p>Get the global scope (always active).</p> <p>Returns:</p> Type Description <code>ScopeHookStore</code> <p>Global scope hook store</p> Source code in <code>src/hookedllm/__init__.py</code> <pre><code>def global_scope(self) -&gt; ScopeHookStore:\n    \"\"\"\n    Get the global scope (always active).\n\n    Returns:\n        Global scope hook store\n    \"\"\"\n    return self.registry.get_global_scope()\n</code></pre>"},{"location":"api/#hookedllm.HookedLLMContext.scope","title":"<code>scope(name: str) -&gt; ScopeHookStore</code>","text":"<p>Get a scope manager from this context.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Scope name</p> required <p>Returns:</p> Type Description <code>ScopeHookStore</code> <p>Scope hook store</p> Example <p>ctx.scope(\"evaluation\").after(my_hook)</p> Source code in <code>src/hookedllm/__init__.py</code> <pre><code>def scope(self, name: str) -&gt; ScopeHookStore:\n    \"\"\"\n    Get a scope manager from this context.\n\n    Args:\n        name: Scope name\n\n    Returns:\n        Scope hook store\n\n    Example:\n        ctx.scope(\"evaluation\").after(my_hook)\n    \"\"\"\n    return self.registry.get_scope(name)\n</code></pre>"},{"location":"api/#hookedllm.HookedLLMContext.wrap","title":"<code>wrap(client: Any, scope: str | list[str] | None = None) -&gt; HookedClientWrapper</code>","text":"<p>Wrap a client using this context's dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>OpenAI-compatible client</p> required <code>scope</code> <code>str | list[str] | None</code> <p>None, single scope name, or list of scope names</p> <code>None</code> <p>Returns:</p> Type Description <code>HookedClientWrapper</code> <p>Wrapped client with injected dependencies</p> Example <p>client = ctx.wrap(AsyncOpenAI(), scope=\"evaluation\")</p> Source code in <code>src/hookedllm/__init__.py</code> <pre><code>def wrap(self, client: Any, scope: str | list[str] | None = None) -&gt; HookedClientWrapper:\n    \"\"\"\n    Wrap a client using this context's dependencies.\n\n    Args:\n        client: OpenAI-compatible client\n        scope: None, single scope name, or list of scope names\n\n    Returns:\n        Wrapped client with injected dependencies\n\n    Example:\n        client = ctx.wrap(AsyncOpenAI(), scope=\"evaluation\")\n    \"\"\"\n    # Convert scope to list\n    if scope is None:\n        scope_list = None\n    elif isinstance(scope, str):\n        scope_list = [scope]\n    else:\n        scope_list = scope\n\n    # Get scopes from registry\n    scopes = self.registry.get_scopes_for_client(scope_list)\n\n    # Create wrapper with injected dependencies (DI!)\n    return HookedClientWrapper(client, scopes, self.executor)\n</code></pre>"},{"location":"api/#hookedllm.InMemoryScopeRegistry","title":"<code>InMemoryScopeRegistry()</code>","text":"<p>Concrete implementation of ScopeRegistry.</p> <p>Single Responsibility: Only manages scope lifecycle. Creates and retrieves scopes, manages global scope.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def __init__(self):\n    self._scopes: dict[str, InMemoryScopeHookStore] = {}\n    self._global = InMemoryScopeHookStore(\"__global__\")\n</code></pre>"},{"location":"api/#hookedllm.InMemoryScopeRegistry-functions","title":"Functions","text":""},{"location":"api/#hookedllm.InMemoryScopeRegistry.get_global_scope","title":"<code>get_global_scope() -&gt; InMemoryScopeHookStore</code>","text":"<p>Get the global scope (always active).</p> <p>Returns:</p> Type Description <code>InMemoryScopeHookStore</code> <p>The global scope hook store</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_global_scope(self) -&gt; InMemoryScopeHookStore:\n    \"\"\"\n    Get the global scope (always active).\n\n    Returns:\n        The global scope hook store\n    \"\"\"\n    return self._global\n</code></pre>"},{"location":"api/#hookedllm.InMemoryScopeRegistry.get_scope","title":"<code>get_scope(name: str) -&gt; InMemoryScopeHookStore</code>","text":"<p>Get or create a named scope.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Scope name</p> required <p>Returns:</p> Type Description <code>InMemoryScopeHookStore</code> <p>Scope hook store for the named scope</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_scope(self, name: str) -&gt; InMemoryScopeHookStore:\n    \"\"\"\n    Get or create a named scope.\n\n    Args:\n        name: Scope name\n\n    Returns:\n        Scope hook store for the named scope\n    \"\"\"\n    if name not in self._scopes:\n        self._scopes[name] = InMemoryScopeHookStore(name)\n    return self._scopes[name]\n</code></pre>"},{"location":"api/#hookedllm.InMemoryScopeRegistry.get_scopes_for_client","title":"<code>get_scopes_for_client(scope_names: list[str] | None = None) -&gt; list[ScopeHookStore]</code>","text":"<p>Get list of scopes for a client.</p> <p>Always includes the global scope, plus any requested scopes.</p> <p>Parameters:</p> Name Type Description Default <code>scope_names</code> <code>list[str] | None</code> <p>List of scope names, or None for global only</p> <code>None</code> <p>Returns:</p> Type Description <code>list[ScopeHookStore]</code> <p>List of scope hook stores (global + requested scopes)</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_scopes_for_client(self, scope_names: list[str] | None = None) -&gt; list[ScopeHookStore]:\n    \"\"\"\n    Get list of scopes for a client.\n\n    Always includes the global scope, plus any requested scopes.\n\n    Args:\n        scope_names: List of scope names, or None for global only\n\n    Returns:\n        List of scope hook stores (global + requested scopes)\n    \"\"\"\n    scopes: list[ScopeHookStore] = [self._global]  # Always include global\n\n    if scope_names:\n        for name in scope_names:\n            scopes.append(self.get_scope(name))\n\n    return scopes\n</code></pre>"},{"location":"api/#hookedllm.InMemoryScopeRegistry.scope","title":"<code>scope(name: str) -&gt; InMemoryScopeHookStore</code>","text":"<p>Alias for get_scope for more fluent API.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def scope(self, name: str) -&gt; InMemoryScopeHookStore:\n    \"\"\"Alias for get_scope for more fluent API.\"\"\"\n    return self.get_scope(name)\n</code></pre>"},{"location":"api/#hookedllm-functions","title":"Functions","text":""},{"location":"api/#hookedllm.after","title":"<code>after(hook: AfterHook, *, when: Rule | None = None) -&gt; None</code>","text":"<p>Register a global after hook (uses default context).</p> <p>Parameters:</p> Name Type Description Default <code>hook</code> <code>AfterHook</code> <p>After hook function</p> required <code>when</code> <code>Rule | None</code> <p>Optional rule for conditional execution</p> <code>None</code> Example <p>hookedllm.after(my_hook, when=hookedllm.when.tag(\"production\"))</p> Source code in <code>src/hookedllm/__init__.py</code> <pre><code>def after(hook: AfterHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"\n    Register a global after hook (uses default context).\n\n    Args:\n        hook: After hook function\n        when: Optional rule for conditional execution\n\n    Example:\n        hookedllm.after(my_hook, when=hookedllm.when.tag(\"production\"))\n    \"\"\"\n    _default_context.after(hook, when=when)\n</code></pre>"},{"location":"api/#hookedllm.before","title":"<code>before(hook: BeforeHook, *, when: Rule | None = None) -&gt; None</code>","text":"<p>Register a global before hook (uses default context).</p> <p>Parameters:</p> Name Type Description Default <code>hook</code> <code>BeforeHook</code> <p>Before hook function</p> required <code>when</code> <code>Rule | None</code> <p>Optional rule for conditional execution</p> <code>None</code> Example <p>hookedllm.before(my_hook, when=hookedllm.when.model(\"gpt-4\"))</p> Source code in <code>src/hookedllm/__init__.py</code> <pre><code>def before(hook: BeforeHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"\n    Register a global before hook (uses default context).\n\n    Args:\n        hook: Before hook function\n        when: Optional rule for conditional execution\n\n    Example:\n        hookedllm.before(my_hook, when=hookedllm.when.model(\"gpt-4\"))\n    \"\"\"\n    _default_context.before(hook, when=when)\n</code></pre>"},{"location":"api/#hookedllm.create_context","title":"<code>create_context(registry: ScopeRegistry | None = None, executor: HookExecutor | None = None) -&gt; HookedLLMContext</code>","text":"<p>Create a custom context with injected dependencies.</p> <p>Use this for: - Testing (inject mocks) - Custom implementations - Isolated environments</p> <p>Parameters:</p> Name Type Description Default <code>registry</code> <code>ScopeRegistry | None</code> <p>Custom scope registry</p> <code>None</code> <code>executor</code> <code>HookExecutor | None</code> <p>Custom hook executor</p> <code>None</code> <p>Returns:</p> Type Description <code>HookedLLMContext</code> <p>New context instance</p> Example <p>ctx = hookedllm.create_context(     executor=MyCustomExecutor(logger=my_logger) ) client = ctx.wrap(AsyncOpenAI(), scope=\"test\")</p> Source code in <code>src/hookedllm/__init__.py</code> <pre><code>def create_context(\n    registry: ScopeRegistry | None = None, executor: HookExecutor | None = None\n) -&gt; HookedLLMContext:\n    \"\"\"\n    Create a custom context with injected dependencies.\n\n    Use this for:\n    - Testing (inject mocks)\n    - Custom implementations\n    - Isolated environments\n\n    Args:\n        registry: Custom scope registry\n        executor: Custom hook executor\n\n    Returns:\n        New context instance\n\n    Example:\n        ctx = hookedllm.create_context(\n            executor=MyCustomExecutor(logger=my_logger)\n        )\n        client = ctx.wrap(AsyncOpenAI(), scope=\"test\")\n    \"\"\"\n    return HookedLLMContext(registry, executor)\n</code></pre>"},{"location":"api/#hookedllm.error","title":"<code>error(hook: ErrorHook, *, when: Rule | None = None) -&gt; None</code>","text":"<p>Register a global error hook (uses default context).</p> <p>Parameters:</p> Name Type Description Default <code>hook</code> <code>ErrorHook</code> <p>Error hook function</p> required <code>when</code> <code>Rule | None</code> <p>Optional rule for conditional execution</p> <code>None</code> Example <p>hookedllm.error(my_hook)</p> Source code in <code>src/hookedllm/__init__.py</code> <pre><code>def error(hook: ErrorHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"\n    Register a global error hook (uses default context).\n\n    Args:\n        hook: Error hook function\n        when: Optional rule for conditional execution\n\n    Example:\n        hookedllm.error(my_hook)\n    \"\"\"\n    _default_context.error(hook, when=when)\n</code></pre>"},{"location":"api/#hookedllm.finally_","title":"<code>finally_(hook: FinallyHook, *, when: Rule | None = None) -&gt; None</code>","text":"<p>Register a global finally hook (uses default context).</p> <p>Parameters:</p> Name Type Description Default <code>hook</code> <code>FinallyHook</code> <p>Finally hook function</p> required <code>when</code> <code>Rule | None</code> <p>Optional rule for conditional execution</p> <code>None</code> Example <p>hookedllm.finally_(my_hook)</p> Source code in <code>src/hookedllm/__init__.py</code> <pre><code>def finally_(hook: FinallyHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"\n    Register a global finally hook (uses default context).\n\n    Args:\n        hook: Finally hook function\n        when: Optional rule for conditional execution\n\n    Example:\n        hookedllm.finally_(my_hook)\n    \"\"\"\n    _default_context.finally_(hook, when=when)\n</code></pre>"},{"location":"api/#hookedllm.scope","title":"<code>scope(name: str) -&gt; ScopeHookStore</code>","text":"<p>Get a scope manager (uses default context).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Scope name</p> required <p>Returns:</p> Type Description <code>ScopeHookStore</code> <p>Scope hook store</p> Example <p>hookedllm.scope(\"evaluation\").after(my_hook)</p> Source code in <code>src/hookedllm/__init__.py</code> <pre><code>def scope(name: str) -&gt; ScopeHookStore:\n    \"\"\"\n    Get a scope manager (uses default context).\n\n    Args:\n        name: Scope name\n\n    Returns:\n        Scope hook store\n\n    Example:\n        hookedllm.scope(\"evaluation\").after(my_hook)\n    \"\"\"\n    return _default_context.scope(name)\n</code></pre>"},{"location":"api/#hookedllm.wrap","title":"<code>wrap(client: Any, scope: str | list[str] | None = None) -&gt; HookedClientWrapper</code>","text":"<p>Wrap a client with hook support (uses default context).</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>OpenAI-compatible client</p> required <code>scope</code> <code>str | list[str] | None</code> <p>None, single scope name, or list of scope names</p> <code>None</code> <p>Returns:</p> Type Description <code>HookedClientWrapper</code> <p>Wrapped client</p> Example <p>from openai import AsyncOpenAI client = hookedllm.wrap(AsyncOpenAI(), scope=\"evaluation\")</p> Source code in <code>src/hookedllm/__init__.py</code> <pre><code>def wrap(client: Any, scope: str | list[str] | None = None) -&gt; HookedClientWrapper:\n    \"\"\"\n    Wrap a client with hook support (uses default context).\n\n    Args:\n        client: OpenAI-compatible client\n        scope: None, single scope name, or list of scope names\n\n    Returns:\n        Wrapped client\n\n    Example:\n        from openai import AsyncOpenAI\n        client = hookedllm.wrap(AsyncOpenAI(), scope=\"evaluation\")\n    \"\"\"\n    return _default_context.wrap(client, scope)\n</code></pre>"},{"location":"api/#hookedllm-modules","title":"Modules","text":""},{"location":"api/#hookedllm.config","title":"<code>config</code>","text":"<p>Configuration loading for hookedllm.</p> <p>Provides YAML-based configuration loading (requires pyyaml).</p>"},{"location":"api/#hookedllm.config-classes","title":"Classes","text":""},{"location":"api/#hookedllm.config.HookConfig","title":"<code>HookConfig(name: str, type: Literal['before', 'after', 'error', 'finally'], module: str, function: str | None = None, class_name: str | None = None, when: WhenConfig | None = None, args: dict[str, Any] | None = None)</code>  <code>dataclass</code>","text":"<p>Single hook configuration from YAML.</p>"},{"location":"api/#hookedllm.config.RootConfig","title":"<code>RootConfig(global_hooks: list[HookConfig] | None = None, scopes: dict[str, ScopeConfig] | None = None)</code>  <code>dataclass</code>","text":"<p>Root configuration schema.</p>"},{"location":"api/#hookedllm.config.ScopeConfig","title":"<code>ScopeConfig(hooks: list[HookConfig])</code>  <code>dataclass</code>","text":"<p>Scope configuration with its hooks.</p>"},{"location":"api/#hookedllm.config.WhenConfig","title":"<code>WhenConfig(model: str | None = None, models: list[str] | None = None, tag: str | None = None, tags: list[str] | None = None, metadata: dict[str, Any] | None = None, all_calls: bool = False)</code>  <code>dataclass</code>","text":"<p>Rule configuration from YAML.</p>"},{"location":"api/#hookedllm.config-functions","title":"Functions","text":""},{"location":"api/#hookedllm.config.load_config","title":"<code>load_config(path: str, context: Any | None = None) -&gt; None</code>","text":"<p>Load hooks from a YAML configuration file.</p> <p>Requires pyyaml: pip install hookedllm[config]</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to YAML config file</p> required <code>context</code> <code>Any | None</code> <p>Optional HookedLLMContext to use (default: uses default context)</p> <code>None</code> Example YAML <p>global_hooks:   - name: metrics     type: finally     module: hookedllm.hooks.metrics     class_name: MetricsHook</p> <p>scopes:   evaluation:     hooks:       - name: evaluate         type: after         module: my_app.hooks         function: evaluate_response         when:           model: gpt-4</p> Example usage <p>import hookedllm hookedllm.load_config(\"hooks.yaml\")</p> Source code in <code>src/hookedllm/config/loader.py</code> <pre><code>def load_config(path: str, context: Any | None = None) -&gt; None:\n    \"\"\"\n    Load hooks from a YAML configuration file.\n\n    Requires pyyaml: pip install hookedllm[config]\n\n    Args:\n        path: Path to YAML config file\n        context: Optional HookedLLMContext to use (default: uses default context)\n\n    Example YAML:\n        global_hooks:\n          - name: metrics\n            type: finally\n            module: hookedllm.hooks.metrics\n            class_name: MetricsHook\n\n        scopes:\n          evaluation:\n            hooks:\n              - name: evaluate\n                type: after\n                module: my_app.hooks\n                function: evaluate_response\n                when:\n                  model: gpt-4\n\n    Example usage:\n        import hookedllm\n        hookedllm.load_config(\"hooks.yaml\")\n    \"\"\"\n    try:\n        import yaml\n    except ImportError as e:\n        raise ImportError(\n            \"PyYAML is required for config loading. \" \"Install with: pip install hookedllm[config]\"\n        ) from e\n\n    # Load YAML file\n    config_path = Path(path)\n    if not config_path.exists():\n        raise FileNotFoundError(f\"Config file not found: {path}\")\n\n    with open(config_path) as f:\n        data = yaml.safe_load(f)\n\n    if not data:\n        return  # Empty config\n\n    # Use provided context or default\n    if context is None:\n        import hookedllm\n\n        ctx = hookedllm._default_context\n    else:\n        ctx = context\n\n    # Load global hooks\n    if \"global_hooks\" in data and data[\"global_hooks\"]:\n        for hook_config in data[\"global_hooks\"]:\n            _register_hook_from_config(hook_config, ctx.global_scope(), ctx)\n\n    # Load scoped hooks\n    if \"scopes\" in data and data[\"scopes\"]:\n        for scope_name, scope_data in data[\"scopes\"].items():\n            scope = ctx.scope(scope_name)\n            if \"hooks\" in scope_data:\n                for hook_config in scope_data[\"hooks\"]:\n                    _register_hook_from_config(hook_config, scope, ctx)\n</code></pre>"},{"location":"api/#hookedllm.config-modules","title":"Modules","text":""},{"location":"api/#hookedllm.config.loader","title":"<code>loader</code>","text":"<p>YAML configuration loader for hookedllm.</p> <p>Loads hook configurations from YAML files and registers them.</p> Classes\u00b6 Functions\u00b6 <code>load_config(path: str, context: Any | None = None) -&gt; None</code> \u00b6 <p>Load hooks from a YAML configuration file.</p> <p>Requires pyyaml: pip install hookedllm[config]</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to YAML config file</p> required <code>context</code> <code>Any | None</code> <p>Optional HookedLLMContext to use (default: uses default context)</p> <code>None</code> Example YAML <p>global_hooks:   - name: metrics     type: finally     module: hookedllm.hooks.metrics     class_name: MetricsHook</p> <p>scopes:   evaluation:     hooks:       - name: evaluate         type: after         module: my_app.hooks         function: evaluate_response         when:           model: gpt-4</p> Example usage <p>import hookedllm hookedllm.load_config(\"hooks.yaml\")</p> Source code in <code>src/hookedllm/config/loader.py</code> <pre><code>def load_config(path: str, context: Any | None = None) -&gt; None:\n    \"\"\"\n    Load hooks from a YAML configuration file.\n\n    Requires pyyaml: pip install hookedllm[config]\n\n    Args:\n        path: Path to YAML config file\n        context: Optional HookedLLMContext to use (default: uses default context)\n\n    Example YAML:\n        global_hooks:\n          - name: metrics\n            type: finally\n            module: hookedllm.hooks.metrics\n            class_name: MetricsHook\n\n        scopes:\n          evaluation:\n            hooks:\n              - name: evaluate\n                type: after\n                module: my_app.hooks\n                function: evaluate_response\n                when:\n                  model: gpt-4\n\n    Example usage:\n        import hookedllm\n        hookedllm.load_config(\"hooks.yaml\")\n    \"\"\"\n    try:\n        import yaml\n    except ImportError as e:\n        raise ImportError(\n            \"PyYAML is required for config loading. \" \"Install with: pip install hookedllm[config]\"\n        ) from e\n\n    # Load YAML file\n    config_path = Path(path)\n    if not config_path.exists():\n        raise FileNotFoundError(f\"Config file not found: {path}\")\n\n    with open(config_path) as f:\n        data = yaml.safe_load(f)\n\n    if not data:\n        return  # Empty config\n\n    # Use provided context or default\n    if context is None:\n        import hookedllm\n\n        ctx = hookedllm._default_context\n    else:\n        ctx = context\n\n    # Load global hooks\n    if \"global_hooks\" in data and data[\"global_hooks\"]:\n        for hook_config in data[\"global_hooks\"]:\n            _register_hook_from_config(hook_config, ctx.global_scope(), ctx)\n\n    # Load scoped hooks\n    if \"scopes\" in data and data[\"scopes\"]:\n        for scope_name, scope_data in data[\"scopes\"].items():\n            scope = ctx.scope(scope_name)\n            if \"hooks\" in scope_data:\n                for hook_config in scope_data[\"hooks\"]:\n                    _register_hook_from_config(hook_config, scope, ctx)\n</code></pre>"},{"location":"api/#hookedllm.config.schema","title":"<code>schema</code>","text":"<p>Configuration schema for YAML-based hook registration.</p> <p>Defines the structure of YAML configuration files.</p> Classes\u00b6 <code>HookConfig(name: str, type: Literal['before', 'after', 'error', 'finally'], module: str, function: str | None = None, class_name: str | None = None, when: WhenConfig | None = None, args: dict[str, Any] | None = None)</code> <code>dataclass</code> \u00b6 <p>Single hook configuration from YAML.</p> <code>RootConfig(global_hooks: list[HookConfig] | None = None, scopes: dict[str, ScopeConfig] | None = None)</code> <code>dataclass</code> \u00b6 <p>Root configuration schema.</p> <code>ScopeConfig(hooks: list[HookConfig])</code> <code>dataclass</code> \u00b6 <p>Scope configuration with its hooks.</p> <code>WhenConfig(model: str | None = None, models: list[str] | None = None, tag: str | None = None, tags: list[str] | None = None, metadata: dict[str, Any] | None = None, all_calls: bool = False)</code> <code>dataclass</code> \u00b6 <p>Rule configuration from YAML.</p>"},{"location":"api/#hookedllm.core","title":"<code>core</code>","text":"<p>Core module for hookedllm.</p> <p>Contains the fundamental types, protocols, and implementations.</p>"},{"location":"api/#hookedllm.core-classes","title":"Classes","text":""},{"location":"api/#hookedllm.core.CallContext","title":"<code>CallContext(call_id: str = (lambda: str(uuid4()))(), parent_id: str | None = None, provider: str = '', model: str = '', route: str = 'chat', tags: list[str] = list(), started_at: datetime = (lambda: datetime.now(timezone.utc))(), metadata: dict[str, Any] = dict())</code>  <code>dataclass</code>","text":"<p>Context for a single LLM call lifecycle.</p> <p>Contains metadata about the call including timing, tags, and custom metadata.</p>"},{"location":"api/#hookedllm.core.CallInput","title":"<code>CallInput(model: str, messages: Sequence[Message], params: dict[str, Any] = dict(), metadata: dict[str, Any] = dict())</code>  <code>dataclass</code>","text":"<p>Normalized input for an LLM call.</p> <p>This represents the input parameters in a provider-agnostic way.</p>"},{"location":"api/#hookedllm.core.CallOutput","title":"<code>CallOutput(text: str | None, raw: Any, usage: dict[str, Any] | None = None, finish_reason: str | None = None)</code>  <code>dataclass</code>","text":"<p>Normalized output from an LLM call.</p> <p>This represents the response in a provider-agnostic way while preserving the original response object.</p>"},{"location":"api/#hookedllm.core.CallResult","title":"<code>CallResult(input: CallInput, output: CallOutput | None, context: CallContext, error: BaseException | None, ended_at: datetime, elapsed_ms: float)</code>  <code>dataclass</code>","text":"<p>Complete result of an LLM call.</p> <p>Contains the input, output, context, any error that occurred, and timing information. This is passed to finally hooks.</p>"},{"location":"api/#hookedllm.core.CompositeRule","title":"<code>CompositeRule(rules: list[Any], operator: Literal['and', 'or'])</code>  <code>dataclass</code>","text":"<p>Combine multiple rules with AND/OR logic.</p>"},{"location":"api/#hookedllm.core.CustomRule","title":"<code>CustomRule(predicate: Callable[[CallInput, CallContext], bool])</code>  <code>dataclass</code>","text":"<p>Custom predicate function.</p>"},{"location":"api/#hookedllm.core.DefaultHookExecutor","title":"<code>DefaultHookExecutor(error_handler: Callable[[Exception, str], None] | None = None, logger: Any | None = None)</code>","text":"<p>Concrete hook executor with dependency injection.</p> <p>Single Responsibility: Only executes hooks, doesn't store them. Dependencies injected: error_handler, logger</p> <p>Guarantees: - Hook failures never break the main LLM call - Hooks execute in order - Rules are evaluated before execution</p> <p>Initialize executor with optional dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>error_handler</code> <code>Callable[[Exception, str], None] | None</code> <p>Called when a hook fails. Signature: (error, context_str)</p> <code>None</code> <code>logger</code> <code>Any | None</code> <p>Logger instance (must have .error() method)</p> <code>None</code> Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>def __init__(\n    self,\n    error_handler: Callable[[Exception, str], None] | None = None,\n    logger: Any | None = None,\n):\n    \"\"\"\n    Initialize executor with optional dependencies.\n\n    Args:\n        error_handler: Called when a hook fails. Signature: (error, context_str)\n        logger: Logger instance (must have .error() method)\n    \"\"\"\n    self._error_handler = error_handler or self._default_error_handler\n    self._logger = logger\n</code></pre> Functions\u00b6 <code>execute_after(hooks: list[tuple[AfterHook, Rule | None]], call_input: CallInput, call_output: CallOutput, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Execute after hooks with rule matching.</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[tuple[AfterHook, Rule | None]]</code> <p>List of (hook, rule) tuples</p> required <code>call_input</code> <code>CallInput</code> <p>The LLM call input</p> required <code>call_output</code> <code>CallOutput</code> <p>The LLM call output</p> required <code>context</code> <code>CallContext</code> <p>The call context</p> required Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>async def execute_after(\n    self,\n    hooks: list[tuple[AfterHook, Rule | None]],\n    call_input: CallInput,\n    call_output: CallOutput,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"\n    Execute after hooks with rule matching.\n\n    Args:\n        hooks: List of (hook, rule) tuples\n        call_input: The LLM call input\n        call_output: The LLM call output\n        context: The call context\n    \"\"\"\n    for hook, rule in hooks:\n        # Check if rule matches\n        if rule is None or rule.matches(call_input, context):\n            try:\n                await hook(call_input, call_output, context)\n            except Exception as e:\n                hook_name = getattr(hook, \"__name__\", str(hook))\n                self._error_handler(e, f\"After hook {hook_name}\")\n                if self._logger:\n                    self._logger.error(f\"After hook {hook_name} failed: {e}\")\n</code></pre> <code>execute_before(hooks: list[tuple[BeforeHook, Rule | None]], call_input: CallInput, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Execute before hooks with rule matching.</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[tuple[BeforeHook, Rule | None]]</code> <p>List of (hook, rule) tuples</p> required <code>call_input</code> <code>CallInput</code> <p>The LLM call input</p> required <code>context</code> <code>CallContext</code> <p>The call context</p> required Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>async def execute_before(\n    self,\n    hooks: list[tuple[BeforeHook, Rule | None]],\n    call_input: CallInput,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"\n    Execute before hooks with rule matching.\n\n    Args:\n        hooks: List of (hook, rule) tuples\n        call_input: The LLM call input\n        context: The call context\n    \"\"\"\n    for hook, rule in hooks:\n        # Check if rule matches\n        if rule is None or rule.matches(call_input, context):\n            try:\n                await hook(call_input, context)\n            except Exception as e:\n                hook_name = getattr(hook, \"__name__\", str(hook))\n                self._error_handler(e, f\"Before hook {hook_name}\")\n                if self._logger:\n                    self._logger.error(f\"Before hook {hook_name} failed: {e}\")\n</code></pre> <code>execute_error(hooks: list[tuple[ErrorHook, Rule | None]], call_input: CallInput, error: BaseException, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Execute error hooks with rule matching.</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[tuple[ErrorHook, Rule | None]]</code> <p>List of (hook, rule) tuples</p> required <code>call_input</code> <code>CallInput</code> <p>The LLM call input</p> required <code>error</code> <code>BaseException</code> <p>The error that occurred</p> required <code>context</code> <code>CallContext</code> <p>The call context</p> required Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>async def execute_error(\n    self,\n    hooks: list[tuple[ErrorHook, Rule | None]],\n    call_input: CallInput,\n    error: BaseException,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"\n    Execute error hooks with rule matching.\n\n    Args:\n        hooks: List of (hook, rule) tuples\n        call_input: The LLM call input\n        error: The error that occurred\n        context: The call context\n    \"\"\"\n    for hook, rule in hooks:\n        # Check if rule matches\n        if rule is None or rule.matches(call_input, context):\n            try:\n                await hook(call_input, error, context)\n            except Exception as e:\n                hook_name = getattr(hook, \"__name__\", str(hook))\n                self._error_handler(e, f\"Error hook {hook_name}\")\n                if self._logger:\n                    self._logger.error(f\"Error hook {hook_name} failed: {e}\")\n</code></pre> <code>execute_finally(hooks: list[tuple[FinallyHook, Rule | None]], result: CallResult) -&gt; None</code> <code>async</code> \u00b6 <p>Execute finally hooks.</p> <p>Note: Finally hooks don't use rule matching - they always run.</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[tuple[FinallyHook, Rule | None]]</code> <p>List of (hook, rule) tuples (rule is ignored for finally hooks)</p> required <code>result</code> <code>CallResult</code> <p>The complete call result</p> required Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>async def execute_finally(\n    self, hooks: list[tuple[FinallyHook, Rule | None]], result: CallResult\n) -&gt; None:\n    \"\"\"\n    Execute finally hooks.\n\n    Note: Finally hooks don't use rule matching - they always run.\n\n    Args:\n        hooks: List of (hook, rule) tuples (rule is ignored for finally hooks)\n        result: The complete call result\n    \"\"\"\n    for hook, _rule in hooks:\n        # Finally hooks always run, ignore rule\n        try:\n            await hook(result)\n        except Exception as e:\n            hook_name = getattr(hook, \"__name__\", str(hook))\n            self._error_handler(e, f\"Finally hook {hook_name}\")\n            if self._logger:\n                self._logger.error(f\"Finally hook {hook_name} failed: {e}\")\n</code></pre>"},{"location":"api/#hookedllm.core.HookExecutor","title":"<code>HookExecutor</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for executing hooks.</p> <p>Single Responsibility: Hook execution only, no storage.</p> Functions\u00b6 <code>execute_after(hooks: list[tuple[AfterHook, Rule | None]], call_input: CallInput, call_output: CallOutput, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Execute after hooks with rule matching.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>async def execute_after(\n    self,\n    hooks: list[tuple[AfterHook, Rule | None]],\n    call_input: CallInput,\n    call_output: CallOutput,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"Execute after hooks with rule matching.\"\"\"\n    ...\n</code></pre> <code>execute_before(hooks: list[tuple[BeforeHook, Rule | None]], call_input: CallInput, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Execute before hooks with rule matching.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>async def execute_before(\n    self,\n    hooks: list[tuple[BeforeHook, Rule | None]],\n    call_input: CallInput,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"Execute before hooks with rule matching.\"\"\"\n    ...\n</code></pre> <code>execute_error(hooks: list[tuple[ErrorHook, Rule | None]], call_input: CallInput, error: BaseException, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Execute error hooks with rule matching.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>async def execute_error(\n    self,\n    hooks: list[tuple[ErrorHook, Rule | None]],\n    call_input: CallInput,\n    error: BaseException,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"Execute error hooks with rule matching.\"\"\"\n    ...\n</code></pre> <code>execute_finally(hooks: list[tuple[FinallyHook, Rule | None]], result: CallResult) -&gt; None</code> <code>async</code> \u00b6 <p>Execute finally hooks.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>async def execute_finally(\n    self, hooks: list[tuple[FinallyHook, Rule | None]], result: CallResult\n) -&gt; None:\n    \"\"\"Execute finally hooks.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.HookedClientWrapper","title":"<code>HookedClientWrapper(original_client: Any, scopes: list[ScopeHookStore], executor: HookExecutor)</code>","text":"<p>Transparent proxy with all dependencies injected.</p> <p>No global state - all dependencies passed via constructor (DI). Intercepts provider SDK methods to inject hook execution.</p> <p>Initialize wrapper with injected dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>original_client</code> <code>Any</code> <p>The original provider client</p> required <code>scopes</code> <code>list[ScopeHookStore]</code> <p>List of scope hook stores to use</p> required <code>executor</code> <code>HookExecutor</code> <p>Hook executor instance</p> required Source code in <code>src/hookedllm/core/wrapper.py</code> <pre><code>def __init__(self, original_client: Any, scopes: list[ScopeHookStore], executor: HookExecutor):\n    \"\"\"\n    Initialize wrapper with injected dependencies.\n\n    Args:\n        original_client: The original provider client\n        scopes: List of scope hook stores to use\n        executor: Hook executor instance\n    \"\"\"\n    self._original = original_client\n    self._scopes = scopes\n    self._executor = executor\n    self._adapter = _detect_provider_adapter(original_client)\n    self._wrapper_path = self._adapter.get_wrapper_path(original_client)\n</code></pre> Functions\u00b6 <code>__getattr__(name: str) -&gt; Any</code> \u00b6 <p>Intercept attribute access.</p> <p>Wraps attributes based on the detected provider's wrapper path.</p> Source code in <code>src/hookedllm/core/wrapper.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"\n    Intercept attribute access.\n\n    Wraps attributes based on the detected provider's wrapper path.\n    \"\"\"\n    attr = getattr(self._original, name)\n\n    # Check if this attribute is part of the wrapper path\n    if len(self._wrapper_path) &gt; 0 and name == self._wrapper_path[0]:\n        # Create a dynamic wrapper for the next level\n        return _create_wrapper_for_path(\n            attr, self._wrapper_path[1:], self._scopes, self._executor, self._adapter\n        )\n\n    return attr\n</code></pre>"},{"location":"api/#hookedllm.core.InMemoryScopeHookStore","title":"<code>InMemoryScopeHookStore(scope_name: str)</code>","text":"<p>Concrete implementation of ScopeHookStore.</p> <p>Single Responsibility: Only stores hooks. Stores hooks in memory with their associated rules.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def __init__(self, scope_name: str):\n    self._scope_name = scope_name\n    self._before: list[tuple[BeforeHook, Rule | None]] = []\n    self._after: list[tuple[AfterHook, Rule | None]] = []\n    self._error: list[tuple[ErrorHook, Rule | None]] = []\n    self._finally: list[tuple[FinallyHook, Rule | None]] = []\n</code></pre> Attributes\u00b6 <code>name: str</code> <code>property</code> \u00b6 <p>Get the scope name.</p> Functions\u00b6 <code>add_after(hook: AfterHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add an after hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def add_after(self, hook: AfterHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add an after hook with optional execution rule.\"\"\"\n    self._after.append((hook, rule))\n</code></pre> <code>add_before(hook: BeforeHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add a before hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def add_before(self, hook: BeforeHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add a before hook with optional execution rule.\"\"\"\n    self._before.append((hook, rule))\n</code></pre> <code>add_error(hook: ErrorHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add an error hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def add_error(self, hook: ErrorHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add an error hook with optional execution rule.\"\"\"\n    self._error.append((hook, rule))\n</code></pre> <code>add_finally(hook: FinallyHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add a finally hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def add_finally(self, hook: FinallyHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add a finally hook with optional execution rule.\"\"\"\n    self._finally.append((hook, rule))\n</code></pre> <code>after(hook: AfterHook, *, when: Rule | None = None) -&gt; None</code> \u00b6 <p>Alias for add_after with keyword 'when' parameter.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def after(self, hook: AfterHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"Alias for add_after with keyword 'when' parameter.\"\"\"\n    self.add_after(hook, when)\n</code></pre> <code>before(hook: BeforeHook, *, when: Rule | None = None) -&gt; None</code> \u00b6 <p>Alias for add_before with keyword 'when' parameter.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def before(self, hook: BeforeHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"Alias for add_before with keyword 'when' parameter.\"\"\"\n    self.add_before(hook, when)\n</code></pre> <code>error(hook: ErrorHook, *, when: Rule | None = None) -&gt; None</code> \u00b6 <p>Alias for add_error with keyword 'when' parameter.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def error(self, hook: ErrorHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"Alias for add_error with keyword 'when' parameter.\"\"\"\n    self.add_error(hook, when)\n</code></pre> <code>finally_(hook: FinallyHook, *, when: Rule | None = None) -&gt; None</code> \u00b6 <p>Alias for add_finally with keyword 'when' parameter.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def finally_(self, hook: FinallyHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"Alias for add_finally with keyword 'when' parameter.\"\"\"\n    self.add_finally(hook, when)\n</code></pre> <code>get_after_hooks() -&gt; list[tuple[AfterHook, Rule | None]]</code> \u00b6 <p>Get all after hooks with their rules (returns copy).</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_after_hooks(self) -&gt; list[tuple[AfterHook, Rule | None]]:\n    \"\"\"Get all after hooks with their rules (returns copy).\"\"\"\n    return self._after.copy()\n</code></pre> <code>get_before_hooks() -&gt; list[tuple[BeforeHook, Rule | None]]</code> \u00b6 <p>Get all before hooks with their rules (returns copy).</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_before_hooks(self) -&gt; list[tuple[BeforeHook, Rule | None]]:\n    \"\"\"Get all before hooks with their rules (returns copy).\"\"\"\n    return self._before.copy()\n</code></pre> <code>get_error_hooks() -&gt; list[tuple[ErrorHook, Rule | None]]</code> \u00b6 <p>Get all error hooks with their rules (returns copy).</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_error_hooks(self) -&gt; list[tuple[ErrorHook, Rule | None]]:\n    \"\"\"Get all error hooks with their rules (returns copy).\"\"\"\n    return self._error.copy()\n</code></pre> <code>get_finally_hooks() -&gt; list[tuple[FinallyHook, Rule | None]]</code> \u00b6 <p>Get all finally hooks with their rules (returns copy).</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_finally_hooks(self) -&gt; list[tuple[FinallyHook, Rule | None]]:\n    \"\"\"Get all finally hooks with their rules (returns copy).\"\"\"\n    return self._finally.copy()\n</code></pre>"},{"location":"api/#hookedllm.core.InMemoryScopeRegistry","title":"<code>InMemoryScopeRegistry()</code>","text":"<p>Concrete implementation of ScopeRegistry.</p> <p>Single Responsibility: Only manages scope lifecycle. Creates and retrieves scopes, manages global scope.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def __init__(self):\n    self._scopes: dict[str, InMemoryScopeHookStore] = {}\n    self._global = InMemoryScopeHookStore(\"__global__\")\n</code></pre> Functions\u00b6 <code>get_global_scope() -&gt; InMemoryScopeHookStore</code> \u00b6 <p>Get the global scope (always active).</p> <p>Returns:</p> Type Description <code>InMemoryScopeHookStore</code> <p>The global scope hook store</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_global_scope(self) -&gt; InMemoryScopeHookStore:\n    \"\"\"\n    Get the global scope (always active).\n\n    Returns:\n        The global scope hook store\n    \"\"\"\n    return self._global\n</code></pre> <code>get_scope(name: str) -&gt; InMemoryScopeHookStore</code> \u00b6 <p>Get or create a named scope.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Scope name</p> required <p>Returns:</p> Type Description <code>InMemoryScopeHookStore</code> <p>Scope hook store for the named scope</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_scope(self, name: str) -&gt; InMemoryScopeHookStore:\n    \"\"\"\n    Get or create a named scope.\n\n    Args:\n        name: Scope name\n\n    Returns:\n        Scope hook store for the named scope\n    \"\"\"\n    if name not in self._scopes:\n        self._scopes[name] = InMemoryScopeHookStore(name)\n    return self._scopes[name]\n</code></pre> <code>get_scopes_for_client(scope_names: list[str] | None = None) -&gt; list[ScopeHookStore]</code> \u00b6 <p>Get list of scopes for a client.</p> <p>Always includes the global scope, plus any requested scopes.</p> <p>Parameters:</p> Name Type Description Default <code>scope_names</code> <code>list[str] | None</code> <p>List of scope names, or None for global only</p> <code>None</code> <p>Returns:</p> Type Description <code>list[ScopeHookStore]</code> <p>List of scope hook stores (global + requested scopes)</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_scopes_for_client(self, scope_names: list[str] | None = None) -&gt; list[ScopeHookStore]:\n    \"\"\"\n    Get list of scopes for a client.\n\n    Always includes the global scope, plus any requested scopes.\n\n    Args:\n        scope_names: List of scope names, or None for global only\n\n    Returns:\n        List of scope hook stores (global + requested scopes)\n    \"\"\"\n    scopes: list[ScopeHookStore] = [self._global]  # Always include global\n\n    if scope_names:\n        for name in scope_names:\n            scopes.append(self.get_scope(name))\n\n    return scopes\n</code></pre> <code>scope(name: str) -&gt; InMemoryScopeHookStore</code> \u00b6 <p>Alias for get_scope for more fluent API.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def scope(self, name: str) -&gt; InMemoryScopeHookStore:\n    \"\"\"Alias for get_scope for more fluent API.\"\"\"\n    return self.get_scope(name)\n</code></pre>"},{"location":"api/#hookedllm.core.Message","title":"<code>Message(role: str, content: Any)</code>  <code>dataclass</code>","text":"<p>A single message in an LLM conversation.</p>"},{"location":"api/#hookedllm.core.MetadataRule","title":"<code>MetadataRule(conditions: dict[str, Any])</code>  <code>dataclass</code>","text":"<p>Match based on metadata key-value pairs.</p>"},{"location":"api/#hookedllm.core.ModelRule","title":"<code>ModelRule(models: list[str])</code>  <code>dataclass</code>","text":"<p>Match specific model(s).</p>"},{"location":"api/#hookedllm.core.NotRule","title":"<code>NotRule(rule: Any)</code>  <code>dataclass</code>","text":"<p>Negate a rule.</p>"},{"location":"api/#hookedllm.core.Rule","title":"<code>Rule</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for hook execution rules.</p> <p>Rules determine whether a hook should execute based on the call input and context.</p> Functions\u00b6 <code>__and__(other: Rule) -&gt; Rule</code> \u00b6 <p>Combine rules with AND logic.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def __and__(self, other: Rule) -&gt; Rule:\n    \"\"\"Combine rules with AND logic.\"\"\"\n    ...\n</code></pre> <code>__invert__() -&gt; Rule</code> \u00b6 <p>Negate the rule (NOT logic).</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def __invert__(self) -&gt; Rule:\n    \"\"\"Negate the rule (NOT logic).\"\"\"\n    ...\n</code></pre> <code>__or__(other: Rule) -&gt; Rule</code> \u00b6 <p>Combine rules with OR logic.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def __or__(self, other: Rule) -&gt; Rule:\n    \"\"\"Combine rules with OR logic.\"\"\"\n    ...\n</code></pre> <code>matches(call_input: CallInput, context: CallContext) -&gt; bool</code> \u00b6 <p>Check if this rule matches the given call.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def matches(self, call_input: CallInput, context: CallContext) -&gt; bool:\n    \"\"\"Check if this rule matches the given call.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.RuleBuilder","title":"<code>RuleBuilder</code>","text":"<p>Fluent API for building rules.</p> <p>Used via the 'when' global instance.</p> Functions\u00b6 <code>always() -&gt; CustomRule</code> <code>staticmethod</code> \u00b6 <p>Always matches.</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef always() -&gt; CustomRule:\n    \"\"\"Always matches.\"\"\"\n    return CustomRule(lambda i, c: True)\n</code></pre> <code>custom(predicate: Callable[[CallInput, CallContext], bool]) -&gt; CustomRule</code> <code>staticmethod</code> \u00b6 <p>Custom predicate function.</p> Example <p>when.custom(lambda i, c: c.metadata.get(\"score\", 0) &gt; 0.8)</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef custom(predicate: Callable[[CallInput, CallContext], bool]) -&gt; CustomRule:\n    \"\"\"\n    Custom predicate function.\n\n    Example:\n        when.custom(lambda i, c: c.metadata.get(\"score\", 0) &gt; 0.8)\n    \"\"\"\n    return CustomRule(predicate)\n</code></pre> <code>metadata(**conditions: Any) -&gt; MetadataRule</code> <code>staticmethod</code> \u00b6 <p>Match metadata conditions.</p> Example <p>when.metadata(user_tier=\"premium\", region=\"us-east\")</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef metadata(**conditions: Any) -&gt; MetadataRule:\n    \"\"\"\n    Match metadata conditions.\n\n    Example:\n        when.metadata(user_tier=\"premium\", region=\"us-east\")\n    \"\"\"\n    return MetadataRule(conditions)\n</code></pre> <code>model(*models: str) -&gt; ModelRule</code> <code>staticmethod</code> \u00b6 <p>Match specific model(s).</p> Example <p>when.model(\"gpt-4\", \"gpt-4-turbo\")</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef model(*models: str) -&gt; ModelRule:\n    \"\"\"\n    Match specific model(s).\n\n    Example:\n        when.model(\"gpt-4\", \"gpt-4-turbo\")\n    \"\"\"\n    return ModelRule(list(models))\n</code></pre> <code>never() -&gt; CustomRule</code> <code>staticmethod</code> \u00b6 <p>Never matches.</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef never() -&gt; CustomRule:\n    \"\"\"Never matches.\"\"\"\n    return CustomRule(lambda i, c: False)\n</code></pre> <code>tag(*tags: str, all_: bool = False) -&gt; TagRule</code> <code>staticmethod</code> \u00b6 <p>Match if context has tag(s).</p> <p>Parameters:</p> Name Type Description Default <code>*tags</code> <code>str</code> <p>Tag names to match</p> <code>()</code> <code>all_</code> <code>bool</code> <p>If True, all tags must be present. If False, any tag matches.</p> <code>False</code> Example <p>when.tag(\"production\", \"critical\")</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef tag(*tags: str, all_: bool = False) -&gt; TagRule:\n    \"\"\"\n    Match if context has tag(s).\n\n    Args:\n        *tags: Tag names to match\n        all_: If True, all tags must be present. If False, any tag matches.\n\n    Example:\n        when.tag(\"production\", \"critical\")\n    \"\"\"\n    return TagRule(list(tags), require_all=all_)\n</code></pre>"},{"location":"api/#hookedllm.core.ScopeHookStore","title":"<code>ScopeHookStore</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for storing hooks in a scope.</p> <p>Single Responsibility: Storage only, no execution logic.</p> Functions\u00b6 <code>add_after(hook: AfterHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add an after hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def add_after(self, hook: AfterHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add an after hook with optional execution rule.\"\"\"\n    ...\n</code></pre> <code>add_before(hook: BeforeHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add a before hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def add_before(self, hook: BeforeHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add a before hook with optional execution rule.\"\"\"\n    ...\n</code></pre> <code>add_error(hook: ErrorHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add an error hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def add_error(self, hook: ErrorHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add an error hook with optional execution rule.\"\"\"\n    ...\n</code></pre> <code>add_finally(hook: FinallyHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add a finally hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def add_finally(self, hook: FinallyHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add a finally hook with optional execution rule.\"\"\"\n    ...\n</code></pre> <code>get_after_hooks() -&gt; list[tuple[AfterHook, Rule | None]]</code> \u00b6 <p>Get all after hooks with their rules.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_after_hooks(self) -&gt; list[tuple[AfterHook, Rule | None]]:\n    \"\"\"Get all after hooks with their rules.\"\"\"\n    ...\n</code></pre> <code>get_before_hooks() -&gt; list[tuple[BeforeHook, Rule | None]]</code> \u00b6 <p>Get all before hooks with their rules.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_before_hooks(self) -&gt; list[tuple[BeforeHook, Rule | None]]:\n    \"\"\"Get all before hooks with their rules.\"\"\"\n    ...\n</code></pre> <code>get_error_hooks() -&gt; list[tuple[ErrorHook, Rule | None]]</code> \u00b6 <p>Get all error hooks with their rules.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_error_hooks(self) -&gt; list[tuple[ErrorHook, Rule | None]]:\n    \"\"\"Get all error hooks with their rules.\"\"\"\n    ...\n</code></pre> <code>get_finally_hooks() -&gt; list[tuple[FinallyHook, Rule | None]]</code> \u00b6 <p>Get all finally hooks with their rules.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_finally_hooks(self) -&gt; list[tuple[FinallyHook, Rule | None]]:\n    \"\"\"Get all finally hooks with their rules.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.ScopeRegistry","title":"<code>ScopeRegistry</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for managing scopes.</p> <p>Single Responsibility: Scope lifecycle management only.</p> Functions\u00b6 <code>get_global_scope() -&gt; ScopeHookStore</code> \u00b6 <p>Get the global scope (always active).</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_global_scope(self) -&gt; ScopeHookStore:\n    \"\"\"Get the global scope (always active).\"\"\"\n    ...\n</code></pre> <code>get_scope(name: str) -&gt; ScopeHookStore</code> \u00b6 <p>Get or create a named scope.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_scope(self, name: str) -&gt; ScopeHookStore:\n    \"\"\"Get or create a named scope.\"\"\"\n    ...\n</code></pre> <code>get_scopes_for_client(scope_names: list[str] | None) -&gt; list[ScopeHookStore]</code> \u00b6 <p>Get list of scopes for a client.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_scopes_for_client(self, scope_names: list[str] | None) -&gt; list[ScopeHookStore]:\n    \"\"\"Get list of scopes for a client.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.TagRule","title":"<code>TagRule(tags: list[str], require_all: bool = False)</code>  <code>dataclass</code>","text":"<p>Match if context has specific tag(s).</p>"},{"location":"api/#hookedllm.core-modules","title":"Modules","text":""},{"location":"api/#hookedllm.core.executor","title":"<code>executor</code>","text":"<p>Hook executor with dependency injection.</p> <p>Executes hooks with rule matching while isolating failures.</p> Classes\u00b6 <code>DefaultHookExecutor(error_handler: Callable[[Exception, str], None] | None = None, logger: Any | None = None)</code> \u00b6 <p>Concrete hook executor with dependency injection.</p> <p>Single Responsibility: Only executes hooks, doesn't store them. Dependencies injected: error_handler, logger</p> <p>Guarantees: - Hook failures never break the main LLM call - Hooks execute in order - Rules are evaluated before execution</p> <p>Initialize executor with optional dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>error_handler</code> <code>Callable[[Exception, str], None] | None</code> <p>Called when a hook fails. Signature: (error, context_str)</p> <code>None</code> <code>logger</code> <code>Any | None</code> <p>Logger instance (must have .error() method)</p> <code>None</code> Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>def __init__(\n    self,\n    error_handler: Callable[[Exception, str], None] | None = None,\n    logger: Any | None = None,\n):\n    \"\"\"\n    Initialize executor with optional dependencies.\n\n    Args:\n        error_handler: Called when a hook fails. Signature: (error, context_str)\n        logger: Logger instance (must have .error() method)\n    \"\"\"\n    self._error_handler = error_handler or self._default_error_handler\n    self._logger = logger\n</code></pre> Functions\u00b6 <code>execute_after(hooks: list[tuple[AfterHook, Rule | None]], call_input: CallInput, call_output: CallOutput, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Execute after hooks with rule matching.</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[tuple[AfterHook, Rule | None]]</code> <p>List of (hook, rule) tuples</p> required <code>call_input</code> <code>CallInput</code> <p>The LLM call input</p> required <code>call_output</code> <code>CallOutput</code> <p>The LLM call output</p> required <code>context</code> <code>CallContext</code> <p>The call context</p> required Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>async def execute_after(\n    self,\n    hooks: list[tuple[AfterHook, Rule | None]],\n    call_input: CallInput,\n    call_output: CallOutput,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"\n    Execute after hooks with rule matching.\n\n    Args:\n        hooks: List of (hook, rule) tuples\n        call_input: The LLM call input\n        call_output: The LLM call output\n        context: The call context\n    \"\"\"\n    for hook, rule in hooks:\n        # Check if rule matches\n        if rule is None or rule.matches(call_input, context):\n            try:\n                await hook(call_input, call_output, context)\n            except Exception as e:\n                hook_name = getattr(hook, \"__name__\", str(hook))\n                self._error_handler(e, f\"After hook {hook_name}\")\n                if self._logger:\n                    self._logger.error(f\"After hook {hook_name} failed: {e}\")\n</code></pre> <code>execute_before(hooks: list[tuple[BeforeHook, Rule | None]], call_input: CallInput, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Execute before hooks with rule matching.</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[tuple[BeforeHook, Rule | None]]</code> <p>List of (hook, rule) tuples</p> required <code>call_input</code> <code>CallInput</code> <p>The LLM call input</p> required <code>context</code> <code>CallContext</code> <p>The call context</p> required Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>async def execute_before(\n    self,\n    hooks: list[tuple[BeforeHook, Rule | None]],\n    call_input: CallInput,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"\n    Execute before hooks with rule matching.\n\n    Args:\n        hooks: List of (hook, rule) tuples\n        call_input: The LLM call input\n        context: The call context\n    \"\"\"\n    for hook, rule in hooks:\n        # Check if rule matches\n        if rule is None or rule.matches(call_input, context):\n            try:\n                await hook(call_input, context)\n            except Exception as e:\n                hook_name = getattr(hook, \"__name__\", str(hook))\n                self._error_handler(e, f\"Before hook {hook_name}\")\n                if self._logger:\n                    self._logger.error(f\"Before hook {hook_name} failed: {e}\")\n</code></pre> <code>execute_error(hooks: list[tuple[ErrorHook, Rule | None]], call_input: CallInput, error: BaseException, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Execute error hooks with rule matching.</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[tuple[ErrorHook, Rule | None]]</code> <p>List of (hook, rule) tuples</p> required <code>call_input</code> <code>CallInput</code> <p>The LLM call input</p> required <code>error</code> <code>BaseException</code> <p>The error that occurred</p> required <code>context</code> <code>CallContext</code> <p>The call context</p> required Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>async def execute_error(\n    self,\n    hooks: list[tuple[ErrorHook, Rule | None]],\n    call_input: CallInput,\n    error: BaseException,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"\n    Execute error hooks with rule matching.\n\n    Args:\n        hooks: List of (hook, rule) tuples\n        call_input: The LLM call input\n        error: The error that occurred\n        context: The call context\n    \"\"\"\n    for hook, rule in hooks:\n        # Check if rule matches\n        if rule is None or rule.matches(call_input, context):\n            try:\n                await hook(call_input, error, context)\n            except Exception as e:\n                hook_name = getattr(hook, \"__name__\", str(hook))\n                self._error_handler(e, f\"Error hook {hook_name}\")\n                if self._logger:\n                    self._logger.error(f\"Error hook {hook_name} failed: {e}\")\n</code></pre> <code>execute_finally(hooks: list[tuple[FinallyHook, Rule | None]], result: CallResult) -&gt; None</code> <code>async</code> \u00b6 <p>Execute finally hooks.</p> <p>Note: Finally hooks don't use rule matching - they always run.</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[tuple[FinallyHook, Rule | None]]</code> <p>List of (hook, rule) tuples (rule is ignored for finally hooks)</p> required <code>result</code> <code>CallResult</code> <p>The complete call result</p> required Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>async def execute_finally(\n    self, hooks: list[tuple[FinallyHook, Rule | None]], result: CallResult\n) -&gt; None:\n    \"\"\"\n    Execute finally hooks.\n\n    Note: Finally hooks don't use rule matching - they always run.\n\n    Args:\n        hooks: List of (hook, rule) tuples (rule is ignored for finally hooks)\n        result: The complete call result\n    \"\"\"\n    for hook, _rule in hooks:\n        # Finally hooks always run, ignore rule\n        try:\n            await hook(result)\n        except Exception as e:\n            hook_name = getattr(hook, \"__name__\", str(hook))\n            self._error_handler(e, f\"Finally hook {hook_name}\")\n            if self._logger:\n                self._logger.error(f\"Finally hook {hook_name} failed: {e}\")\n</code></pre>"},{"location":"api/#hookedllm.core.protocols","title":"<code>protocols</code>","text":"<p>Protocol definitions for hooks and dependency injection.</p> <p>These protocols define the interfaces that components must implement, following the Dependency Inversion Principle.</p> Classes\u00b6 <code>HookExecutor</code> \u00b6 <p>               Bases: <code>Protocol</code></p> <p>Protocol for executing hooks.</p> <p>Single Responsibility: Hook execution only, no storage.</p> Functions\u00b6 <code>execute_after(hooks: list[tuple[AfterHook, Rule | None]], call_input: CallInput, call_output: CallOutput, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Execute after hooks with rule matching.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>async def execute_after(\n    self,\n    hooks: list[tuple[AfterHook, Rule | None]],\n    call_input: CallInput,\n    call_output: CallOutput,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"Execute after hooks with rule matching.\"\"\"\n    ...\n</code></pre> <code>execute_before(hooks: list[tuple[BeforeHook, Rule | None]], call_input: CallInput, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Execute before hooks with rule matching.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>async def execute_before(\n    self,\n    hooks: list[tuple[BeforeHook, Rule | None]],\n    call_input: CallInput,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"Execute before hooks with rule matching.\"\"\"\n    ...\n</code></pre> <code>execute_error(hooks: list[tuple[ErrorHook, Rule | None]], call_input: CallInput, error: BaseException, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Execute error hooks with rule matching.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>async def execute_error(\n    self,\n    hooks: list[tuple[ErrorHook, Rule | None]],\n    call_input: CallInput,\n    error: BaseException,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"Execute error hooks with rule matching.\"\"\"\n    ...\n</code></pre> <code>execute_finally(hooks: list[tuple[FinallyHook, Rule | None]], result: CallResult) -&gt; None</code> <code>async</code> \u00b6 <p>Execute finally hooks.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>async def execute_finally(\n    self, hooks: list[tuple[FinallyHook, Rule | None]], result: CallResult\n) -&gt; None:\n    \"\"\"Execute finally hooks.\"\"\"\n    ...\n</code></pre> <code>Rule</code> \u00b6 <p>               Bases: <code>Protocol</code></p> <p>Protocol for hook execution rules.</p> <p>Rules determine whether a hook should execute based on the call input and context.</p> Functions\u00b6 <code>__and__(other: Rule) -&gt; Rule</code> \u00b6 <p>Combine rules with AND logic.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def __and__(self, other: Rule) -&gt; Rule:\n    \"\"\"Combine rules with AND logic.\"\"\"\n    ...\n</code></pre> <code>__invert__() -&gt; Rule</code> \u00b6 <p>Negate the rule (NOT logic).</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def __invert__(self) -&gt; Rule:\n    \"\"\"Negate the rule (NOT logic).\"\"\"\n    ...\n</code></pre> <code>__or__(other: Rule) -&gt; Rule</code> \u00b6 <p>Combine rules with OR logic.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def __or__(self, other: Rule) -&gt; Rule:\n    \"\"\"Combine rules with OR logic.\"\"\"\n    ...\n</code></pre> <code>matches(call_input: CallInput, context: CallContext) -&gt; bool</code> \u00b6 <p>Check if this rule matches the given call.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def matches(self, call_input: CallInput, context: CallContext) -&gt; bool:\n    \"\"\"Check if this rule matches the given call.\"\"\"\n    ...\n</code></pre> <code>ScopeHookStore</code> \u00b6 <p>               Bases: <code>Protocol</code></p> <p>Protocol for storing hooks in a scope.</p> <p>Single Responsibility: Storage only, no execution logic.</p> Functions\u00b6 <code>add_after(hook: AfterHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add an after hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def add_after(self, hook: AfterHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add an after hook with optional execution rule.\"\"\"\n    ...\n</code></pre> <code>add_before(hook: BeforeHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add a before hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def add_before(self, hook: BeforeHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add a before hook with optional execution rule.\"\"\"\n    ...\n</code></pre> <code>add_error(hook: ErrorHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add an error hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def add_error(self, hook: ErrorHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add an error hook with optional execution rule.\"\"\"\n    ...\n</code></pre> <code>add_finally(hook: FinallyHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add a finally hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def add_finally(self, hook: FinallyHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add a finally hook with optional execution rule.\"\"\"\n    ...\n</code></pre> <code>get_after_hooks() -&gt; list[tuple[AfterHook, Rule | None]]</code> \u00b6 <p>Get all after hooks with their rules.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_after_hooks(self) -&gt; list[tuple[AfterHook, Rule | None]]:\n    \"\"\"Get all after hooks with their rules.\"\"\"\n    ...\n</code></pre> <code>get_before_hooks() -&gt; list[tuple[BeforeHook, Rule | None]]</code> \u00b6 <p>Get all before hooks with their rules.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_before_hooks(self) -&gt; list[tuple[BeforeHook, Rule | None]]:\n    \"\"\"Get all before hooks with their rules.\"\"\"\n    ...\n</code></pre> <code>get_error_hooks() -&gt; list[tuple[ErrorHook, Rule | None]]</code> \u00b6 <p>Get all error hooks with their rules.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_error_hooks(self) -&gt; list[tuple[ErrorHook, Rule | None]]:\n    \"\"\"Get all error hooks with their rules.\"\"\"\n    ...\n</code></pre> <code>get_finally_hooks() -&gt; list[tuple[FinallyHook, Rule | None]]</code> \u00b6 <p>Get all finally hooks with their rules.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_finally_hooks(self) -&gt; list[tuple[FinallyHook, Rule | None]]:\n    \"\"\"Get all finally hooks with their rules.\"\"\"\n    ...\n</code></pre> <code>ScopeRegistry</code> \u00b6 <p>               Bases: <code>Protocol</code></p> <p>Protocol for managing scopes.</p> <p>Single Responsibility: Scope lifecycle management only.</p> Functions\u00b6 <code>get_global_scope() -&gt; ScopeHookStore</code> \u00b6 <p>Get the global scope (always active).</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_global_scope(self) -&gt; ScopeHookStore:\n    \"\"\"Get the global scope (always active).\"\"\"\n    ...\n</code></pre> <code>get_scope(name: str) -&gt; ScopeHookStore</code> \u00b6 <p>Get or create a named scope.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_scope(self, name: str) -&gt; ScopeHookStore:\n    \"\"\"Get or create a named scope.\"\"\"\n    ...\n</code></pre> <code>get_scopes_for_client(scope_names: list[str] | None) -&gt; list[ScopeHookStore]</code> \u00b6 <p>Get list of scopes for a client.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_scopes_for_client(self, scope_names: list[str] | None) -&gt; list[ScopeHookStore]:\n    \"\"\"Get list of scopes for a client.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.rules","title":"<code>rules</code>","text":"<p>Rule system for conditional hook execution.</p> <p>Rules determine when hooks should execute based on call input and context. Supports composition via AND, OR, NOT operations.</p> Classes\u00b6 <code>CompositeRule(rules: list[Any], operator: Literal['and', 'or'])</code> <code>dataclass</code> \u00b6 <p>Combine multiple rules with AND/OR logic.</p> <code>CustomRule(predicate: Callable[[CallInput, CallContext], bool])</code> <code>dataclass</code> \u00b6 <p>Custom predicate function.</p> <code>MetadataRule(conditions: dict[str, Any])</code> <code>dataclass</code> \u00b6 <p>Match based on metadata key-value pairs.</p> <code>ModelRule(models: list[str])</code> <code>dataclass</code> \u00b6 <p>Match specific model(s).</p> <code>NotRule(rule: Any)</code> <code>dataclass</code> \u00b6 <p>Negate a rule.</p> <code>RuleBuilder</code> \u00b6 <p>Fluent API for building rules.</p> <p>Used via the 'when' global instance.</p> Functions\u00b6 <code>always() -&gt; CustomRule</code> <code>staticmethod</code> \u00b6 <p>Always matches.</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef always() -&gt; CustomRule:\n    \"\"\"Always matches.\"\"\"\n    return CustomRule(lambda i, c: True)\n</code></pre> <code>custom(predicate: Callable[[CallInput, CallContext], bool]) -&gt; CustomRule</code> <code>staticmethod</code> \u00b6 <p>Custom predicate function.</p> Example <p>when.custom(lambda i, c: c.metadata.get(\"score\", 0) &gt; 0.8)</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef custom(predicate: Callable[[CallInput, CallContext], bool]) -&gt; CustomRule:\n    \"\"\"\n    Custom predicate function.\n\n    Example:\n        when.custom(lambda i, c: c.metadata.get(\"score\", 0) &gt; 0.8)\n    \"\"\"\n    return CustomRule(predicate)\n</code></pre> <code>metadata(**conditions: Any) -&gt; MetadataRule</code> <code>staticmethod</code> \u00b6 <p>Match metadata conditions.</p> Example <p>when.metadata(user_tier=\"premium\", region=\"us-east\")</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef metadata(**conditions: Any) -&gt; MetadataRule:\n    \"\"\"\n    Match metadata conditions.\n\n    Example:\n        when.metadata(user_tier=\"premium\", region=\"us-east\")\n    \"\"\"\n    return MetadataRule(conditions)\n</code></pre> <code>model(*models: str) -&gt; ModelRule</code> <code>staticmethod</code> \u00b6 <p>Match specific model(s).</p> Example <p>when.model(\"gpt-4\", \"gpt-4-turbo\")</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef model(*models: str) -&gt; ModelRule:\n    \"\"\"\n    Match specific model(s).\n\n    Example:\n        when.model(\"gpt-4\", \"gpt-4-turbo\")\n    \"\"\"\n    return ModelRule(list(models))\n</code></pre> <code>never() -&gt; CustomRule</code> <code>staticmethod</code> \u00b6 <p>Never matches.</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef never() -&gt; CustomRule:\n    \"\"\"Never matches.\"\"\"\n    return CustomRule(lambda i, c: False)\n</code></pre> <code>tag(*tags: str, all_: bool = False) -&gt; TagRule</code> <code>staticmethod</code> \u00b6 <p>Match if context has tag(s).</p> <p>Parameters:</p> Name Type Description Default <code>*tags</code> <code>str</code> <p>Tag names to match</p> <code>()</code> <code>all_</code> <code>bool</code> <p>If True, all tags must be present. If False, any tag matches.</p> <code>False</code> Example <p>when.tag(\"production\", \"critical\")</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef tag(*tags: str, all_: bool = False) -&gt; TagRule:\n    \"\"\"\n    Match if context has tag(s).\n\n    Args:\n        *tags: Tag names to match\n        all_: If True, all tags must be present. If False, any tag matches.\n\n    Example:\n        when.tag(\"production\", \"critical\")\n    \"\"\"\n    return TagRule(list(tags), require_all=all_)\n</code></pre> <code>TagRule(tags: list[str], require_all: bool = False)</code> <code>dataclass</code> \u00b6 <p>Match if context has specific tag(s).</p>"},{"location":"api/#hookedllm.core.scopes","title":"<code>scopes</code>","text":"<p>Scope management for isolated hook execution.</p> <p>Scopes allow hooks to be registered and executed only for specific clients, preventing interference across different application contexts.</p> Classes\u00b6 <code>InMemoryScopeHookStore(scope_name: str)</code> \u00b6 <p>Concrete implementation of ScopeHookStore.</p> <p>Single Responsibility: Only stores hooks. Stores hooks in memory with their associated rules.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def __init__(self, scope_name: str):\n    self._scope_name = scope_name\n    self._before: list[tuple[BeforeHook, Rule | None]] = []\n    self._after: list[tuple[AfterHook, Rule | None]] = []\n    self._error: list[tuple[ErrorHook, Rule | None]] = []\n    self._finally: list[tuple[FinallyHook, Rule | None]] = []\n</code></pre> Attributes\u00b6 <code>name: str</code> <code>property</code> \u00b6 <p>Get the scope name.</p> Functions\u00b6 <code>add_after(hook: AfterHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add an after hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def add_after(self, hook: AfterHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add an after hook with optional execution rule.\"\"\"\n    self._after.append((hook, rule))\n</code></pre> <code>add_before(hook: BeforeHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add a before hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def add_before(self, hook: BeforeHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add a before hook with optional execution rule.\"\"\"\n    self._before.append((hook, rule))\n</code></pre> <code>add_error(hook: ErrorHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add an error hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def add_error(self, hook: ErrorHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add an error hook with optional execution rule.\"\"\"\n    self._error.append((hook, rule))\n</code></pre> <code>add_finally(hook: FinallyHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add a finally hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def add_finally(self, hook: FinallyHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add a finally hook with optional execution rule.\"\"\"\n    self._finally.append((hook, rule))\n</code></pre> <code>after(hook: AfterHook, *, when: Rule | None = None) -&gt; None</code> \u00b6 <p>Alias for add_after with keyword 'when' parameter.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def after(self, hook: AfterHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"Alias for add_after with keyword 'when' parameter.\"\"\"\n    self.add_after(hook, when)\n</code></pre> <code>before(hook: BeforeHook, *, when: Rule | None = None) -&gt; None</code> \u00b6 <p>Alias for add_before with keyword 'when' parameter.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def before(self, hook: BeforeHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"Alias for add_before with keyword 'when' parameter.\"\"\"\n    self.add_before(hook, when)\n</code></pre> <code>error(hook: ErrorHook, *, when: Rule | None = None) -&gt; None</code> \u00b6 <p>Alias for add_error with keyword 'when' parameter.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def error(self, hook: ErrorHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"Alias for add_error with keyword 'when' parameter.\"\"\"\n    self.add_error(hook, when)\n</code></pre> <code>finally_(hook: FinallyHook, *, when: Rule | None = None) -&gt; None</code> \u00b6 <p>Alias for add_finally with keyword 'when' parameter.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def finally_(self, hook: FinallyHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"Alias for add_finally with keyword 'when' parameter.\"\"\"\n    self.add_finally(hook, when)\n</code></pre> <code>get_after_hooks() -&gt; list[tuple[AfterHook, Rule | None]]</code> \u00b6 <p>Get all after hooks with their rules (returns copy).</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_after_hooks(self) -&gt; list[tuple[AfterHook, Rule | None]]:\n    \"\"\"Get all after hooks with their rules (returns copy).\"\"\"\n    return self._after.copy()\n</code></pre> <code>get_before_hooks() -&gt; list[tuple[BeforeHook, Rule | None]]</code> \u00b6 <p>Get all before hooks with their rules (returns copy).</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_before_hooks(self) -&gt; list[tuple[BeforeHook, Rule | None]]:\n    \"\"\"Get all before hooks with their rules (returns copy).\"\"\"\n    return self._before.copy()\n</code></pre> <code>get_error_hooks() -&gt; list[tuple[ErrorHook, Rule | None]]</code> \u00b6 <p>Get all error hooks with their rules (returns copy).</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_error_hooks(self) -&gt; list[tuple[ErrorHook, Rule | None]]:\n    \"\"\"Get all error hooks with their rules (returns copy).\"\"\"\n    return self._error.copy()\n</code></pre> <code>get_finally_hooks() -&gt; list[tuple[FinallyHook, Rule | None]]</code> \u00b6 <p>Get all finally hooks with their rules (returns copy).</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_finally_hooks(self) -&gt; list[tuple[FinallyHook, Rule | None]]:\n    \"\"\"Get all finally hooks with their rules (returns copy).\"\"\"\n    return self._finally.copy()\n</code></pre> <code>InMemoryScopeRegistry()</code> \u00b6 <p>Concrete implementation of ScopeRegistry.</p> <p>Single Responsibility: Only manages scope lifecycle. Creates and retrieves scopes, manages global scope.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def __init__(self):\n    self._scopes: dict[str, InMemoryScopeHookStore] = {}\n    self._global = InMemoryScopeHookStore(\"__global__\")\n</code></pre> Functions\u00b6 <code>get_global_scope() -&gt; InMemoryScopeHookStore</code> \u00b6 <p>Get the global scope (always active).</p> <p>Returns:</p> Type Description <code>InMemoryScopeHookStore</code> <p>The global scope hook store</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_global_scope(self) -&gt; InMemoryScopeHookStore:\n    \"\"\"\n    Get the global scope (always active).\n\n    Returns:\n        The global scope hook store\n    \"\"\"\n    return self._global\n</code></pre> <code>get_scope(name: str) -&gt; InMemoryScopeHookStore</code> \u00b6 <p>Get or create a named scope.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Scope name</p> required <p>Returns:</p> Type Description <code>InMemoryScopeHookStore</code> <p>Scope hook store for the named scope</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_scope(self, name: str) -&gt; InMemoryScopeHookStore:\n    \"\"\"\n    Get or create a named scope.\n\n    Args:\n        name: Scope name\n\n    Returns:\n        Scope hook store for the named scope\n    \"\"\"\n    if name not in self._scopes:\n        self._scopes[name] = InMemoryScopeHookStore(name)\n    return self._scopes[name]\n</code></pre> <code>get_scopes_for_client(scope_names: list[str] | None = None) -&gt; list[ScopeHookStore]</code> \u00b6 <p>Get list of scopes for a client.</p> <p>Always includes the global scope, plus any requested scopes.</p> <p>Parameters:</p> Name Type Description Default <code>scope_names</code> <code>list[str] | None</code> <p>List of scope names, or None for global only</p> <code>None</code> <p>Returns:</p> Type Description <code>list[ScopeHookStore]</code> <p>List of scope hook stores (global + requested scopes)</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_scopes_for_client(self, scope_names: list[str] | None = None) -&gt; list[ScopeHookStore]:\n    \"\"\"\n    Get list of scopes for a client.\n\n    Always includes the global scope, plus any requested scopes.\n\n    Args:\n        scope_names: List of scope names, or None for global only\n\n    Returns:\n        List of scope hook stores (global + requested scopes)\n    \"\"\"\n    scopes: list[ScopeHookStore] = [self._global]  # Always include global\n\n    if scope_names:\n        for name in scope_names:\n            scopes.append(self.get_scope(name))\n\n    return scopes\n</code></pre> <code>scope(name: str) -&gt; InMemoryScopeHookStore</code> \u00b6 <p>Alias for get_scope for more fluent API.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def scope(self, name: str) -&gt; InMemoryScopeHookStore:\n    \"\"\"Alias for get_scope for more fluent API.\"\"\"\n    return self.get_scope(name)\n</code></pre>"},{"location":"api/#hookedllm.core.types","title":"<code>types</code>","text":"<p>Core data types for hookedllm.</p> <p>These types represent the data that flows through the hook system.</p> Classes\u00b6 <code>CallContext(call_id: str = (lambda: str(uuid4()))(), parent_id: str | None = None, provider: str = '', model: str = '', route: str = 'chat', tags: list[str] = list(), started_at: datetime = (lambda: datetime.now(timezone.utc))(), metadata: dict[str, Any] = dict())</code> <code>dataclass</code> \u00b6 <p>Context for a single LLM call lifecycle.</p> <p>Contains metadata about the call including timing, tags, and custom metadata.</p> <code>CallInput(model: str, messages: Sequence[Message], params: dict[str, Any] = dict(), metadata: dict[str, Any] = dict())</code> <code>dataclass</code> \u00b6 <p>Normalized input for an LLM call.</p> <p>This represents the input parameters in a provider-agnostic way.</p> <code>CallOutput(text: str | None, raw: Any, usage: dict[str, Any] | None = None, finish_reason: str | None = None)</code> <code>dataclass</code> \u00b6 <p>Normalized output from an LLM call.</p> <p>This represents the response in a provider-agnostic way while preserving the original response object.</p> <code>CallResult(input: CallInput, output: CallOutput | None, context: CallContext, error: BaseException | None, ended_at: datetime, elapsed_ms: float)</code> <code>dataclass</code> \u00b6 <p>Complete result of an LLM call.</p> <p>Contains the input, output, context, any error that occurred, and timing information. This is passed to finally hooks.</p> <code>Message(role: str, content: Any)</code> <code>dataclass</code> \u00b6 <p>A single message in an LLM conversation.</p>"},{"location":"api/#hookedllm.core.wrapper","title":"<code>wrapper</code>","text":"<p>Transparent wrapper for intercepting LLM API calls.</p> <p>Wraps provider clients to inject hook execution while preserving the original SDK interface and return types.</p> Classes\u00b6 <code>HookedClientWrapper(original_client: Any, scopes: list[ScopeHookStore], executor: HookExecutor)</code> \u00b6 <p>Transparent proxy with all dependencies injected.</p> <p>No global state - all dependencies passed via constructor (DI). Intercepts provider SDK methods to inject hook execution.</p> <p>Initialize wrapper with injected dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>original_client</code> <code>Any</code> <p>The original provider client</p> required <code>scopes</code> <code>list[ScopeHookStore]</code> <p>List of scope hook stores to use</p> required <code>executor</code> <code>HookExecutor</code> <p>Hook executor instance</p> required Source code in <code>src/hookedllm/core/wrapper.py</code> <pre><code>def __init__(self, original_client: Any, scopes: list[ScopeHookStore], executor: HookExecutor):\n    \"\"\"\n    Initialize wrapper with injected dependencies.\n\n    Args:\n        original_client: The original provider client\n        scopes: List of scope hook stores to use\n        executor: Hook executor instance\n    \"\"\"\n    self._original = original_client\n    self._scopes = scopes\n    self._executor = executor\n    self._adapter = _detect_provider_adapter(original_client)\n    self._wrapper_path = self._adapter.get_wrapper_path(original_client)\n</code></pre> Functions\u00b6 <code>__getattr__(name: str) -&gt; Any</code> \u00b6 <p>Intercept attribute access.</p> <p>Wraps attributes based on the detected provider's wrapper path.</p> Source code in <code>src/hookedllm/core/wrapper.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"\n    Intercept attribute access.\n\n    Wraps attributes based on the detected provider's wrapper path.\n    \"\"\"\n    attr = getattr(self._original, name)\n\n    # Check if this attribute is part of the wrapper path\n    if len(self._wrapper_path) &gt; 0 and name == self._wrapper_path[0]:\n        # Create a dynamic wrapper for the next level\n        return _create_wrapper_for_path(\n            attr, self._wrapper_path[1:], self._scopes, self._executor, self._adapter\n        )\n\n    return attr\n</code></pre> <code>HookedCompletionsWrapper(original_completions: Any, scopes: list[ScopeHookStore], executor: HookExecutor, adapter: Any)</code> \u00b6 <p>Wraps completions.create() with hook execution.</p> <p>All dependencies injected, no global state.</p> <p>Initialize completions wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>original_completions</code> <code>Any</code> <p>Original completions object from SDK</p> required <code>scopes</code> <code>list[ScopeHookStore]</code> <p>List of scope hook stores</p> required <code>executor</code> <code>HookExecutor</code> <p>Hook executor instance</p> required <code>adapter</code> <code>Any</code> <p>Provider adapter instance</p> required Source code in <code>src/hookedllm/core/wrapper.py</code> <pre><code>def __init__(\n    self,\n    original_completions: Any,\n    scopes: list[ScopeHookStore],\n    executor: HookExecutor,\n    adapter: Any,\n):\n    \"\"\"\n    Initialize completions wrapper.\n\n    Args:\n        original_completions: Original completions object from SDK\n        scopes: List of scope hook stores\n        executor: Hook executor instance\n        adapter: Provider adapter instance\n    \"\"\"\n    self._original = original_completions\n    self._scopes = scopes\n    self._executor = executor\n    self._adapter = adapter\n</code></pre> Functions\u00b6 <code>__getattr__(name: str) -&gt; Any</code> \u00b6 <p>Pass through other attributes to original object.</p> Source code in <code>src/hookedllm/core/wrapper.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"Pass through other attributes to original object.\"\"\"\n    return getattr(self._original, name)\n</code></pre> <code>create(*, model: str, messages: list[dict], **kwargs: Any) -&gt; Any</code> <code>async</code> \u00b6 <p>Hooked create method.</p> <p>Flow: 1. Use adapter to normalize input (extracts tags, metadata, creates CallInput/Context) 2. Collect all hooks from all scopes 3. Execute before hooks 4. Call original SDK method 5. Use adapter to normalize output 6. Execute after hooks (on success) or error hooks (on failure) 7. Always execute finally hooks 8. Return original SDK response type</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name</p> required <code>messages</code> <code>list[dict]</code> <p>List of message dicts</p> required <code>**kwargs</code> <code>Any</code> <p>Other parameters passed to SDK</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Original SDK response object</p> Source code in <code>src/hookedllm/core/wrapper.py</code> <pre><code>async def create(self, *, model: str, messages: list[dict], **kwargs: Any) -&gt; Any:\n    \"\"\"\n    Hooked create method.\n\n    Flow:\n    1. Use adapter to normalize input (extracts tags, metadata, creates CallInput/Context)\n    2. Collect all hooks from all scopes\n    3. Execute before hooks\n    4. Call original SDK method\n    5. Use adapter to normalize output\n    6. Execute after hooks (on success) or error hooks (on failure)\n    7. Always execute finally hooks\n    8. Return original SDK response type\n\n    Args:\n        model: Model name\n        messages: List of message dicts\n        **kwargs: Other parameters passed to SDK\n\n    Returns:\n        Original SDK response object\n    \"\"\"\n    # Use adapter to normalize input\n    call_input, context = self._adapter.normalize_input(\n        self._adapter.PROVIDER_NAME,\n        self._original.create,\n        model=model,\n        messages=messages,\n        **kwargs,\n    )\n\n    # Collect all hooks from all scopes\n    all_before = []\n    all_after = []\n    all_error = []\n    all_finally = []\n\n    for scope in self._scopes:\n        all_before.extend(scope.get_before_hooks())\n        all_after.extend(scope.get_after_hooks())\n        all_error.extend(scope.get_error_hooks())\n        all_finally.extend(scope.get_finally_hooks())\n\n    # Execute hook flow\n    t0 = time.perf_counter()\n    output = None\n    error = None\n\n    try:\n        # Before hooks\n        await self._executor.execute_before(all_before, call_input, context)\n\n        # Original SDK call\n        response = await self._original.create(model=model, messages=messages, **kwargs)\n\n        # Use adapter to normalize output\n        output = self._adapter.normalize_output(response)\n\n        # After hooks\n        await self._executor.execute_after(all_after, call_input, output, context)\n\n        return response  # Return ORIGINAL SDK response!\n\n    except BaseException as e:\n        error = e\n        await self._executor.execute_error(all_error, call_input, e, context)\n        raise\n\n    finally:\n        elapsed = (time.perf_counter() - t0) * 1000.0\n        result = CallResult(\n            input=call_input,\n            output=output,\n            context=context,\n            error=error,\n            ended_at=datetime.now(timezone.utc),\n            elapsed_ms=elapsed,\n        )\n        await self._executor.execute_finally(all_finally, result)\n</code></pre> <code>HookedPathWrapper(original_obj: Any, remaining_path: list[str], scopes: list[ScopeHookStore], executor: HookExecutor, adapter: Any)</code> \u00b6 <p>Intermediate wrapper for provider-specific attribute paths.</p> <p>Handles paths like [\"chat\", \"completions\"] for OpenAI or [\"messages\"] for Anthropic.</p> <p>Initialize path wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>original_obj</code> <code>Any</code> <p>The object being wrapped</p> required <code>remaining_path</code> <code>list[str]</code> <p>Remaining attribute names to intercept</p> required <code>scopes</code> <code>list[ScopeHookStore]</code> <p>List of scope hook stores</p> required <code>executor</code> <code>HookExecutor</code> <p>Hook executor instance</p> required <code>adapter</code> <code>Any</code> <p>Provider adapter instance</p> required Source code in <code>src/hookedllm/core/wrapper.py</code> <pre><code>def __init__(\n    self,\n    original_obj: Any,\n    remaining_path: list[str],\n    scopes: list[ScopeHookStore],\n    executor: HookExecutor,\n    adapter: Any,\n):\n    \"\"\"\n    Initialize path wrapper.\n\n    Args:\n        original_obj: The object being wrapped\n        remaining_path: Remaining attribute names to intercept\n        scopes: List of scope hook stores\n        executor: Hook executor instance\n        adapter: Provider adapter instance\n    \"\"\"\n    self._original = original_obj\n    self._remaining_path = remaining_path\n    self._scopes = scopes\n    self._executor = executor\n    self._adapter = adapter\n</code></pre> Functions\u00b6 <code>__getattr__(name: str) -&gt; Any</code> \u00b6 <p>Intercept attribute access.</p> <p>If the attribute matches the next in the path, wrap it. Otherwise pass through.</p> Source code in <code>src/hookedllm/core/wrapper.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"\n    Intercept attribute access.\n\n    If the attribute matches the next in the path, wrap it.\n    Otherwise pass through.\n    \"\"\"\n    attr = getattr(self._original, name)\n\n    if len(self._remaining_path) &gt; 0 and name == self._remaining_path[0]:\n        # Continue wrapping down the path\n        return _create_wrapper_for_path(\n            attr, self._remaining_path[1:], self._scopes, self._executor, self._adapter\n        )\n\n    return attr\n</code></pre>"},{"location":"api/#hookedllm.hooks","title":"<code>hooks</code>","text":"<p>Built-in hook helpers for common use cases.</p>"},{"location":"api/#hookedllm.hooks-classes","title":"Classes","text":""},{"location":"api/#hookedllm.hooks.EvaluationHook","title":"<code>EvaluationHook(evaluator_client: Any, criteria: dict[str, str], model: str = 'gpt-4o-mini', store_in_metadata: bool = True)</code>","text":"<p>Evaluate LLM responses using another LLM.</p> <p>This is an after hook that calls a separate \"evaluator\" LLM to assess the quality of responses based on configurable criteria.</p> Usage <p>from openai import AsyncOpenAI</p> <p>evaluator = AsyncOpenAI()  # Separate client for evaluation criteria = {     \"clarity\": \"Is the response clear and easy to understand?\",     \"accuracy\": \"Is the response factually accurate?\",     \"relevance\": \"Does the response address the user's question?\" }</p> <p>eval_hook = EvaluationHook(evaluator, criteria) hookedllm.scope(\"evaluation\").after(eval_hook)</p> <p>Initialize evaluation hook.</p> <p>Parameters:</p> Name Type Description Default <code>evaluator_client</code> <code>Any</code> <p>OpenAI-compatible client for evaluation calls</p> required <code>criteria</code> <code>dict[str, str]</code> <p>Dict mapping criterion name to description</p> required <code>model</code> <code>str</code> <p>Model to use for evaluation (default: gpt-4o-mini)</p> <code>'gpt-4o-mini'</code> <code>store_in_metadata</code> <code>bool</code> <p>If True, store results in context.metadata</p> <code>True</code> Source code in <code>src/hookedllm/hooks/evaluation.py</code> <pre><code>def __init__(\n    self,\n    evaluator_client: Any,\n    criteria: dict[str, str],\n    model: str = \"gpt-4o-mini\",\n    store_in_metadata: bool = True,\n):\n    \"\"\"\n    Initialize evaluation hook.\n\n    Args:\n        evaluator_client: OpenAI-compatible client for evaluation calls\n        criteria: Dict mapping criterion name to description\n        model: Model to use for evaluation (default: gpt-4o-mini)\n        store_in_metadata: If True, store results in context.metadata\n    \"\"\"\n    self.evaluator = evaluator_client\n    self.criteria = criteria\n    self.model = model\n    self.store_in_metadata = store_in_metadata\n</code></pre> Functions\u00b6 <code>__call__(call_input: CallInput, call_output: CallOutput, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Evaluate the LLM response.</p> <p>Parameters:</p> Name Type Description Default <code>call_input</code> <code>CallInput</code> <p>The original call input</p> required <code>call_output</code> <code>CallOutput</code> <p>The LLM response</p> required <code>context</code> <code>CallContext</code> <p>The call context</p> required Source code in <code>src/hookedllm/hooks/evaluation.py</code> <pre><code>async def __call__(\n    self, call_input: CallInput, call_output: CallOutput, context: CallContext\n) -&gt; None:\n    \"\"\"\n    Evaluate the LLM response.\n\n    Args:\n        call_input: The original call input\n        call_output: The LLM response\n        context: The call context\n    \"\"\"\n    if not call_output.text:\n        # Nothing to evaluate\n        return\n\n    # Extract the original query\n    original_query = self._extract_query(call_input)\n\n    # Build evaluation prompt\n    eval_prompt = self._build_evaluation_prompt(original_query, call_output.text)\n\n    try:\n        # Call evaluator\n        eval_response = await self.evaluator.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": eval_prompt}],\n            temperature=0.0,  # Deterministic evaluation\n        )\n\n        # Extract evaluation result\n        eval_text = eval_response.choices[0].message.content\n\n        # Try to parse as JSON\n        try:\n            eval_result = json.loads(eval_text)\n        except json.JSONDecodeError:\n            # If not JSON, store raw text\n            eval_result = {\"raw_evaluation\": eval_text}\n\n        # Store in context if requested\n        if self.store_in_metadata:\n            context.metadata[\"evaluation\"] = eval_result\n            context.metadata[\"evaluation_model\"] = self.model\n\n    except Exception as e:\n        # Evaluation failed - don't break the main flow\n        if self.store_in_metadata:\n            context.metadata[\"evaluation_error\"] = str(e)\n</code></pre>"},{"location":"api/#hookedllm.hooks.MetricsHook","title":"<code>MetricsHook(stats: dict[str, Any] | None = None)</code>","text":"<p>Track metrics across LLM calls.</p> <p>This is a finally hook that aggregates metrics including: - Total calls - Total tokens used - Error count - Average latency</p> Usage <p>metrics = MetricsHook() hookedllm.finally_(metrics)</p> <p>Initialize metrics hook.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>dict[str, Any] | None</code> <p>Optional existing stats dict to update.    If None, creates a new dict.</p> <code>None</code> Source code in <code>src/hookedllm/hooks/metrics.py</code> <pre><code>def __init__(self, stats: dict[str, Any] | None = None):\n    \"\"\"\n    Initialize metrics hook.\n\n    Args:\n        stats: Optional existing stats dict to update.\n               If None, creates a new dict.\n    \"\"\"\n    if stats is None:\n        self.stats = {\n            \"total_calls\": 0,\n            \"successful_calls\": 0,\n            \"failed_calls\": 0,\n            \"total_tokens\": 0,\n            \"prompt_tokens\": 0,\n            \"completion_tokens\": 0,\n            \"total_latency_ms\": 0.0,\n        }\n    else:\n        self.stats = stats\n</code></pre> Attributes\u00b6 <code>average_latency_ms: float</code> <code>property</code> \u00b6 <p>Calculate average latency.</p> <code>error_rate: float</code> <code>property</code> \u00b6 <p>Calculate error rate (0.0 to 1.0).</p> <code>success_rate: float</code> <code>property</code> \u00b6 <p>Calculate success rate (0.0 to 1.0).</p> Functions\u00b6 <code>__call__(result: CallResult) -&gt; None</code> <code>async</code> \u00b6 <p>Update metrics based on call result.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>CallResult</code> <p>The complete call result</p> required Source code in <code>src/hookedllm/hooks/metrics.py</code> <pre><code>async def __call__(self, result: CallResult) -&gt; None:\n    \"\"\"\n    Update metrics based on call result.\n\n    Args:\n        result: The complete call result\n    \"\"\"\n    # Increment total calls\n    self.stats[\"total_calls\"] += 1\n\n    # Track success/failure\n    if result.error is None:\n        self.stats[\"successful_calls\"] += 1\n    else:\n        self.stats[\"failed_calls\"] += 1\n\n    # Track tokens\n    if result.output and result.output.usage:\n        usage = result.output.usage\n        self.stats[\"total_tokens\"] += usage.get(\"total_tokens\", 0)\n        self.stats[\"prompt_tokens\"] += usage.get(\"prompt_tokens\", 0)\n        self.stats[\"completion_tokens\"] += usage.get(\"completion_tokens\", 0)\n\n    # Track latency\n    self.stats[\"total_latency_ms\"] += result.elapsed_ms\n</code></pre> <code>reset() -&gt; None</code> \u00b6 <p>Reset all metrics to zero.</p> Source code in <code>src/hookedllm/hooks/metrics.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset all metrics to zero.\"\"\"\n    for key in self.stats:\n        self.stats[key] = 0 if isinstance(self.stats[key], int) else 0.0\n</code></pre> <code>summary() -&gt; dict[str, Any]</code> \u00b6 <p>Get a summary of metrics including calculated values.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with all metrics plus calculated averages</p> Source code in <code>src/hookedllm/hooks/metrics.py</code> <pre><code>def summary(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get a summary of metrics including calculated values.\n\n    Returns:\n        Dict with all metrics plus calculated averages\n    \"\"\"\n    return {\n        **self.stats,\n        \"average_latency_ms\": self.average_latency_ms,\n        \"success_rate\": self.success_rate,\n        \"error_rate\": self.error_rate,\n    }\n</code></pre>"},{"location":"api/#hookedllm.hooks.MetricsHook--later-access-metrics","title":"Later, access metrics","text":"<p>print(metrics.stats)</p>"},{"location":"api/#hookedllm.hooks-modules","title":"Modules","text":""},{"location":"api/#hookedllm.hooks.evaluation","title":"<code>evaluation</code>","text":"<p>Built-in evaluation hook helper.</p> <p>Provides a helper for evaluating LLM responses using another LLM.</p> Classes\u00b6 <code>EvaluationHook(evaluator_client: Any, criteria: dict[str, str], model: str = 'gpt-4o-mini', store_in_metadata: bool = True)</code> \u00b6 <p>Evaluate LLM responses using another LLM.</p> <p>This is an after hook that calls a separate \"evaluator\" LLM to assess the quality of responses based on configurable criteria.</p> Usage <p>from openai import AsyncOpenAI</p> <p>evaluator = AsyncOpenAI()  # Separate client for evaluation criteria = {     \"clarity\": \"Is the response clear and easy to understand?\",     \"accuracy\": \"Is the response factually accurate?\",     \"relevance\": \"Does the response address the user's question?\" }</p> <p>eval_hook = EvaluationHook(evaluator, criteria) hookedllm.scope(\"evaluation\").after(eval_hook)</p> <p>Initialize evaluation hook.</p> <p>Parameters:</p> Name Type Description Default <code>evaluator_client</code> <code>Any</code> <p>OpenAI-compatible client for evaluation calls</p> required <code>criteria</code> <code>dict[str, str]</code> <p>Dict mapping criterion name to description</p> required <code>model</code> <code>str</code> <p>Model to use for evaluation (default: gpt-4o-mini)</p> <code>'gpt-4o-mini'</code> <code>store_in_metadata</code> <code>bool</code> <p>If True, store results in context.metadata</p> <code>True</code> Source code in <code>src/hookedllm/hooks/evaluation.py</code> <pre><code>def __init__(\n    self,\n    evaluator_client: Any,\n    criteria: dict[str, str],\n    model: str = \"gpt-4o-mini\",\n    store_in_metadata: bool = True,\n):\n    \"\"\"\n    Initialize evaluation hook.\n\n    Args:\n        evaluator_client: OpenAI-compatible client for evaluation calls\n        criteria: Dict mapping criterion name to description\n        model: Model to use for evaluation (default: gpt-4o-mini)\n        store_in_metadata: If True, store results in context.metadata\n    \"\"\"\n    self.evaluator = evaluator_client\n    self.criteria = criteria\n    self.model = model\n    self.store_in_metadata = store_in_metadata\n</code></pre> Functions\u00b6 <code>__call__(call_input: CallInput, call_output: CallOutput, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Evaluate the LLM response.</p> <p>Parameters:</p> Name Type Description Default <code>call_input</code> <code>CallInput</code> <p>The original call input</p> required <code>call_output</code> <code>CallOutput</code> <p>The LLM response</p> required <code>context</code> <code>CallContext</code> <p>The call context</p> required Source code in <code>src/hookedllm/hooks/evaluation.py</code> <pre><code>async def __call__(\n    self, call_input: CallInput, call_output: CallOutput, context: CallContext\n) -&gt; None:\n    \"\"\"\n    Evaluate the LLM response.\n\n    Args:\n        call_input: The original call input\n        call_output: The LLM response\n        context: The call context\n    \"\"\"\n    if not call_output.text:\n        # Nothing to evaluate\n        return\n\n    # Extract the original query\n    original_query = self._extract_query(call_input)\n\n    # Build evaluation prompt\n    eval_prompt = self._build_evaluation_prompt(original_query, call_output.text)\n\n    try:\n        # Call evaluator\n        eval_response = await self.evaluator.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": eval_prompt}],\n            temperature=0.0,  # Deterministic evaluation\n        )\n\n        # Extract evaluation result\n        eval_text = eval_response.choices[0].message.content\n\n        # Try to parse as JSON\n        try:\n            eval_result = json.loads(eval_text)\n        except json.JSONDecodeError:\n            # If not JSON, store raw text\n            eval_result = {\"raw_evaluation\": eval_text}\n\n        # Store in context if requested\n        if self.store_in_metadata:\n            context.metadata[\"evaluation\"] = eval_result\n            context.metadata[\"evaluation_model\"] = self.model\n\n    except Exception as e:\n        # Evaluation failed - don't break the main flow\n        if self.store_in_metadata:\n            context.metadata[\"evaluation_error\"] = str(e)\n</code></pre>"},{"location":"api/#hookedllm.hooks.metrics","title":"<code>metrics</code>","text":"<p>Built-in metrics tracking hook.</p> <p>Tracks token usage, call counts, and error rates across LLM calls.</p> Classes\u00b6 <code>MetricsHook(stats: dict[str, Any] | None = None)</code> \u00b6 <p>Track metrics across LLM calls.</p> <p>This is a finally hook that aggregates metrics including: - Total calls - Total tokens used - Error count - Average latency</p> Usage <p>metrics = MetricsHook() hookedllm.finally_(metrics)</p> <p>Initialize metrics hook.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>dict[str, Any] | None</code> <p>Optional existing stats dict to update.    If None, creates a new dict.</p> <code>None</code> Source code in <code>src/hookedllm/hooks/metrics.py</code> <pre><code>def __init__(self, stats: dict[str, Any] | None = None):\n    \"\"\"\n    Initialize metrics hook.\n\n    Args:\n        stats: Optional existing stats dict to update.\n               If None, creates a new dict.\n    \"\"\"\n    if stats is None:\n        self.stats = {\n            \"total_calls\": 0,\n            \"successful_calls\": 0,\n            \"failed_calls\": 0,\n            \"total_tokens\": 0,\n            \"prompt_tokens\": 0,\n            \"completion_tokens\": 0,\n            \"total_latency_ms\": 0.0,\n        }\n    else:\n        self.stats = stats\n</code></pre> Attributes\u00b6 <code>average_latency_ms: float</code> <code>property</code> \u00b6 <p>Calculate average latency.</p> <code>error_rate: float</code> <code>property</code> \u00b6 <p>Calculate error rate (0.0 to 1.0).</p> <code>success_rate: float</code> <code>property</code> \u00b6 <p>Calculate success rate (0.0 to 1.0).</p> Functions\u00b6 <code>__call__(result: CallResult) -&gt; None</code> <code>async</code> \u00b6 <p>Update metrics based on call result.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>CallResult</code> <p>The complete call result</p> required Source code in <code>src/hookedllm/hooks/metrics.py</code> <pre><code>async def __call__(self, result: CallResult) -&gt; None:\n    \"\"\"\n    Update metrics based on call result.\n\n    Args:\n        result: The complete call result\n    \"\"\"\n    # Increment total calls\n    self.stats[\"total_calls\"] += 1\n\n    # Track success/failure\n    if result.error is None:\n        self.stats[\"successful_calls\"] += 1\n    else:\n        self.stats[\"failed_calls\"] += 1\n\n    # Track tokens\n    if result.output and result.output.usage:\n        usage = result.output.usage\n        self.stats[\"total_tokens\"] += usage.get(\"total_tokens\", 0)\n        self.stats[\"prompt_tokens\"] += usage.get(\"prompt_tokens\", 0)\n        self.stats[\"completion_tokens\"] += usage.get(\"completion_tokens\", 0)\n\n    # Track latency\n    self.stats[\"total_latency_ms\"] += result.elapsed_ms\n</code></pre> <code>reset() -&gt; None</code> \u00b6 <p>Reset all metrics to zero.</p> Source code in <code>src/hookedllm/hooks/metrics.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset all metrics to zero.\"\"\"\n    for key in self.stats:\n        self.stats[key] = 0 if isinstance(self.stats[key], int) else 0.0\n</code></pre> <code>summary() -&gt; dict[str, Any]</code> \u00b6 <p>Get a summary of metrics including calculated values.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with all metrics plus calculated averages</p> Source code in <code>src/hookedllm/hooks/metrics.py</code> <pre><code>def summary(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get a summary of metrics including calculated values.\n\n    Returns:\n        Dict with all metrics plus calculated averages\n    \"\"\"\n    return {\n        **self.stats,\n        \"average_latency_ms\": self.average_latency_ms,\n        \"success_rate\": self.success_rate,\n        \"error_rate\": self.error_rate,\n    }\n</code></pre>"},{"location":"api/#hookedllm.hooks.metrics.MetricsHook--later-access-metrics","title":"Later, access metrics","text":"<p>print(metrics.stats)</p>"},{"location":"api/#hookedllm.providers","title":"<code>providers</code>","text":"<p>Provider adapters for multi-provider support.</p> <p>Each adapter handles provider-specific logic for detecting clients, normalizing input/output, and extracting callable methods.</p>"},{"location":"api/#hookedllm.providers-classes","title":"Classes","text":""},{"location":"api/#hookedllm.providers.ProviderAdapter","title":"<code>ProviderAdapter</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for provider-specific adapters.</p> <p>Each adapter handles: - Detecting if a client belongs to this provider - Normalizing provider-specific input/output formats - Extracting the callable method from the client</p> <p>This follows the Open/Closed Principle: new providers can be added by implementing this protocol without modifying existing code.</p> Functions\u00b6 <code>detect(client: Any) -&gt; bool</code> <code>staticmethod</code> \u00b6 <p>Detect if a client belongs to this provider.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>The client instance to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if this adapter can handle the client, False otherwise</p> Source code in <code>src/hookedllm/providers/protocol.py</code> <pre><code>@staticmethod\ndef detect(client: Any) -&gt; bool:\n    \"\"\"\n    Detect if a client belongs to this provider.\n\n    Args:\n        client: The client instance to check\n\n    Returns:\n        True if this adapter can handle the client, False otherwise\n    \"\"\"\n    ...\n</code></pre> <code>get_callable(client: Any) -&gt; Any</code> <code>staticmethod</code> \u00b6 <p>Get the callable method from the client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>The client instance</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The callable method (e.g., client.chat.completions.create)</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the callable cannot be found</p> Source code in <code>src/hookedllm/providers/protocol.py</code> <pre><code>@staticmethod\ndef get_callable(client: Any) -&gt; Any:\n    \"\"\"\n    Get the callable method from the client.\n\n    Args:\n        client: The client instance\n\n    Returns:\n        The callable method (e.g., client.chat.completions.create)\n\n    Raises:\n        AttributeError: If the callable cannot be found\n    \"\"\"\n    ...\n</code></pre> <code>get_wrapper_path(client: Any) -&gt; list[str]</code> <code>staticmethod</code> \u00b6 <p>Get the attribute path to wrap (e.g., [\"chat\", \"completions\"]).</p> <p>This is used by the wrapper to intercept the correct attributes.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>The client instance</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of attribute names to wrap</p> Source code in <code>src/hookedllm/providers/protocol.py</code> <pre><code>@staticmethod\ndef get_wrapper_path(client: Any) -&gt; list[str]:\n    \"\"\"\n    Get the attribute path to wrap (e.g., [\"chat\", \"completions\"]).\n\n    This is used by the wrapper to intercept the correct attributes.\n\n    Args:\n        client: The client instance\n\n    Returns:\n        List of attribute names to wrap\n    \"\"\"\n    ...\n</code></pre> <code>normalize_input(provider_name: str, callable_method: Any, *args: Any, **kwargs: Any) -&gt; tuple[CallInput, CallContext]</code> <code>staticmethod</code> \u00b6 <p>Normalize provider-specific input to CallInput and CallContext.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Name of the provider (e.g., \"openai\", \"anthropic\")</p> required <code>callable_method</code> <code>Any</code> <p>The callable method being called</p> required <code>*args</code> <code>Any</code> <p>Positional arguments passed to the callable</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments passed to the callable</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[CallInput, CallContext]</code> <p>Tuple of (CallInput, CallContext)</p> Source code in <code>src/hookedllm/providers/protocol.py</code> <pre><code>@staticmethod\ndef normalize_input(\n    provider_name: str, callable_method: Any, *args: Any, **kwargs: Any\n) -&gt; tuple[CallInput, CallContext]:\n    \"\"\"\n    Normalize provider-specific input to CallInput and CallContext.\n\n    Args:\n        provider_name: Name of the provider (e.g., \"openai\", \"anthropic\")\n        callable_method: The callable method being called\n        *args: Positional arguments passed to the callable\n        **kwargs: Keyword arguments passed to the callable\n\n    Returns:\n        Tuple of (CallInput, CallContext)\n    \"\"\"\n    ...\n</code></pre> <code>normalize_output(response: Any) -&gt; CallOutput</code> <code>staticmethod</code> \u00b6 <p>Normalize provider-specific response to CallOutput.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Any</code> <p>The raw response from the provider SDK</p> required <p>Returns:</p> Type Description <code>CallOutput</code> <p>Normalized CallOutput</p> Source code in <code>src/hookedllm/providers/protocol.py</code> <pre><code>@staticmethod\ndef normalize_output(response: Any) -&gt; CallOutput:\n    \"\"\"\n    Normalize provider-specific response to CallOutput.\n\n    Args:\n        response: The raw response from the provider SDK\n\n    Returns:\n        Normalized CallOutput\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.providers-modules","title":"Modules","text":""},{"location":"api/#hookedllm.providers.anthropic","title":"<code>anthropic</code>","text":"<p>Anthropic provider adapter.</p> <p>Handles Anthropic SDK-specific logic for detecting clients, normalizing input/output, and extracting callable methods.</p> Classes\u00b6 <code>AnthropicAdapter</code> \u00b6 <p>Adapter for Anthropic SDK clients.</p> <p>Supports both AsyncAnthropic and Anthropic clients. Handles the Anthropic-specific API structure: client.messages.create()</p> Functions\u00b6 <code>detect(client: Any) -&gt; bool</code> <code>staticmethod</code> \u00b6 <p>Detect if a client is an Anthropic client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>The client instance to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if client has Anthropic SDK structure, False otherwise</p> Source code in <code>src/hookedllm/providers/anthropic.py</code> <pre><code>@staticmethod\ndef detect(client: Any) -&gt; bool:\n    \"\"\"\n    Detect if a client is an Anthropic client.\n\n    Args:\n        client: The client instance to check\n\n    Returns:\n        True if client has Anthropic SDK structure, False otherwise\n    \"\"\"\n    # Check for Anthropic structure\n    # Must have messages.create and it should be callable\n    try:\n        if hasattr(client, \"messages\") and hasattr(client.messages, \"create\"):\n            # Verify it's actually callable (not just a MagicMock attribute)\n            create_method = getattr(client.messages, \"create\", None)\n            if callable(create_method):\n                # Additional check: if client also has OpenAI structure (chat.completions.create),\n                # we need to prefer Anthropic only if messages.create was explicitly set\n                # For MagicMock, check if chat.completions.create exists - if it does and is callable,\n                # we need to verify which one was set explicitly\n                # Since Anthropic is checked first, if both exist, prefer Anthropic\n                # But we should verify messages.create is the primary structure\n                return True\n    except (AttributeError, TypeError):\n        pass\n\n    return False\n</code></pre> <code>get_callable(client: Any) -&gt; Any</code> <code>staticmethod</code> \u00b6 <p>Get the Anthropic messages.create callable.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>The Anthropic client instance</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The messages.create method</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the callable cannot be found</p> Source code in <code>src/hookedllm/providers/anthropic.py</code> <pre><code>@staticmethod\ndef get_callable(client: Any) -&gt; Any:\n    \"\"\"\n    Get the Anthropic messages.create callable.\n\n    Args:\n        client: The Anthropic client instance\n\n    Returns:\n        The messages.create method\n\n    Raises:\n        AttributeError: If the callable cannot be found\n    \"\"\"\n    return client.messages.create\n</code></pre> <code>get_wrapper_path(client: Any) -&gt; list[str]</code> <code>staticmethod</code> \u00b6 <p>Get the attribute path to wrap for Anthropic.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>[\"messages\"] - the path to intercept</p> Source code in <code>src/hookedllm/providers/anthropic.py</code> <pre><code>@staticmethod\ndef get_wrapper_path(client: Any) -&gt; list[str]:\n    \"\"\"\n    Get the attribute path to wrap for Anthropic.\n\n    Returns:\n        [\"messages\"] - the path to intercept\n    \"\"\"\n    return [\"messages\"]\n</code></pre> <code>normalize_input(provider_name: str, callable_method: Any, *args: Any, **kwargs: Any) -&gt; tuple[CallInput, CallContext]</code> <code>staticmethod</code> \u00b6 <p>Normalize Anthropic input to CallInput and CallContext.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Provider name (should be \"anthropic\")</p> required <code>callable_method</code> <code>Any</code> <p>The callable method (unused for Anthropic)</p> required <code>*args</code> <code>Any</code> <p>Positional arguments (unused for Anthropic)</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments including model, messages, etc.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[CallInput, CallContext]</code> <p>Tuple of (CallInput, CallContext)</p> Source code in <code>src/hookedllm/providers/anthropic.py</code> <pre><code>@staticmethod\ndef normalize_input(\n    provider_name: str, callable_method: Any, *args: Any, **kwargs: Any\n) -&gt; tuple[CallInput, CallContext]:\n    \"\"\"\n    Normalize Anthropic input to CallInput and CallContext.\n\n    Args:\n        provider_name: Provider name (should be \"anthropic\")\n        callable_method: The callable method (unused for Anthropic)\n        *args: Positional arguments (unused for Anthropic)\n        **kwargs: Keyword arguments including model, messages, etc.\n\n    Returns:\n        Tuple of (CallInput, CallContext)\n    \"\"\"\n    model = kwargs.get(\"model\", \"\")\n    messages = kwargs.get(\"messages\", [])\n\n    # Anthropic uses metadata parameter instead of extra_body\n    metadata_dict = kwargs.get(\"metadata\", {})\n    if isinstance(metadata_dict, dict):\n        tags = metadata_dict.pop(\"hookedllm_tags\", [])\n        custom_metadata = metadata_dict.pop(\"hookedllm_metadata\", {})\n    else:\n        tags = []\n        custom_metadata = {}\n\n    # Normalize messages to internal format\n    # Anthropic messages have role and content fields\n    normalized_messages = []\n    for m in messages:\n        role = m.get(\"role\", \"\")\n        # Anthropic content can be a string or list of content blocks\n        content = m.get(\"content\", \"\")\n        if isinstance(content, list) and len(content) &gt; 0:\n            # Extract text from first text block if it's a list\n            first_block = content[0]\n            if isinstance(first_block, dict) and first_block.get(\"type\") == \"text\":\n                content = first_block.get(\"text\", \"\")\n            elif isinstance(first_block, str):\n                content = first_block\n        normalized_messages.append(Message(role=role, content=content))\n\n    # Create normalized input\n    call_input = CallInput(\n        model=model, messages=normalized_messages, params=kwargs, metadata=custom_metadata\n    )\n\n    # Create context\n    context = CallContext(\n        provider=provider_name, model=model, tags=tags, metadata=custom_metadata\n    )\n\n    return call_input, context\n</code></pre> <code>normalize_output(response: Any) -&gt; CallOutput</code> <code>staticmethod</code> \u00b6 <p>Normalize Anthropic response to CallOutput.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Any</code> <p>Anthropic SDK response object</p> required <p>Returns:</p> Type Description <code>CallOutput</code> <p>Normalized CallOutput</p> Source code in <code>src/hookedllm/providers/anthropic.py</code> <pre><code>@staticmethod\ndef normalize_output(response: Any) -&gt; CallOutput:\n    \"\"\"\n    Normalize Anthropic response to CallOutput.\n\n    Args:\n        response: Anthropic SDK response object\n\n    Returns:\n        Normalized CallOutput\n    \"\"\"\n    try:\n        # Extract text from response\n        # Anthropic response has content as a list of content blocks\n        text = None\n        if hasattr(response, \"content\") and isinstance(response.content, list):\n            if len(response.content) &gt; 0:\n                first_content = response.content[0]\n                if isinstance(first_content, dict):\n                    text = first_content.get(\"text\")\n                elif hasattr(first_content, \"text\"):\n                    text = first_content.text\n\n        # Extract usage\n        # Anthropic uses input_tokens and output_tokens\n        usage = None\n        if hasattr(response, \"usage\"):\n            usage_obj = response.usage\n            if hasattr(usage_obj, \"input_tokens\") and hasattr(usage_obj, \"output_tokens\"):\n                # Convert to dict format\n                if hasattr(usage_obj, \"model_dump\"):\n                    usage = usage_obj.model_dump()\n                elif hasattr(usage_obj, \"dict\"):\n                    usage = usage_obj.dict()\n                elif hasattr(usage_obj, \"__dict__\"):\n                    usage = dict(usage_obj.__dict__)\n                else:\n                    # Fallback: create dict manually\n                    usage = {\n                        \"input_tokens\": usage_obj.input_tokens,\n                        \"output_tokens\": usage_obj.output_tokens,\n                        \"total_tokens\": usage_obj.input_tokens + usage_obj.output_tokens,\n                    }\n                # Ensure total_tokens is present\n                if usage and \"total_tokens\" not in usage:\n                    usage[\"total_tokens\"] = usage.get(\"input_tokens\", 0) + usage.get(\n                        \"output_tokens\", 0\n                    )\n\n        # Extract stop_reason (Anthropic's finish_reason)\n        finish_reason = None\n        if hasattr(response, \"stop_reason\"):\n            finish_reason = response.stop_reason\n\n        return CallOutput(text=text, raw=response, usage=usage, finish_reason=finish_reason)\n    except Exception:\n        # If normalization fails, return minimal output with raw response\n        return CallOutput(text=None, raw=response, usage=None, finish_reason=None)\n</code></pre>"},{"location":"api/#hookedllm.providers.openai","title":"<code>openai</code>","text":"<p>OpenAI provider adapter.</p> <p>Handles OpenAI SDK-specific logic for detecting clients, normalizing input/output, and extracting callable methods.</p> Classes\u00b6 <code>OpenAIAdapter</code> \u00b6 <p>Adapter for OpenAI SDK clients.</p> <p>Supports both AsyncOpenAI and OpenAI clients. Handles the OpenAI-specific API structure: client.chat.completions.create()</p> Functions\u00b6 <code>detect(client: Any) -&gt; bool</code> <code>staticmethod</code> \u00b6 <p>Detect if a client is an OpenAI client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>The client instance to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if client has OpenAI SDK structure, False otherwise</p> Source code in <code>src/hookedllm/providers/openai.py</code> <pre><code>@staticmethod\ndef detect(client: Any) -&gt; bool:\n    \"\"\"\n    Detect if a client is an OpenAI client.\n\n    Args:\n        client: The client instance to check\n\n    Returns:\n        True if client has OpenAI SDK structure, False otherwise\n    \"\"\"\n    # Check for OpenAI structure\n    # Must have chat.completions.create and it should be callable\n    try:\n        # Use getattr with sentinel to avoid MagicMock auto-creation\n        _SENTINEL = object()\n\n        # Check for OpenAI structure\n        chat = getattr(client, \"chat\", _SENTINEL)\n        if chat is _SENTINEL:\n            return False\n\n        completions = getattr(chat, \"completions\", _SENTINEL)\n        if completions is _SENTINEL:\n            return False\n\n        create_method = getattr(completions, \"create\", _SENTINEL)\n        if create_method is _SENTINEL or not callable(create_method):\n            return False\n\n        # Check if it also has Anthropic structure\n        # If both exist, Anthropic should have matched first\n        # But if Anthropic didn't match, it means messages.create wasn't properly set\n        # So we can match OpenAI\n        # However, to avoid false positives with MagicMock, we check:\n        # If messages.create exists and is callable, don't match OpenAI\n        # (Anthropic should have matched if it was properly set)\n        messages = getattr(client, \"messages\", _SENTINEL)\n        if messages is not _SENTINEL:\n            messages_create = getattr(messages, \"create\", _SENTINEL)\n            if messages_create is not _SENTINEL and callable(messages_create):\n                # Both structures exist - since Anthropic is checked first,\n                # if we get here, Anthropic didn't match, which means messages.create\n                # was auto-created by MagicMock. So we should still match OpenAI.\n                # But to be safe, if both are properly callable, prefer Anthropic\n                # Actually, let's be conservative: if both exist, don't match OpenAI\n                return False\n\n        return True\n    except (AttributeError, TypeError):\n        pass\n\n    return False\n</code></pre> <code>get_callable(client: Any) -&gt; Any</code> <code>staticmethod</code> \u00b6 <p>Get the OpenAI completions.create callable.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>The OpenAI client instance</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The chat.completions.create method</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the callable cannot be found</p> Source code in <code>src/hookedllm/providers/openai.py</code> <pre><code>@staticmethod\ndef get_callable(client: Any) -&gt; Any:\n    \"\"\"\n    Get the OpenAI completions.create callable.\n\n    Args:\n        client: The OpenAI client instance\n\n    Returns:\n        The chat.completions.create method\n\n    Raises:\n        AttributeError: If the callable cannot be found\n    \"\"\"\n    return client.chat.completions.create\n</code></pre> <code>get_wrapper_path(client: Any) -&gt; list[str]</code> <code>staticmethod</code> \u00b6 <p>Get the attribute path to wrap for OpenAI.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>[\"chat\", \"completions\"] - the path to intercept</p> Source code in <code>src/hookedllm/providers/openai.py</code> <pre><code>@staticmethod\ndef get_wrapper_path(client: Any) -&gt; list[str]:\n    \"\"\"\n    Get the attribute path to wrap for OpenAI.\n\n    Returns:\n        [\"chat\", \"completions\"] - the path to intercept\n    \"\"\"\n    return [\"chat\", \"completions\"]\n</code></pre> <code>normalize_input(provider_name: str, callable_method: Any, *args: Any, **kwargs: Any) -&gt; tuple[CallInput, CallContext]</code> <code>staticmethod</code> \u00b6 <p>Normalize OpenAI input to CallInput and CallContext.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Provider name (should be \"openai\")</p> required <code>callable_method</code> <code>Any</code> <p>The callable method (unused for OpenAI)</p> required <code>*args</code> <code>Any</code> <p>Positional arguments (unused for OpenAI)</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments including model, messages, etc.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[CallInput, CallContext]</code> <p>Tuple of (CallInput, CallContext)</p> Source code in <code>src/hookedllm/providers/openai.py</code> <pre><code>@staticmethod\ndef normalize_input(\n    provider_name: str, callable_method: Any, *args: Any, **kwargs: Any\n) -&gt; tuple[CallInput, CallContext]:\n    \"\"\"\n    Normalize OpenAI input to CallInput and CallContext.\n\n    Args:\n        provider_name: Provider name (should be \"openai\")\n        callable_method: The callable method (unused for OpenAI)\n        *args: Positional arguments (unused for OpenAI)\n        **kwargs: Keyword arguments including model, messages, etc.\n\n    Returns:\n        Tuple of (CallInput, CallContext)\n    \"\"\"\n    model = kwargs.get(\"model\", \"\")\n    messages = kwargs.get(\"messages\", [])\n\n    # Extract hookedllm-specific params from extra_body\n    extra_body = kwargs.get(\"extra_body\", {})\n    if isinstance(extra_body, dict):\n        tags = extra_body.pop(\"hookedllm_tags\", [])\n        metadata = extra_body.pop(\"hookedllm_metadata\", {})\n    else:\n        tags = []\n        metadata = {}\n\n    # Normalize messages to internal format\n    normalized_messages = [\n        Message(role=m.get(\"role\", \"\"), content=m.get(\"content\", \"\")) for m in messages\n    ]\n\n    # Create normalized input\n    call_input = CallInput(\n        model=model, messages=normalized_messages, params=kwargs, metadata=metadata\n    )\n\n    # Create context\n    context = CallContext(provider=provider_name, model=model, tags=tags, metadata=metadata)\n\n    return call_input, context\n</code></pre> <code>normalize_output(response: Any) -&gt; CallOutput</code> <code>staticmethod</code> \u00b6 <p>Normalize OpenAI response to CallOutput.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Any</code> <p>OpenAI SDK response object</p> required <p>Returns:</p> Type Description <code>CallOutput</code> <p>Normalized CallOutput</p> Source code in <code>src/hookedllm/providers/openai.py</code> <pre><code>@staticmethod\ndef normalize_output(response: Any) -&gt; CallOutput:\n    \"\"\"\n    Normalize OpenAI response to CallOutput.\n\n    Args:\n        response: OpenAI SDK response object\n\n    Returns:\n        Normalized CallOutput\n    \"\"\"\n    try:\n        # Extract text from response\n        text = None\n        if hasattr(response, \"choices\") and len(response.choices) &gt; 0:\n            choice = response.choices[0]\n            if hasattr(choice, \"message\"):\n                text = getattr(choice.message, \"content\", None)\n\n        # Extract usage\n        usage = None\n        if hasattr(response, \"usage\"):\n            # Try to convert to dict\n            usage_obj = response.usage\n            if hasattr(usage_obj, \"model_dump\"):\n                usage = usage_obj.model_dump()\n            elif hasattr(usage_obj, \"dict\"):\n                usage = usage_obj.dict()\n            elif hasattr(usage_obj, \"__dict__\"):\n                usage = dict(usage_obj.__dict__)\n\n        # Extract finish_reason\n        finish_reason = None\n        if hasattr(response, \"choices\") and len(response.choices) &gt; 0:\n            finish_reason = getattr(response.choices[0], \"finish_reason\", None)\n\n        return CallOutput(text=text, raw=response, usage=usage, finish_reason=finish_reason)\n    except Exception:\n        # If normalization fails, return minimal output with raw response\n        return CallOutput(text=None, raw=response, usage=None, finish_reason=None)\n</code></pre>"},{"location":"api/#hookedllm.providers.protocol","title":"<code>protocol</code>","text":"<p>Provider adapter protocol for multi-provider support.</p> <p>Defines the interface that all provider adapters must implement, following the Dependency Inversion Principle.</p> Classes\u00b6 <code>ProviderAdapter</code> \u00b6 <p>               Bases: <code>Protocol</code></p> <p>Protocol for provider-specific adapters.</p> <p>Each adapter handles: - Detecting if a client belongs to this provider - Normalizing provider-specific input/output formats - Extracting the callable method from the client</p> <p>This follows the Open/Closed Principle: new providers can be added by implementing this protocol without modifying existing code.</p> Functions\u00b6 <code>detect(client: Any) -&gt; bool</code> <code>staticmethod</code> \u00b6 <p>Detect if a client belongs to this provider.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>The client instance to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if this adapter can handle the client, False otherwise</p> Source code in <code>src/hookedllm/providers/protocol.py</code> <pre><code>@staticmethod\ndef detect(client: Any) -&gt; bool:\n    \"\"\"\n    Detect if a client belongs to this provider.\n\n    Args:\n        client: The client instance to check\n\n    Returns:\n        True if this adapter can handle the client, False otherwise\n    \"\"\"\n    ...\n</code></pre> <code>get_callable(client: Any) -&gt; Any</code> <code>staticmethod</code> \u00b6 <p>Get the callable method from the client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>The client instance</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The callable method (e.g., client.chat.completions.create)</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the callable cannot be found</p> Source code in <code>src/hookedllm/providers/protocol.py</code> <pre><code>@staticmethod\ndef get_callable(client: Any) -&gt; Any:\n    \"\"\"\n    Get the callable method from the client.\n\n    Args:\n        client: The client instance\n\n    Returns:\n        The callable method (e.g., client.chat.completions.create)\n\n    Raises:\n        AttributeError: If the callable cannot be found\n    \"\"\"\n    ...\n</code></pre> <code>get_wrapper_path(client: Any) -&gt; list[str]</code> <code>staticmethod</code> \u00b6 <p>Get the attribute path to wrap (e.g., [\"chat\", \"completions\"]).</p> <p>This is used by the wrapper to intercept the correct attributes.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>The client instance</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of attribute names to wrap</p> Source code in <code>src/hookedllm/providers/protocol.py</code> <pre><code>@staticmethod\ndef get_wrapper_path(client: Any) -&gt; list[str]:\n    \"\"\"\n    Get the attribute path to wrap (e.g., [\"chat\", \"completions\"]).\n\n    This is used by the wrapper to intercept the correct attributes.\n\n    Args:\n        client: The client instance\n\n    Returns:\n        List of attribute names to wrap\n    \"\"\"\n    ...\n</code></pre> <code>normalize_input(provider_name: str, callable_method: Any, *args: Any, **kwargs: Any) -&gt; tuple[CallInput, CallContext]</code> <code>staticmethod</code> \u00b6 <p>Normalize provider-specific input to CallInput and CallContext.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Name of the provider (e.g., \"openai\", \"anthropic\")</p> required <code>callable_method</code> <code>Any</code> <p>The callable method being called</p> required <code>*args</code> <code>Any</code> <p>Positional arguments passed to the callable</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments passed to the callable</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[CallInput, CallContext]</code> <p>Tuple of (CallInput, CallContext)</p> Source code in <code>src/hookedllm/providers/protocol.py</code> <pre><code>@staticmethod\ndef normalize_input(\n    provider_name: str, callable_method: Any, *args: Any, **kwargs: Any\n) -&gt; tuple[CallInput, CallContext]:\n    \"\"\"\n    Normalize provider-specific input to CallInput and CallContext.\n\n    Args:\n        provider_name: Name of the provider (e.g., \"openai\", \"anthropic\")\n        callable_method: The callable method being called\n        *args: Positional arguments passed to the callable\n        **kwargs: Keyword arguments passed to the callable\n\n    Returns:\n        Tuple of (CallInput, CallContext)\n    \"\"\"\n    ...\n</code></pre> <code>normalize_output(response: Any) -&gt; CallOutput</code> <code>staticmethod</code> \u00b6 <p>Normalize provider-specific response to CallOutput.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Any</code> <p>The raw response from the provider SDK</p> required <p>Returns:</p> Type Description <code>CallOutput</code> <p>Normalized CallOutput</p> Source code in <code>src/hookedllm/providers/protocol.py</code> <pre><code>@staticmethod\ndef normalize_output(response: Any) -&gt; CallOutput:\n    \"\"\"\n    Normalize provider-specific response to CallOutput.\n\n    Args:\n        response: The raw response from the provider SDK\n\n    Returns:\n        Normalized CallOutput\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#core-components","title":"Core Components","text":""},{"location":"api/#hookedllm.core","title":"<code>core</code>","text":"<p>Core module for hookedllm.</p> <p>Contains the fundamental types, protocols, and implementations.</p>"},{"location":"api/#hookedllm.core-classes","title":"Classes","text":""},{"location":"api/#hookedllm.core.CallContext","title":"<code>CallContext(call_id: str = (lambda: str(uuid4()))(), parent_id: str | None = None, provider: str = '', model: str = '', route: str = 'chat', tags: list[str] = list(), started_at: datetime = (lambda: datetime.now(timezone.utc))(), metadata: dict[str, Any] = dict())</code>  <code>dataclass</code>","text":"<p>Context for a single LLM call lifecycle.</p> <p>Contains metadata about the call including timing, tags, and custom metadata.</p>"},{"location":"api/#hookedllm.core.CallInput","title":"<code>CallInput(model: str, messages: Sequence[Message], params: dict[str, Any] = dict(), metadata: dict[str, Any] = dict())</code>  <code>dataclass</code>","text":"<p>Normalized input for an LLM call.</p> <p>This represents the input parameters in a provider-agnostic way.</p>"},{"location":"api/#hookedllm.core.CallOutput","title":"<code>CallOutput(text: str | None, raw: Any, usage: dict[str, Any] | None = None, finish_reason: str | None = None)</code>  <code>dataclass</code>","text":"<p>Normalized output from an LLM call.</p> <p>This represents the response in a provider-agnostic way while preserving the original response object.</p>"},{"location":"api/#hookedllm.core.CallResult","title":"<code>CallResult(input: CallInput, output: CallOutput | None, context: CallContext, error: BaseException | None, ended_at: datetime, elapsed_ms: float)</code>  <code>dataclass</code>","text":"<p>Complete result of an LLM call.</p> <p>Contains the input, output, context, any error that occurred, and timing information. This is passed to finally hooks.</p>"},{"location":"api/#hookedllm.core.CompositeRule","title":"<code>CompositeRule(rules: list[Any], operator: Literal['and', 'or'])</code>  <code>dataclass</code>","text":"<p>Combine multiple rules with AND/OR logic.</p>"},{"location":"api/#hookedllm.core.CustomRule","title":"<code>CustomRule(predicate: Callable[[CallInput, CallContext], bool])</code>  <code>dataclass</code>","text":"<p>Custom predicate function.</p>"},{"location":"api/#hookedllm.core.DefaultHookExecutor","title":"<code>DefaultHookExecutor(error_handler: Callable[[Exception, str], None] | None = None, logger: Any | None = None)</code>","text":"<p>Concrete hook executor with dependency injection.</p> <p>Single Responsibility: Only executes hooks, doesn't store them. Dependencies injected: error_handler, logger</p> <p>Guarantees: - Hook failures never break the main LLM call - Hooks execute in order - Rules are evaluated before execution</p> <p>Initialize executor with optional dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>error_handler</code> <code>Callable[[Exception, str], None] | None</code> <p>Called when a hook fails. Signature: (error, context_str)</p> <code>None</code> <code>logger</code> <code>Any | None</code> <p>Logger instance (must have .error() method)</p> <code>None</code> Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>def __init__(\n    self,\n    error_handler: Callable[[Exception, str], None] | None = None,\n    logger: Any | None = None,\n):\n    \"\"\"\n    Initialize executor with optional dependencies.\n\n    Args:\n        error_handler: Called when a hook fails. Signature: (error, context_str)\n        logger: Logger instance (must have .error() method)\n    \"\"\"\n    self._error_handler = error_handler or self._default_error_handler\n    self._logger = logger\n</code></pre>"},{"location":"api/#hookedllm.core.DefaultHookExecutor-functions","title":"Functions","text":""},{"location":"api/#hookedllm.core.DefaultHookExecutor.execute_after","title":"<code>execute_after(hooks: list[tuple[AfterHook, Rule | None]], call_input: CallInput, call_output: CallOutput, context: CallContext) -&gt; None</code>  <code>async</code>","text":"<p>Execute after hooks with rule matching.</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[tuple[AfterHook, Rule | None]]</code> <p>List of (hook, rule) tuples</p> required <code>call_input</code> <code>CallInput</code> <p>The LLM call input</p> required <code>call_output</code> <code>CallOutput</code> <p>The LLM call output</p> required <code>context</code> <code>CallContext</code> <p>The call context</p> required Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>async def execute_after(\n    self,\n    hooks: list[tuple[AfterHook, Rule | None]],\n    call_input: CallInput,\n    call_output: CallOutput,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"\n    Execute after hooks with rule matching.\n\n    Args:\n        hooks: List of (hook, rule) tuples\n        call_input: The LLM call input\n        call_output: The LLM call output\n        context: The call context\n    \"\"\"\n    for hook, rule in hooks:\n        # Check if rule matches\n        if rule is None or rule.matches(call_input, context):\n            try:\n                await hook(call_input, call_output, context)\n            except Exception as e:\n                hook_name = getattr(hook, \"__name__\", str(hook))\n                self._error_handler(e, f\"After hook {hook_name}\")\n                if self._logger:\n                    self._logger.error(f\"After hook {hook_name} failed: {e}\")\n</code></pre>"},{"location":"api/#hookedllm.core.DefaultHookExecutor.execute_before","title":"<code>execute_before(hooks: list[tuple[BeforeHook, Rule | None]], call_input: CallInput, context: CallContext) -&gt; None</code>  <code>async</code>","text":"<p>Execute before hooks with rule matching.</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[tuple[BeforeHook, Rule | None]]</code> <p>List of (hook, rule) tuples</p> required <code>call_input</code> <code>CallInput</code> <p>The LLM call input</p> required <code>context</code> <code>CallContext</code> <p>The call context</p> required Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>async def execute_before(\n    self,\n    hooks: list[tuple[BeforeHook, Rule | None]],\n    call_input: CallInput,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"\n    Execute before hooks with rule matching.\n\n    Args:\n        hooks: List of (hook, rule) tuples\n        call_input: The LLM call input\n        context: The call context\n    \"\"\"\n    for hook, rule in hooks:\n        # Check if rule matches\n        if rule is None or rule.matches(call_input, context):\n            try:\n                await hook(call_input, context)\n            except Exception as e:\n                hook_name = getattr(hook, \"__name__\", str(hook))\n                self._error_handler(e, f\"Before hook {hook_name}\")\n                if self._logger:\n                    self._logger.error(f\"Before hook {hook_name} failed: {e}\")\n</code></pre>"},{"location":"api/#hookedllm.core.DefaultHookExecutor.execute_error","title":"<code>execute_error(hooks: list[tuple[ErrorHook, Rule | None]], call_input: CallInput, error: BaseException, context: CallContext) -&gt; None</code>  <code>async</code>","text":"<p>Execute error hooks with rule matching.</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[tuple[ErrorHook, Rule | None]]</code> <p>List of (hook, rule) tuples</p> required <code>call_input</code> <code>CallInput</code> <p>The LLM call input</p> required <code>error</code> <code>BaseException</code> <p>The error that occurred</p> required <code>context</code> <code>CallContext</code> <p>The call context</p> required Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>async def execute_error(\n    self,\n    hooks: list[tuple[ErrorHook, Rule | None]],\n    call_input: CallInput,\n    error: BaseException,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"\n    Execute error hooks with rule matching.\n\n    Args:\n        hooks: List of (hook, rule) tuples\n        call_input: The LLM call input\n        error: The error that occurred\n        context: The call context\n    \"\"\"\n    for hook, rule in hooks:\n        # Check if rule matches\n        if rule is None or rule.matches(call_input, context):\n            try:\n                await hook(call_input, error, context)\n            except Exception as e:\n                hook_name = getattr(hook, \"__name__\", str(hook))\n                self._error_handler(e, f\"Error hook {hook_name}\")\n                if self._logger:\n                    self._logger.error(f\"Error hook {hook_name} failed: {e}\")\n</code></pre>"},{"location":"api/#hookedllm.core.DefaultHookExecutor.execute_finally","title":"<code>execute_finally(hooks: list[tuple[FinallyHook, Rule | None]], result: CallResult) -&gt; None</code>  <code>async</code>","text":"<p>Execute finally hooks.</p> <p>Note: Finally hooks don't use rule matching - they always run.</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[tuple[FinallyHook, Rule | None]]</code> <p>List of (hook, rule) tuples (rule is ignored for finally hooks)</p> required <code>result</code> <code>CallResult</code> <p>The complete call result</p> required Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>async def execute_finally(\n    self, hooks: list[tuple[FinallyHook, Rule | None]], result: CallResult\n) -&gt; None:\n    \"\"\"\n    Execute finally hooks.\n\n    Note: Finally hooks don't use rule matching - they always run.\n\n    Args:\n        hooks: List of (hook, rule) tuples (rule is ignored for finally hooks)\n        result: The complete call result\n    \"\"\"\n    for hook, _rule in hooks:\n        # Finally hooks always run, ignore rule\n        try:\n            await hook(result)\n        except Exception as e:\n            hook_name = getattr(hook, \"__name__\", str(hook))\n            self._error_handler(e, f\"Finally hook {hook_name}\")\n            if self._logger:\n                self._logger.error(f\"Finally hook {hook_name} failed: {e}\")\n</code></pre>"},{"location":"api/#hookedllm.core.HookExecutor","title":"<code>HookExecutor</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for executing hooks.</p> <p>Single Responsibility: Hook execution only, no storage.</p>"},{"location":"api/#hookedllm.core.HookExecutor-functions","title":"Functions","text":""},{"location":"api/#hookedllm.core.HookExecutor.execute_after","title":"<code>execute_after(hooks: list[tuple[AfterHook, Rule | None]], call_input: CallInput, call_output: CallOutput, context: CallContext) -&gt; None</code>  <code>async</code>","text":"<p>Execute after hooks with rule matching.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>async def execute_after(\n    self,\n    hooks: list[tuple[AfterHook, Rule | None]],\n    call_input: CallInput,\n    call_output: CallOutput,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"Execute after hooks with rule matching.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.HookExecutor.execute_before","title":"<code>execute_before(hooks: list[tuple[BeforeHook, Rule | None]], call_input: CallInput, context: CallContext) -&gt; None</code>  <code>async</code>","text":"<p>Execute before hooks with rule matching.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>async def execute_before(\n    self,\n    hooks: list[tuple[BeforeHook, Rule | None]],\n    call_input: CallInput,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"Execute before hooks with rule matching.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.HookExecutor.execute_error","title":"<code>execute_error(hooks: list[tuple[ErrorHook, Rule | None]], call_input: CallInput, error: BaseException, context: CallContext) -&gt; None</code>  <code>async</code>","text":"<p>Execute error hooks with rule matching.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>async def execute_error(\n    self,\n    hooks: list[tuple[ErrorHook, Rule | None]],\n    call_input: CallInput,\n    error: BaseException,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"Execute error hooks with rule matching.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.HookExecutor.execute_finally","title":"<code>execute_finally(hooks: list[tuple[FinallyHook, Rule | None]], result: CallResult) -&gt; None</code>  <code>async</code>","text":"<p>Execute finally hooks.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>async def execute_finally(\n    self, hooks: list[tuple[FinallyHook, Rule | None]], result: CallResult\n) -&gt; None:\n    \"\"\"Execute finally hooks.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.HookedClientWrapper","title":"<code>HookedClientWrapper(original_client: Any, scopes: list[ScopeHookStore], executor: HookExecutor)</code>","text":"<p>Transparent proxy with all dependencies injected.</p> <p>No global state - all dependencies passed via constructor (DI). Intercepts provider SDK methods to inject hook execution.</p> <p>Initialize wrapper with injected dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>original_client</code> <code>Any</code> <p>The original provider client</p> required <code>scopes</code> <code>list[ScopeHookStore]</code> <p>List of scope hook stores to use</p> required <code>executor</code> <code>HookExecutor</code> <p>Hook executor instance</p> required Source code in <code>src/hookedllm/core/wrapper.py</code> <pre><code>def __init__(self, original_client: Any, scopes: list[ScopeHookStore], executor: HookExecutor):\n    \"\"\"\n    Initialize wrapper with injected dependencies.\n\n    Args:\n        original_client: The original provider client\n        scopes: List of scope hook stores to use\n        executor: Hook executor instance\n    \"\"\"\n    self._original = original_client\n    self._scopes = scopes\n    self._executor = executor\n    self._adapter = _detect_provider_adapter(original_client)\n    self._wrapper_path = self._adapter.get_wrapper_path(original_client)\n</code></pre>"},{"location":"api/#hookedllm.core.HookedClientWrapper-functions","title":"Functions","text":""},{"location":"api/#hookedllm.core.HookedClientWrapper.__getattr__","title":"<code>__getattr__(name: str) -&gt; Any</code>","text":"<p>Intercept attribute access.</p> <p>Wraps attributes based on the detected provider's wrapper path.</p> Source code in <code>src/hookedllm/core/wrapper.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"\n    Intercept attribute access.\n\n    Wraps attributes based on the detected provider's wrapper path.\n    \"\"\"\n    attr = getattr(self._original, name)\n\n    # Check if this attribute is part of the wrapper path\n    if len(self._wrapper_path) &gt; 0 and name == self._wrapper_path[0]:\n        # Create a dynamic wrapper for the next level\n        return _create_wrapper_for_path(\n            attr, self._wrapper_path[1:], self._scopes, self._executor, self._adapter\n        )\n\n    return attr\n</code></pre>"},{"location":"api/#hookedllm.core.InMemoryScopeHookStore","title":"<code>InMemoryScopeHookStore(scope_name: str)</code>","text":"<p>Concrete implementation of ScopeHookStore.</p> <p>Single Responsibility: Only stores hooks. Stores hooks in memory with their associated rules.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def __init__(self, scope_name: str):\n    self._scope_name = scope_name\n    self._before: list[tuple[BeforeHook, Rule | None]] = []\n    self._after: list[tuple[AfterHook, Rule | None]] = []\n    self._error: list[tuple[ErrorHook, Rule | None]] = []\n    self._finally: list[tuple[FinallyHook, Rule | None]] = []\n</code></pre>"},{"location":"api/#hookedllm.core.InMemoryScopeHookStore-attributes","title":"Attributes","text":""},{"location":"api/#hookedllm.core.InMemoryScopeHookStore.name","title":"<code>name: str</code>  <code>property</code>","text":"<p>Get the scope name.</p>"},{"location":"api/#hookedllm.core.InMemoryScopeHookStore-functions","title":"Functions","text":""},{"location":"api/#hookedllm.core.InMemoryScopeHookStore.add_after","title":"<code>add_after(hook: AfterHook, rule: Rule | None = None) -&gt; None</code>","text":"<p>Add an after hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def add_after(self, hook: AfterHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add an after hook with optional execution rule.\"\"\"\n    self._after.append((hook, rule))\n</code></pre>"},{"location":"api/#hookedllm.core.InMemoryScopeHookStore.add_before","title":"<code>add_before(hook: BeforeHook, rule: Rule | None = None) -&gt; None</code>","text":"<p>Add a before hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def add_before(self, hook: BeforeHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add a before hook with optional execution rule.\"\"\"\n    self._before.append((hook, rule))\n</code></pre>"},{"location":"api/#hookedllm.core.InMemoryScopeHookStore.add_error","title":"<code>add_error(hook: ErrorHook, rule: Rule | None = None) -&gt; None</code>","text":"<p>Add an error hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def add_error(self, hook: ErrorHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add an error hook with optional execution rule.\"\"\"\n    self._error.append((hook, rule))\n</code></pre>"},{"location":"api/#hookedllm.core.InMemoryScopeHookStore.add_finally","title":"<code>add_finally(hook: FinallyHook, rule: Rule | None = None) -&gt; None</code>","text":"<p>Add a finally hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def add_finally(self, hook: FinallyHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add a finally hook with optional execution rule.\"\"\"\n    self._finally.append((hook, rule))\n</code></pre>"},{"location":"api/#hookedllm.core.InMemoryScopeHookStore.after","title":"<code>after(hook: AfterHook, *, when: Rule | None = None) -&gt; None</code>","text":"<p>Alias for add_after with keyword 'when' parameter.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def after(self, hook: AfterHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"Alias for add_after with keyword 'when' parameter.\"\"\"\n    self.add_after(hook, when)\n</code></pre>"},{"location":"api/#hookedllm.core.InMemoryScopeHookStore.before","title":"<code>before(hook: BeforeHook, *, when: Rule | None = None) -&gt; None</code>","text":"<p>Alias for add_before with keyword 'when' parameter.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def before(self, hook: BeforeHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"Alias for add_before with keyword 'when' parameter.\"\"\"\n    self.add_before(hook, when)\n</code></pre>"},{"location":"api/#hookedllm.core.InMemoryScopeHookStore.error","title":"<code>error(hook: ErrorHook, *, when: Rule | None = None) -&gt; None</code>","text":"<p>Alias for add_error with keyword 'when' parameter.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def error(self, hook: ErrorHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"Alias for add_error with keyword 'when' parameter.\"\"\"\n    self.add_error(hook, when)\n</code></pre>"},{"location":"api/#hookedllm.core.InMemoryScopeHookStore.finally_","title":"<code>finally_(hook: FinallyHook, *, when: Rule | None = None) -&gt; None</code>","text":"<p>Alias for add_finally with keyword 'when' parameter.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def finally_(self, hook: FinallyHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"Alias for add_finally with keyword 'when' parameter.\"\"\"\n    self.add_finally(hook, when)\n</code></pre>"},{"location":"api/#hookedllm.core.InMemoryScopeHookStore.get_after_hooks","title":"<code>get_after_hooks() -&gt; list[tuple[AfterHook, Rule | None]]</code>","text":"<p>Get all after hooks with their rules (returns copy).</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_after_hooks(self) -&gt; list[tuple[AfterHook, Rule | None]]:\n    \"\"\"Get all after hooks with their rules (returns copy).\"\"\"\n    return self._after.copy()\n</code></pre>"},{"location":"api/#hookedllm.core.InMemoryScopeHookStore.get_before_hooks","title":"<code>get_before_hooks() -&gt; list[tuple[BeforeHook, Rule | None]]</code>","text":"<p>Get all before hooks with their rules (returns copy).</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_before_hooks(self) -&gt; list[tuple[BeforeHook, Rule | None]]:\n    \"\"\"Get all before hooks with their rules (returns copy).\"\"\"\n    return self._before.copy()\n</code></pre>"},{"location":"api/#hookedllm.core.InMemoryScopeHookStore.get_error_hooks","title":"<code>get_error_hooks() -&gt; list[tuple[ErrorHook, Rule | None]]</code>","text":"<p>Get all error hooks with their rules (returns copy).</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_error_hooks(self) -&gt; list[tuple[ErrorHook, Rule | None]]:\n    \"\"\"Get all error hooks with their rules (returns copy).\"\"\"\n    return self._error.copy()\n</code></pre>"},{"location":"api/#hookedllm.core.InMemoryScopeHookStore.get_finally_hooks","title":"<code>get_finally_hooks() -&gt; list[tuple[FinallyHook, Rule | None]]</code>","text":"<p>Get all finally hooks with their rules (returns copy).</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_finally_hooks(self) -&gt; list[tuple[FinallyHook, Rule | None]]:\n    \"\"\"Get all finally hooks with their rules (returns copy).\"\"\"\n    return self._finally.copy()\n</code></pre>"},{"location":"api/#hookedllm.core.InMemoryScopeRegistry","title":"<code>InMemoryScopeRegistry()</code>","text":"<p>Concrete implementation of ScopeRegistry.</p> <p>Single Responsibility: Only manages scope lifecycle. Creates and retrieves scopes, manages global scope.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def __init__(self):\n    self._scopes: dict[str, InMemoryScopeHookStore] = {}\n    self._global = InMemoryScopeHookStore(\"__global__\")\n</code></pre>"},{"location":"api/#hookedllm.core.InMemoryScopeRegistry-functions","title":"Functions","text":""},{"location":"api/#hookedllm.core.InMemoryScopeRegistry.get_global_scope","title":"<code>get_global_scope() -&gt; InMemoryScopeHookStore</code>","text":"<p>Get the global scope (always active).</p> <p>Returns:</p> Type Description <code>InMemoryScopeHookStore</code> <p>The global scope hook store</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_global_scope(self) -&gt; InMemoryScopeHookStore:\n    \"\"\"\n    Get the global scope (always active).\n\n    Returns:\n        The global scope hook store\n    \"\"\"\n    return self._global\n</code></pre>"},{"location":"api/#hookedllm.core.InMemoryScopeRegistry.get_scope","title":"<code>get_scope(name: str) -&gt; InMemoryScopeHookStore</code>","text":"<p>Get or create a named scope.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Scope name</p> required <p>Returns:</p> Type Description <code>InMemoryScopeHookStore</code> <p>Scope hook store for the named scope</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_scope(self, name: str) -&gt; InMemoryScopeHookStore:\n    \"\"\"\n    Get or create a named scope.\n\n    Args:\n        name: Scope name\n\n    Returns:\n        Scope hook store for the named scope\n    \"\"\"\n    if name not in self._scopes:\n        self._scopes[name] = InMemoryScopeHookStore(name)\n    return self._scopes[name]\n</code></pre>"},{"location":"api/#hookedllm.core.InMemoryScopeRegistry.get_scopes_for_client","title":"<code>get_scopes_for_client(scope_names: list[str] | None = None) -&gt; list[ScopeHookStore]</code>","text":"<p>Get list of scopes for a client.</p> <p>Always includes the global scope, plus any requested scopes.</p> <p>Parameters:</p> Name Type Description Default <code>scope_names</code> <code>list[str] | None</code> <p>List of scope names, or None for global only</p> <code>None</code> <p>Returns:</p> Type Description <code>list[ScopeHookStore]</code> <p>List of scope hook stores (global + requested scopes)</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_scopes_for_client(self, scope_names: list[str] | None = None) -&gt; list[ScopeHookStore]:\n    \"\"\"\n    Get list of scopes for a client.\n\n    Always includes the global scope, plus any requested scopes.\n\n    Args:\n        scope_names: List of scope names, or None for global only\n\n    Returns:\n        List of scope hook stores (global + requested scopes)\n    \"\"\"\n    scopes: list[ScopeHookStore] = [self._global]  # Always include global\n\n    if scope_names:\n        for name in scope_names:\n            scopes.append(self.get_scope(name))\n\n    return scopes\n</code></pre>"},{"location":"api/#hookedllm.core.InMemoryScopeRegistry.scope","title":"<code>scope(name: str) -&gt; InMemoryScopeHookStore</code>","text":"<p>Alias for get_scope for more fluent API.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def scope(self, name: str) -&gt; InMemoryScopeHookStore:\n    \"\"\"Alias for get_scope for more fluent API.\"\"\"\n    return self.get_scope(name)\n</code></pre>"},{"location":"api/#hookedllm.core.Message","title":"<code>Message(role: str, content: Any)</code>  <code>dataclass</code>","text":"<p>A single message in an LLM conversation.</p>"},{"location":"api/#hookedllm.core.MetadataRule","title":"<code>MetadataRule(conditions: dict[str, Any])</code>  <code>dataclass</code>","text":"<p>Match based on metadata key-value pairs.</p>"},{"location":"api/#hookedllm.core.ModelRule","title":"<code>ModelRule(models: list[str])</code>  <code>dataclass</code>","text":"<p>Match specific model(s).</p>"},{"location":"api/#hookedllm.core.NotRule","title":"<code>NotRule(rule: Any)</code>  <code>dataclass</code>","text":"<p>Negate a rule.</p>"},{"location":"api/#hookedllm.core.Rule","title":"<code>Rule</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for hook execution rules.</p> <p>Rules determine whether a hook should execute based on the call input and context.</p>"},{"location":"api/#hookedllm.core.Rule-functions","title":"Functions","text":""},{"location":"api/#hookedllm.core.Rule.__and__","title":"<code>__and__(other: Rule) -&gt; Rule</code>","text":"<p>Combine rules with AND logic.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def __and__(self, other: Rule) -&gt; Rule:\n    \"\"\"Combine rules with AND logic.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.Rule.__invert__","title":"<code>__invert__() -&gt; Rule</code>","text":"<p>Negate the rule (NOT logic).</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def __invert__(self) -&gt; Rule:\n    \"\"\"Negate the rule (NOT logic).\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.Rule.__or__","title":"<code>__or__(other: Rule) -&gt; Rule</code>","text":"<p>Combine rules with OR logic.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def __or__(self, other: Rule) -&gt; Rule:\n    \"\"\"Combine rules with OR logic.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.Rule.matches","title":"<code>matches(call_input: CallInput, context: CallContext) -&gt; bool</code>","text":"<p>Check if this rule matches the given call.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def matches(self, call_input: CallInput, context: CallContext) -&gt; bool:\n    \"\"\"Check if this rule matches the given call.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.RuleBuilder","title":"<code>RuleBuilder</code>","text":"<p>Fluent API for building rules.</p> <p>Used via the 'when' global instance.</p>"},{"location":"api/#hookedllm.core.RuleBuilder-functions","title":"Functions","text":""},{"location":"api/#hookedllm.core.RuleBuilder.always","title":"<code>always() -&gt; CustomRule</code>  <code>staticmethod</code>","text":"<p>Always matches.</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef always() -&gt; CustomRule:\n    \"\"\"Always matches.\"\"\"\n    return CustomRule(lambda i, c: True)\n</code></pre>"},{"location":"api/#hookedllm.core.RuleBuilder.custom","title":"<code>custom(predicate: Callable[[CallInput, CallContext], bool]) -&gt; CustomRule</code>  <code>staticmethod</code>","text":"<p>Custom predicate function.</p> Example <p>when.custom(lambda i, c: c.metadata.get(\"score\", 0) &gt; 0.8)</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef custom(predicate: Callable[[CallInput, CallContext], bool]) -&gt; CustomRule:\n    \"\"\"\n    Custom predicate function.\n\n    Example:\n        when.custom(lambda i, c: c.metadata.get(\"score\", 0) &gt; 0.8)\n    \"\"\"\n    return CustomRule(predicate)\n</code></pre>"},{"location":"api/#hookedllm.core.RuleBuilder.metadata","title":"<code>metadata(**conditions: Any) -&gt; MetadataRule</code>  <code>staticmethod</code>","text":"<p>Match metadata conditions.</p> Example <p>when.metadata(user_tier=\"premium\", region=\"us-east\")</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef metadata(**conditions: Any) -&gt; MetadataRule:\n    \"\"\"\n    Match metadata conditions.\n\n    Example:\n        when.metadata(user_tier=\"premium\", region=\"us-east\")\n    \"\"\"\n    return MetadataRule(conditions)\n</code></pre>"},{"location":"api/#hookedllm.core.RuleBuilder.model","title":"<code>model(*models: str) -&gt; ModelRule</code>  <code>staticmethod</code>","text":"<p>Match specific model(s).</p> Example <p>when.model(\"gpt-4\", \"gpt-4-turbo\")</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef model(*models: str) -&gt; ModelRule:\n    \"\"\"\n    Match specific model(s).\n\n    Example:\n        when.model(\"gpt-4\", \"gpt-4-turbo\")\n    \"\"\"\n    return ModelRule(list(models))\n</code></pre>"},{"location":"api/#hookedllm.core.RuleBuilder.never","title":"<code>never() -&gt; CustomRule</code>  <code>staticmethod</code>","text":"<p>Never matches.</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef never() -&gt; CustomRule:\n    \"\"\"Never matches.\"\"\"\n    return CustomRule(lambda i, c: False)\n</code></pre>"},{"location":"api/#hookedllm.core.RuleBuilder.tag","title":"<code>tag(*tags: str, all_: bool = False) -&gt; TagRule</code>  <code>staticmethod</code>","text":"<p>Match if context has tag(s).</p> <p>Parameters:</p> Name Type Description Default <code>*tags</code> <code>str</code> <p>Tag names to match</p> <code>()</code> <code>all_</code> <code>bool</code> <p>If True, all tags must be present. If False, any tag matches.</p> <code>False</code> Example <p>when.tag(\"production\", \"critical\")</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef tag(*tags: str, all_: bool = False) -&gt; TagRule:\n    \"\"\"\n    Match if context has tag(s).\n\n    Args:\n        *tags: Tag names to match\n        all_: If True, all tags must be present. If False, any tag matches.\n\n    Example:\n        when.tag(\"production\", \"critical\")\n    \"\"\"\n    return TagRule(list(tags), require_all=all_)\n</code></pre>"},{"location":"api/#hookedllm.core.ScopeHookStore","title":"<code>ScopeHookStore</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for storing hooks in a scope.</p> <p>Single Responsibility: Storage only, no execution logic.</p>"},{"location":"api/#hookedllm.core.ScopeHookStore-functions","title":"Functions","text":""},{"location":"api/#hookedllm.core.ScopeHookStore.add_after","title":"<code>add_after(hook: AfterHook, rule: Rule | None = None) -&gt; None</code>","text":"<p>Add an after hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def add_after(self, hook: AfterHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add an after hook with optional execution rule.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.ScopeHookStore.add_before","title":"<code>add_before(hook: BeforeHook, rule: Rule | None = None) -&gt; None</code>","text":"<p>Add a before hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def add_before(self, hook: BeforeHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add a before hook with optional execution rule.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.ScopeHookStore.add_error","title":"<code>add_error(hook: ErrorHook, rule: Rule | None = None) -&gt; None</code>","text":"<p>Add an error hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def add_error(self, hook: ErrorHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add an error hook with optional execution rule.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.ScopeHookStore.add_finally","title":"<code>add_finally(hook: FinallyHook, rule: Rule | None = None) -&gt; None</code>","text":"<p>Add a finally hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def add_finally(self, hook: FinallyHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add a finally hook with optional execution rule.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.ScopeHookStore.get_after_hooks","title":"<code>get_after_hooks() -&gt; list[tuple[AfterHook, Rule | None]]</code>","text":"<p>Get all after hooks with their rules.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_after_hooks(self) -&gt; list[tuple[AfterHook, Rule | None]]:\n    \"\"\"Get all after hooks with their rules.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.ScopeHookStore.get_before_hooks","title":"<code>get_before_hooks() -&gt; list[tuple[BeforeHook, Rule | None]]</code>","text":"<p>Get all before hooks with their rules.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_before_hooks(self) -&gt; list[tuple[BeforeHook, Rule | None]]:\n    \"\"\"Get all before hooks with their rules.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.ScopeHookStore.get_error_hooks","title":"<code>get_error_hooks() -&gt; list[tuple[ErrorHook, Rule | None]]</code>","text":"<p>Get all error hooks with their rules.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_error_hooks(self) -&gt; list[tuple[ErrorHook, Rule | None]]:\n    \"\"\"Get all error hooks with their rules.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.ScopeHookStore.get_finally_hooks","title":"<code>get_finally_hooks() -&gt; list[tuple[FinallyHook, Rule | None]]</code>","text":"<p>Get all finally hooks with their rules.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_finally_hooks(self) -&gt; list[tuple[FinallyHook, Rule | None]]:\n    \"\"\"Get all finally hooks with their rules.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.ScopeRegistry","title":"<code>ScopeRegistry</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for managing scopes.</p> <p>Single Responsibility: Scope lifecycle management only.</p>"},{"location":"api/#hookedllm.core.ScopeRegistry-functions","title":"Functions","text":""},{"location":"api/#hookedllm.core.ScopeRegistry.get_global_scope","title":"<code>get_global_scope() -&gt; ScopeHookStore</code>","text":"<p>Get the global scope (always active).</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_global_scope(self) -&gt; ScopeHookStore:\n    \"\"\"Get the global scope (always active).\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.ScopeRegistry.get_scope","title":"<code>get_scope(name: str) -&gt; ScopeHookStore</code>","text":"<p>Get or create a named scope.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_scope(self, name: str) -&gt; ScopeHookStore:\n    \"\"\"Get or create a named scope.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.ScopeRegistry.get_scopes_for_client","title":"<code>get_scopes_for_client(scope_names: list[str] | None) -&gt; list[ScopeHookStore]</code>","text":"<p>Get list of scopes for a client.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_scopes_for_client(self, scope_names: list[str] | None) -&gt; list[ScopeHookStore]:\n    \"\"\"Get list of scopes for a client.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.TagRule","title":"<code>TagRule(tags: list[str], require_all: bool = False)</code>  <code>dataclass</code>","text":"<p>Match if context has specific tag(s).</p>"},{"location":"api/#hookedllm.core-modules","title":"Modules","text":""},{"location":"api/#hookedllm.core.executor","title":"<code>executor</code>","text":"<p>Hook executor with dependency injection.</p> <p>Executes hooks with rule matching while isolating failures.</p>"},{"location":"api/#hookedllm.core.executor-classes","title":"Classes","text":""},{"location":"api/#hookedllm.core.executor.DefaultHookExecutor","title":"<code>DefaultHookExecutor(error_handler: Callable[[Exception, str], None] | None = None, logger: Any | None = None)</code>","text":"<p>Concrete hook executor with dependency injection.</p> <p>Single Responsibility: Only executes hooks, doesn't store them. Dependencies injected: error_handler, logger</p> <p>Guarantees: - Hook failures never break the main LLM call - Hooks execute in order - Rules are evaluated before execution</p> <p>Initialize executor with optional dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>error_handler</code> <code>Callable[[Exception, str], None] | None</code> <p>Called when a hook fails. Signature: (error, context_str)</p> <code>None</code> <code>logger</code> <code>Any | None</code> <p>Logger instance (must have .error() method)</p> <code>None</code> Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>def __init__(\n    self,\n    error_handler: Callable[[Exception, str], None] | None = None,\n    logger: Any | None = None,\n):\n    \"\"\"\n    Initialize executor with optional dependencies.\n\n    Args:\n        error_handler: Called when a hook fails. Signature: (error, context_str)\n        logger: Logger instance (must have .error() method)\n    \"\"\"\n    self._error_handler = error_handler or self._default_error_handler\n    self._logger = logger\n</code></pre> Functions\u00b6 <code>execute_after(hooks: list[tuple[AfterHook, Rule | None]], call_input: CallInput, call_output: CallOutput, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Execute after hooks with rule matching.</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[tuple[AfterHook, Rule | None]]</code> <p>List of (hook, rule) tuples</p> required <code>call_input</code> <code>CallInput</code> <p>The LLM call input</p> required <code>call_output</code> <code>CallOutput</code> <p>The LLM call output</p> required <code>context</code> <code>CallContext</code> <p>The call context</p> required Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>async def execute_after(\n    self,\n    hooks: list[tuple[AfterHook, Rule | None]],\n    call_input: CallInput,\n    call_output: CallOutput,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"\n    Execute after hooks with rule matching.\n\n    Args:\n        hooks: List of (hook, rule) tuples\n        call_input: The LLM call input\n        call_output: The LLM call output\n        context: The call context\n    \"\"\"\n    for hook, rule in hooks:\n        # Check if rule matches\n        if rule is None or rule.matches(call_input, context):\n            try:\n                await hook(call_input, call_output, context)\n            except Exception as e:\n                hook_name = getattr(hook, \"__name__\", str(hook))\n                self._error_handler(e, f\"After hook {hook_name}\")\n                if self._logger:\n                    self._logger.error(f\"After hook {hook_name} failed: {e}\")\n</code></pre> <code>execute_before(hooks: list[tuple[BeforeHook, Rule | None]], call_input: CallInput, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Execute before hooks with rule matching.</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[tuple[BeforeHook, Rule | None]]</code> <p>List of (hook, rule) tuples</p> required <code>call_input</code> <code>CallInput</code> <p>The LLM call input</p> required <code>context</code> <code>CallContext</code> <p>The call context</p> required Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>async def execute_before(\n    self,\n    hooks: list[tuple[BeforeHook, Rule | None]],\n    call_input: CallInput,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"\n    Execute before hooks with rule matching.\n\n    Args:\n        hooks: List of (hook, rule) tuples\n        call_input: The LLM call input\n        context: The call context\n    \"\"\"\n    for hook, rule in hooks:\n        # Check if rule matches\n        if rule is None or rule.matches(call_input, context):\n            try:\n                await hook(call_input, context)\n            except Exception as e:\n                hook_name = getattr(hook, \"__name__\", str(hook))\n                self._error_handler(e, f\"Before hook {hook_name}\")\n                if self._logger:\n                    self._logger.error(f\"Before hook {hook_name} failed: {e}\")\n</code></pre> <code>execute_error(hooks: list[tuple[ErrorHook, Rule | None]], call_input: CallInput, error: BaseException, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Execute error hooks with rule matching.</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[tuple[ErrorHook, Rule | None]]</code> <p>List of (hook, rule) tuples</p> required <code>call_input</code> <code>CallInput</code> <p>The LLM call input</p> required <code>error</code> <code>BaseException</code> <p>The error that occurred</p> required <code>context</code> <code>CallContext</code> <p>The call context</p> required Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>async def execute_error(\n    self,\n    hooks: list[tuple[ErrorHook, Rule | None]],\n    call_input: CallInput,\n    error: BaseException,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"\n    Execute error hooks with rule matching.\n\n    Args:\n        hooks: List of (hook, rule) tuples\n        call_input: The LLM call input\n        error: The error that occurred\n        context: The call context\n    \"\"\"\n    for hook, rule in hooks:\n        # Check if rule matches\n        if rule is None or rule.matches(call_input, context):\n            try:\n                await hook(call_input, error, context)\n            except Exception as e:\n                hook_name = getattr(hook, \"__name__\", str(hook))\n                self._error_handler(e, f\"Error hook {hook_name}\")\n                if self._logger:\n                    self._logger.error(f\"Error hook {hook_name} failed: {e}\")\n</code></pre> <code>execute_finally(hooks: list[tuple[FinallyHook, Rule | None]], result: CallResult) -&gt; None</code> <code>async</code> \u00b6 <p>Execute finally hooks.</p> <p>Note: Finally hooks don't use rule matching - they always run.</p> <p>Parameters:</p> Name Type Description Default <code>hooks</code> <code>list[tuple[FinallyHook, Rule | None]]</code> <p>List of (hook, rule) tuples (rule is ignored for finally hooks)</p> required <code>result</code> <code>CallResult</code> <p>The complete call result</p> required Source code in <code>src/hookedllm/core/executor.py</code> <pre><code>async def execute_finally(\n    self, hooks: list[tuple[FinallyHook, Rule | None]], result: CallResult\n) -&gt; None:\n    \"\"\"\n    Execute finally hooks.\n\n    Note: Finally hooks don't use rule matching - they always run.\n\n    Args:\n        hooks: List of (hook, rule) tuples (rule is ignored for finally hooks)\n        result: The complete call result\n    \"\"\"\n    for hook, _rule in hooks:\n        # Finally hooks always run, ignore rule\n        try:\n            await hook(result)\n        except Exception as e:\n            hook_name = getattr(hook, \"__name__\", str(hook))\n            self._error_handler(e, f\"Finally hook {hook_name}\")\n            if self._logger:\n                self._logger.error(f\"Finally hook {hook_name} failed: {e}\")\n</code></pre>"},{"location":"api/#hookedllm.core.protocols","title":"<code>protocols</code>","text":"<p>Protocol definitions for hooks and dependency injection.</p> <p>These protocols define the interfaces that components must implement, following the Dependency Inversion Principle.</p>"},{"location":"api/#hookedllm.core.protocols-classes","title":"Classes","text":""},{"location":"api/#hookedllm.core.protocols.HookExecutor","title":"<code>HookExecutor</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for executing hooks.</p> <p>Single Responsibility: Hook execution only, no storage.</p> Functions\u00b6 <code>execute_after(hooks: list[tuple[AfterHook, Rule | None]], call_input: CallInput, call_output: CallOutput, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Execute after hooks with rule matching.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>async def execute_after(\n    self,\n    hooks: list[tuple[AfterHook, Rule | None]],\n    call_input: CallInput,\n    call_output: CallOutput,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"Execute after hooks with rule matching.\"\"\"\n    ...\n</code></pre> <code>execute_before(hooks: list[tuple[BeforeHook, Rule | None]], call_input: CallInput, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Execute before hooks with rule matching.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>async def execute_before(\n    self,\n    hooks: list[tuple[BeforeHook, Rule | None]],\n    call_input: CallInput,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"Execute before hooks with rule matching.\"\"\"\n    ...\n</code></pre> <code>execute_error(hooks: list[tuple[ErrorHook, Rule | None]], call_input: CallInput, error: BaseException, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Execute error hooks with rule matching.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>async def execute_error(\n    self,\n    hooks: list[tuple[ErrorHook, Rule | None]],\n    call_input: CallInput,\n    error: BaseException,\n    context: CallContext,\n) -&gt; None:\n    \"\"\"Execute error hooks with rule matching.\"\"\"\n    ...\n</code></pre> <code>execute_finally(hooks: list[tuple[FinallyHook, Rule | None]], result: CallResult) -&gt; None</code> <code>async</code> \u00b6 <p>Execute finally hooks.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>async def execute_finally(\n    self, hooks: list[tuple[FinallyHook, Rule | None]], result: CallResult\n) -&gt; None:\n    \"\"\"Execute finally hooks.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.protocols.Rule","title":"<code>Rule</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for hook execution rules.</p> <p>Rules determine whether a hook should execute based on the call input and context.</p> Functions\u00b6 <code>__and__(other: Rule) -&gt; Rule</code> \u00b6 <p>Combine rules with AND logic.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def __and__(self, other: Rule) -&gt; Rule:\n    \"\"\"Combine rules with AND logic.\"\"\"\n    ...\n</code></pre> <code>__invert__() -&gt; Rule</code> \u00b6 <p>Negate the rule (NOT logic).</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def __invert__(self) -&gt; Rule:\n    \"\"\"Negate the rule (NOT logic).\"\"\"\n    ...\n</code></pre> <code>__or__(other: Rule) -&gt; Rule</code> \u00b6 <p>Combine rules with OR logic.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def __or__(self, other: Rule) -&gt; Rule:\n    \"\"\"Combine rules with OR logic.\"\"\"\n    ...\n</code></pre> <code>matches(call_input: CallInput, context: CallContext) -&gt; bool</code> \u00b6 <p>Check if this rule matches the given call.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def matches(self, call_input: CallInput, context: CallContext) -&gt; bool:\n    \"\"\"Check if this rule matches the given call.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.protocols.ScopeHookStore","title":"<code>ScopeHookStore</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for storing hooks in a scope.</p> <p>Single Responsibility: Storage only, no execution logic.</p> Functions\u00b6 <code>add_after(hook: AfterHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add an after hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def add_after(self, hook: AfterHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add an after hook with optional execution rule.\"\"\"\n    ...\n</code></pre> <code>add_before(hook: BeforeHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add a before hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def add_before(self, hook: BeforeHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add a before hook with optional execution rule.\"\"\"\n    ...\n</code></pre> <code>add_error(hook: ErrorHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add an error hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def add_error(self, hook: ErrorHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add an error hook with optional execution rule.\"\"\"\n    ...\n</code></pre> <code>add_finally(hook: FinallyHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add a finally hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def add_finally(self, hook: FinallyHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add a finally hook with optional execution rule.\"\"\"\n    ...\n</code></pre> <code>get_after_hooks() -&gt; list[tuple[AfterHook, Rule | None]]</code> \u00b6 <p>Get all after hooks with their rules.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_after_hooks(self) -&gt; list[tuple[AfterHook, Rule | None]]:\n    \"\"\"Get all after hooks with their rules.\"\"\"\n    ...\n</code></pre> <code>get_before_hooks() -&gt; list[tuple[BeforeHook, Rule | None]]</code> \u00b6 <p>Get all before hooks with their rules.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_before_hooks(self) -&gt; list[tuple[BeforeHook, Rule | None]]:\n    \"\"\"Get all before hooks with their rules.\"\"\"\n    ...\n</code></pre> <code>get_error_hooks() -&gt; list[tuple[ErrorHook, Rule | None]]</code> \u00b6 <p>Get all error hooks with their rules.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_error_hooks(self) -&gt; list[tuple[ErrorHook, Rule | None]]:\n    \"\"\"Get all error hooks with their rules.\"\"\"\n    ...\n</code></pre> <code>get_finally_hooks() -&gt; list[tuple[FinallyHook, Rule | None]]</code> \u00b6 <p>Get all finally hooks with their rules.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_finally_hooks(self) -&gt; list[tuple[FinallyHook, Rule | None]]:\n    \"\"\"Get all finally hooks with their rules.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.protocols.ScopeRegistry","title":"<code>ScopeRegistry</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for managing scopes.</p> <p>Single Responsibility: Scope lifecycle management only.</p> Functions\u00b6 <code>get_global_scope() -&gt; ScopeHookStore</code> \u00b6 <p>Get the global scope (always active).</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_global_scope(self) -&gt; ScopeHookStore:\n    \"\"\"Get the global scope (always active).\"\"\"\n    ...\n</code></pre> <code>get_scope(name: str) -&gt; ScopeHookStore</code> \u00b6 <p>Get or create a named scope.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_scope(self, name: str) -&gt; ScopeHookStore:\n    \"\"\"Get or create a named scope.\"\"\"\n    ...\n</code></pre> <code>get_scopes_for_client(scope_names: list[str] | None) -&gt; list[ScopeHookStore]</code> \u00b6 <p>Get list of scopes for a client.</p> Source code in <code>src/hookedllm/core/protocols.py</code> <pre><code>def get_scopes_for_client(self, scope_names: list[str] | None) -&gt; list[ScopeHookStore]:\n    \"\"\"Get list of scopes for a client.\"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.core.rules","title":"<code>rules</code>","text":"<p>Rule system for conditional hook execution.</p> <p>Rules determine when hooks should execute based on call input and context. Supports composition via AND, OR, NOT operations.</p>"},{"location":"api/#hookedllm.core.rules-classes","title":"Classes","text":""},{"location":"api/#hookedllm.core.rules.CompositeRule","title":"<code>CompositeRule(rules: list[Any], operator: Literal['and', 'or'])</code>  <code>dataclass</code>","text":"<p>Combine multiple rules with AND/OR logic.</p>"},{"location":"api/#hookedllm.core.rules.CustomRule","title":"<code>CustomRule(predicate: Callable[[CallInput, CallContext], bool])</code>  <code>dataclass</code>","text":"<p>Custom predicate function.</p>"},{"location":"api/#hookedllm.core.rules.MetadataRule","title":"<code>MetadataRule(conditions: dict[str, Any])</code>  <code>dataclass</code>","text":"<p>Match based on metadata key-value pairs.</p>"},{"location":"api/#hookedllm.core.rules.ModelRule","title":"<code>ModelRule(models: list[str])</code>  <code>dataclass</code>","text":"<p>Match specific model(s).</p>"},{"location":"api/#hookedllm.core.rules.NotRule","title":"<code>NotRule(rule: Any)</code>  <code>dataclass</code>","text":"<p>Negate a rule.</p>"},{"location":"api/#hookedllm.core.rules.RuleBuilder","title":"<code>RuleBuilder</code>","text":"<p>Fluent API for building rules.</p> <p>Used via the 'when' global instance.</p> Functions\u00b6 <code>always() -&gt; CustomRule</code> <code>staticmethod</code> \u00b6 <p>Always matches.</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef always() -&gt; CustomRule:\n    \"\"\"Always matches.\"\"\"\n    return CustomRule(lambda i, c: True)\n</code></pre> <code>custom(predicate: Callable[[CallInput, CallContext], bool]) -&gt; CustomRule</code> <code>staticmethod</code> \u00b6 <p>Custom predicate function.</p> Example <p>when.custom(lambda i, c: c.metadata.get(\"score\", 0) &gt; 0.8)</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef custom(predicate: Callable[[CallInput, CallContext], bool]) -&gt; CustomRule:\n    \"\"\"\n    Custom predicate function.\n\n    Example:\n        when.custom(lambda i, c: c.metadata.get(\"score\", 0) &gt; 0.8)\n    \"\"\"\n    return CustomRule(predicate)\n</code></pre> <code>metadata(**conditions: Any) -&gt; MetadataRule</code> <code>staticmethod</code> \u00b6 <p>Match metadata conditions.</p> Example <p>when.metadata(user_tier=\"premium\", region=\"us-east\")</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef metadata(**conditions: Any) -&gt; MetadataRule:\n    \"\"\"\n    Match metadata conditions.\n\n    Example:\n        when.metadata(user_tier=\"premium\", region=\"us-east\")\n    \"\"\"\n    return MetadataRule(conditions)\n</code></pre> <code>model(*models: str) -&gt; ModelRule</code> <code>staticmethod</code> \u00b6 <p>Match specific model(s).</p> Example <p>when.model(\"gpt-4\", \"gpt-4-turbo\")</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef model(*models: str) -&gt; ModelRule:\n    \"\"\"\n    Match specific model(s).\n\n    Example:\n        when.model(\"gpt-4\", \"gpt-4-turbo\")\n    \"\"\"\n    return ModelRule(list(models))\n</code></pre> <code>never() -&gt; CustomRule</code> <code>staticmethod</code> \u00b6 <p>Never matches.</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef never() -&gt; CustomRule:\n    \"\"\"Never matches.\"\"\"\n    return CustomRule(lambda i, c: False)\n</code></pre> <code>tag(*tags: str, all_: bool = False) -&gt; TagRule</code> <code>staticmethod</code> \u00b6 <p>Match if context has tag(s).</p> <p>Parameters:</p> Name Type Description Default <code>*tags</code> <code>str</code> <p>Tag names to match</p> <code>()</code> <code>all_</code> <code>bool</code> <p>If True, all tags must be present. If False, any tag matches.</p> <code>False</code> Example <p>when.tag(\"production\", \"critical\")</p> Source code in <code>src/hookedllm/core/rules.py</code> <pre><code>@staticmethod\ndef tag(*tags: str, all_: bool = False) -&gt; TagRule:\n    \"\"\"\n    Match if context has tag(s).\n\n    Args:\n        *tags: Tag names to match\n        all_: If True, all tags must be present. If False, any tag matches.\n\n    Example:\n        when.tag(\"production\", \"critical\")\n    \"\"\"\n    return TagRule(list(tags), require_all=all_)\n</code></pre>"},{"location":"api/#hookedllm.core.rules.TagRule","title":"<code>TagRule(tags: list[str], require_all: bool = False)</code>  <code>dataclass</code>","text":"<p>Match if context has specific tag(s).</p>"},{"location":"api/#hookedllm.core.scopes","title":"<code>scopes</code>","text":"<p>Scope management for isolated hook execution.</p> <p>Scopes allow hooks to be registered and executed only for specific clients, preventing interference across different application contexts.</p>"},{"location":"api/#hookedllm.core.scopes-classes","title":"Classes","text":""},{"location":"api/#hookedllm.core.scopes.InMemoryScopeHookStore","title":"<code>InMemoryScopeHookStore(scope_name: str)</code>","text":"<p>Concrete implementation of ScopeHookStore.</p> <p>Single Responsibility: Only stores hooks. Stores hooks in memory with their associated rules.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def __init__(self, scope_name: str):\n    self._scope_name = scope_name\n    self._before: list[tuple[BeforeHook, Rule | None]] = []\n    self._after: list[tuple[AfterHook, Rule | None]] = []\n    self._error: list[tuple[ErrorHook, Rule | None]] = []\n    self._finally: list[tuple[FinallyHook, Rule | None]] = []\n</code></pre> Attributes\u00b6 <code>name: str</code> <code>property</code> \u00b6 <p>Get the scope name.</p> Functions\u00b6 <code>add_after(hook: AfterHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add an after hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def add_after(self, hook: AfterHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add an after hook with optional execution rule.\"\"\"\n    self._after.append((hook, rule))\n</code></pre> <code>add_before(hook: BeforeHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add a before hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def add_before(self, hook: BeforeHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add a before hook with optional execution rule.\"\"\"\n    self._before.append((hook, rule))\n</code></pre> <code>add_error(hook: ErrorHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add an error hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def add_error(self, hook: ErrorHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add an error hook with optional execution rule.\"\"\"\n    self._error.append((hook, rule))\n</code></pre> <code>add_finally(hook: FinallyHook, rule: Rule | None = None) -&gt; None</code> \u00b6 <p>Add a finally hook with optional execution rule.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def add_finally(self, hook: FinallyHook, rule: Rule | None = None) -&gt; None:\n    \"\"\"Add a finally hook with optional execution rule.\"\"\"\n    self._finally.append((hook, rule))\n</code></pre> <code>after(hook: AfterHook, *, when: Rule | None = None) -&gt; None</code> \u00b6 <p>Alias for add_after with keyword 'when' parameter.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def after(self, hook: AfterHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"Alias for add_after with keyword 'when' parameter.\"\"\"\n    self.add_after(hook, when)\n</code></pre> <code>before(hook: BeforeHook, *, when: Rule | None = None) -&gt; None</code> \u00b6 <p>Alias for add_before with keyword 'when' parameter.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def before(self, hook: BeforeHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"Alias for add_before with keyword 'when' parameter.\"\"\"\n    self.add_before(hook, when)\n</code></pre> <code>error(hook: ErrorHook, *, when: Rule | None = None) -&gt; None</code> \u00b6 <p>Alias for add_error with keyword 'when' parameter.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def error(self, hook: ErrorHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"Alias for add_error with keyword 'when' parameter.\"\"\"\n    self.add_error(hook, when)\n</code></pre> <code>finally_(hook: FinallyHook, *, when: Rule | None = None) -&gt; None</code> \u00b6 <p>Alias for add_finally with keyword 'when' parameter.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def finally_(self, hook: FinallyHook, *, when: Rule | None = None) -&gt; None:\n    \"\"\"Alias for add_finally with keyword 'when' parameter.\"\"\"\n    self.add_finally(hook, when)\n</code></pre> <code>get_after_hooks() -&gt; list[tuple[AfterHook, Rule | None]]</code> \u00b6 <p>Get all after hooks with their rules (returns copy).</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_after_hooks(self) -&gt; list[tuple[AfterHook, Rule | None]]:\n    \"\"\"Get all after hooks with their rules (returns copy).\"\"\"\n    return self._after.copy()\n</code></pre> <code>get_before_hooks() -&gt; list[tuple[BeforeHook, Rule | None]]</code> \u00b6 <p>Get all before hooks with their rules (returns copy).</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_before_hooks(self) -&gt; list[tuple[BeforeHook, Rule | None]]:\n    \"\"\"Get all before hooks with their rules (returns copy).\"\"\"\n    return self._before.copy()\n</code></pre> <code>get_error_hooks() -&gt; list[tuple[ErrorHook, Rule | None]]</code> \u00b6 <p>Get all error hooks with their rules (returns copy).</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_error_hooks(self) -&gt; list[tuple[ErrorHook, Rule | None]]:\n    \"\"\"Get all error hooks with their rules (returns copy).\"\"\"\n    return self._error.copy()\n</code></pre> <code>get_finally_hooks() -&gt; list[tuple[FinallyHook, Rule | None]]</code> \u00b6 <p>Get all finally hooks with their rules (returns copy).</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_finally_hooks(self) -&gt; list[tuple[FinallyHook, Rule | None]]:\n    \"\"\"Get all finally hooks with their rules (returns copy).\"\"\"\n    return self._finally.copy()\n</code></pre>"},{"location":"api/#hookedllm.core.scopes.InMemoryScopeRegistry","title":"<code>InMemoryScopeRegistry()</code>","text":"<p>Concrete implementation of ScopeRegistry.</p> <p>Single Responsibility: Only manages scope lifecycle. Creates and retrieves scopes, manages global scope.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def __init__(self):\n    self._scopes: dict[str, InMemoryScopeHookStore] = {}\n    self._global = InMemoryScopeHookStore(\"__global__\")\n</code></pre> Functions\u00b6 <code>get_global_scope() -&gt; InMemoryScopeHookStore</code> \u00b6 <p>Get the global scope (always active).</p> <p>Returns:</p> Type Description <code>InMemoryScopeHookStore</code> <p>The global scope hook store</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_global_scope(self) -&gt; InMemoryScopeHookStore:\n    \"\"\"\n    Get the global scope (always active).\n\n    Returns:\n        The global scope hook store\n    \"\"\"\n    return self._global\n</code></pre> <code>get_scope(name: str) -&gt; InMemoryScopeHookStore</code> \u00b6 <p>Get or create a named scope.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Scope name</p> required <p>Returns:</p> Type Description <code>InMemoryScopeHookStore</code> <p>Scope hook store for the named scope</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_scope(self, name: str) -&gt; InMemoryScopeHookStore:\n    \"\"\"\n    Get or create a named scope.\n\n    Args:\n        name: Scope name\n\n    Returns:\n        Scope hook store for the named scope\n    \"\"\"\n    if name not in self._scopes:\n        self._scopes[name] = InMemoryScopeHookStore(name)\n    return self._scopes[name]\n</code></pre> <code>get_scopes_for_client(scope_names: list[str] | None = None) -&gt; list[ScopeHookStore]</code> \u00b6 <p>Get list of scopes for a client.</p> <p>Always includes the global scope, plus any requested scopes.</p> <p>Parameters:</p> Name Type Description Default <code>scope_names</code> <code>list[str] | None</code> <p>List of scope names, or None for global only</p> <code>None</code> <p>Returns:</p> Type Description <code>list[ScopeHookStore]</code> <p>List of scope hook stores (global + requested scopes)</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def get_scopes_for_client(self, scope_names: list[str] | None = None) -&gt; list[ScopeHookStore]:\n    \"\"\"\n    Get list of scopes for a client.\n\n    Always includes the global scope, plus any requested scopes.\n\n    Args:\n        scope_names: List of scope names, or None for global only\n\n    Returns:\n        List of scope hook stores (global + requested scopes)\n    \"\"\"\n    scopes: list[ScopeHookStore] = [self._global]  # Always include global\n\n    if scope_names:\n        for name in scope_names:\n            scopes.append(self.get_scope(name))\n\n    return scopes\n</code></pre> <code>scope(name: str) -&gt; InMemoryScopeHookStore</code> \u00b6 <p>Alias for get_scope for more fluent API.</p> Source code in <code>src/hookedllm/core/scopes.py</code> <pre><code>def scope(self, name: str) -&gt; InMemoryScopeHookStore:\n    \"\"\"Alias for get_scope for more fluent API.\"\"\"\n    return self.get_scope(name)\n</code></pre>"},{"location":"api/#hookedllm.core.types","title":"<code>types</code>","text":"<p>Core data types for hookedllm.</p> <p>These types represent the data that flows through the hook system.</p>"},{"location":"api/#hookedllm.core.types-classes","title":"Classes","text":""},{"location":"api/#hookedllm.core.types.CallContext","title":"<code>CallContext(call_id: str = (lambda: str(uuid4()))(), parent_id: str | None = None, provider: str = '', model: str = '', route: str = 'chat', tags: list[str] = list(), started_at: datetime = (lambda: datetime.now(timezone.utc))(), metadata: dict[str, Any] = dict())</code>  <code>dataclass</code>","text":"<p>Context for a single LLM call lifecycle.</p> <p>Contains metadata about the call including timing, tags, and custom metadata.</p>"},{"location":"api/#hookedllm.core.types.CallInput","title":"<code>CallInput(model: str, messages: Sequence[Message], params: dict[str, Any] = dict(), metadata: dict[str, Any] = dict())</code>  <code>dataclass</code>","text":"<p>Normalized input for an LLM call.</p> <p>This represents the input parameters in a provider-agnostic way.</p>"},{"location":"api/#hookedllm.core.types.CallOutput","title":"<code>CallOutput(text: str | None, raw: Any, usage: dict[str, Any] | None = None, finish_reason: str | None = None)</code>  <code>dataclass</code>","text":"<p>Normalized output from an LLM call.</p> <p>This represents the response in a provider-agnostic way while preserving the original response object.</p>"},{"location":"api/#hookedllm.core.types.CallResult","title":"<code>CallResult(input: CallInput, output: CallOutput | None, context: CallContext, error: BaseException | None, ended_at: datetime, elapsed_ms: float)</code>  <code>dataclass</code>","text":"<p>Complete result of an LLM call.</p> <p>Contains the input, output, context, any error that occurred, and timing information. This is passed to finally hooks.</p>"},{"location":"api/#hookedllm.core.types.Message","title":"<code>Message(role: str, content: Any)</code>  <code>dataclass</code>","text":"<p>A single message in an LLM conversation.</p>"},{"location":"api/#hookedllm.core.wrapper","title":"<code>wrapper</code>","text":"<p>Transparent wrapper for intercepting LLM API calls.</p> <p>Wraps provider clients to inject hook execution while preserving the original SDK interface and return types.</p>"},{"location":"api/#hookedllm.core.wrapper-classes","title":"Classes","text":""},{"location":"api/#hookedllm.core.wrapper.HookedClientWrapper","title":"<code>HookedClientWrapper(original_client: Any, scopes: list[ScopeHookStore], executor: HookExecutor)</code>","text":"<p>Transparent proxy with all dependencies injected.</p> <p>No global state - all dependencies passed via constructor (DI). Intercepts provider SDK methods to inject hook execution.</p> <p>Initialize wrapper with injected dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>original_client</code> <code>Any</code> <p>The original provider client</p> required <code>scopes</code> <code>list[ScopeHookStore]</code> <p>List of scope hook stores to use</p> required <code>executor</code> <code>HookExecutor</code> <p>Hook executor instance</p> required Source code in <code>src/hookedllm/core/wrapper.py</code> <pre><code>def __init__(self, original_client: Any, scopes: list[ScopeHookStore], executor: HookExecutor):\n    \"\"\"\n    Initialize wrapper with injected dependencies.\n\n    Args:\n        original_client: The original provider client\n        scopes: List of scope hook stores to use\n        executor: Hook executor instance\n    \"\"\"\n    self._original = original_client\n    self._scopes = scopes\n    self._executor = executor\n    self._adapter = _detect_provider_adapter(original_client)\n    self._wrapper_path = self._adapter.get_wrapper_path(original_client)\n</code></pre> Functions\u00b6 <code>__getattr__(name: str) -&gt; Any</code> \u00b6 <p>Intercept attribute access.</p> <p>Wraps attributes based on the detected provider's wrapper path.</p> Source code in <code>src/hookedllm/core/wrapper.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"\n    Intercept attribute access.\n\n    Wraps attributes based on the detected provider's wrapper path.\n    \"\"\"\n    attr = getattr(self._original, name)\n\n    # Check if this attribute is part of the wrapper path\n    if len(self._wrapper_path) &gt; 0 and name == self._wrapper_path[0]:\n        # Create a dynamic wrapper for the next level\n        return _create_wrapper_for_path(\n            attr, self._wrapper_path[1:], self._scopes, self._executor, self._adapter\n        )\n\n    return attr\n</code></pre>"},{"location":"api/#hookedllm.core.wrapper.HookedCompletionsWrapper","title":"<code>HookedCompletionsWrapper(original_completions: Any, scopes: list[ScopeHookStore], executor: HookExecutor, adapter: Any)</code>","text":"<p>Wraps completions.create() with hook execution.</p> <p>All dependencies injected, no global state.</p> <p>Initialize completions wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>original_completions</code> <code>Any</code> <p>Original completions object from SDK</p> required <code>scopes</code> <code>list[ScopeHookStore]</code> <p>List of scope hook stores</p> required <code>executor</code> <code>HookExecutor</code> <p>Hook executor instance</p> required <code>adapter</code> <code>Any</code> <p>Provider adapter instance</p> required Source code in <code>src/hookedllm/core/wrapper.py</code> <pre><code>def __init__(\n    self,\n    original_completions: Any,\n    scopes: list[ScopeHookStore],\n    executor: HookExecutor,\n    adapter: Any,\n):\n    \"\"\"\n    Initialize completions wrapper.\n\n    Args:\n        original_completions: Original completions object from SDK\n        scopes: List of scope hook stores\n        executor: Hook executor instance\n        adapter: Provider adapter instance\n    \"\"\"\n    self._original = original_completions\n    self._scopes = scopes\n    self._executor = executor\n    self._adapter = adapter\n</code></pre> Functions\u00b6 <code>__getattr__(name: str) -&gt; Any</code> \u00b6 <p>Pass through other attributes to original object.</p> Source code in <code>src/hookedllm/core/wrapper.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"Pass through other attributes to original object.\"\"\"\n    return getattr(self._original, name)\n</code></pre> <code>create(*, model: str, messages: list[dict], **kwargs: Any) -&gt; Any</code> <code>async</code> \u00b6 <p>Hooked create method.</p> <p>Flow: 1. Use adapter to normalize input (extracts tags, metadata, creates CallInput/Context) 2. Collect all hooks from all scopes 3. Execute before hooks 4. Call original SDK method 5. Use adapter to normalize output 6. Execute after hooks (on success) or error hooks (on failure) 7. Always execute finally hooks 8. Return original SDK response type</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name</p> required <code>messages</code> <code>list[dict]</code> <p>List of message dicts</p> required <code>**kwargs</code> <code>Any</code> <p>Other parameters passed to SDK</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Original SDK response object</p> Source code in <code>src/hookedllm/core/wrapper.py</code> <pre><code>async def create(self, *, model: str, messages: list[dict], **kwargs: Any) -&gt; Any:\n    \"\"\"\n    Hooked create method.\n\n    Flow:\n    1. Use adapter to normalize input (extracts tags, metadata, creates CallInput/Context)\n    2. Collect all hooks from all scopes\n    3. Execute before hooks\n    4. Call original SDK method\n    5. Use adapter to normalize output\n    6. Execute after hooks (on success) or error hooks (on failure)\n    7. Always execute finally hooks\n    8. Return original SDK response type\n\n    Args:\n        model: Model name\n        messages: List of message dicts\n        **kwargs: Other parameters passed to SDK\n\n    Returns:\n        Original SDK response object\n    \"\"\"\n    # Use adapter to normalize input\n    call_input, context = self._adapter.normalize_input(\n        self._adapter.PROVIDER_NAME,\n        self._original.create,\n        model=model,\n        messages=messages,\n        **kwargs,\n    )\n\n    # Collect all hooks from all scopes\n    all_before = []\n    all_after = []\n    all_error = []\n    all_finally = []\n\n    for scope in self._scopes:\n        all_before.extend(scope.get_before_hooks())\n        all_after.extend(scope.get_after_hooks())\n        all_error.extend(scope.get_error_hooks())\n        all_finally.extend(scope.get_finally_hooks())\n\n    # Execute hook flow\n    t0 = time.perf_counter()\n    output = None\n    error = None\n\n    try:\n        # Before hooks\n        await self._executor.execute_before(all_before, call_input, context)\n\n        # Original SDK call\n        response = await self._original.create(model=model, messages=messages, **kwargs)\n\n        # Use adapter to normalize output\n        output = self._adapter.normalize_output(response)\n\n        # After hooks\n        await self._executor.execute_after(all_after, call_input, output, context)\n\n        return response  # Return ORIGINAL SDK response!\n\n    except BaseException as e:\n        error = e\n        await self._executor.execute_error(all_error, call_input, e, context)\n        raise\n\n    finally:\n        elapsed = (time.perf_counter() - t0) * 1000.0\n        result = CallResult(\n            input=call_input,\n            output=output,\n            context=context,\n            error=error,\n            ended_at=datetime.now(timezone.utc),\n            elapsed_ms=elapsed,\n        )\n        await self._executor.execute_finally(all_finally, result)\n</code></pre>"},{"location":"api/#hookedllm.core.wrapper.HookedPathWrapper","title":"<code>HookedPathWrapper(original_obj: Any, remaining_path: list[str], scopes: list[ScopeHookStore], executor: HookExecutor, adapter: Any)</code>","text":"<p>Intermediate wrapper for provider-specific attribute paths.</p> <p>Handles paths like [\"chat\", \"completions\"] for OpenAI or [\"messages\"] for Anthropic.</p> <p>Initialize path wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>original_obj</code> <code>Any</code> <p>The object being wrapped</p> required <code>remaining_path</code> <code>list[str]</code> <p>Remaining attribute names to intercept</p> required <code>scopes</code> <code>list[ScopeHookStore]</code> <p>List of scope hook stores</p> required <code>executor</code> <code>HookExecutor</code> <p>Hook executor instance</p> required <code>adapter</code> <code>Any</code> <p>Provider adapter instance</p> required Source code in <code>src/hookedllm/core/wrapper.py</code> <pre><code>def __init__(\n    self,\n    original_obj: Any,\n    remaining_path: list[str],\n    scopes: list[ScopeHookStore],\n    executor: HookExecutor,\n    adapter: Any,\n):\n    \"\"\"\n    Initialize path wrapper.\n\n    Args:\n        original_obj: The object being wrapped\n        remaining_path: Remaining attribute names to intercept\n        scopes: List of scope hook stores\n        executor: Hook executor instance\n        adapter: Provider adapter instance\n    \"\"\"\n    self._original = original_obj\n    self._remaining_path = remaining_path\n    self._scopes = scopes\n    self._executor = executor\n    self._adapter = adapter\n</code></pre> Functions\u00b6 <code>__getattr__(name: str) -&gt; Any</code> \u00b6 <p>Intercept attribute access.</p> <p>If the attribute matches the next in the path, wrap it. Otherwise pass through.</p> Source code in <code>src/hookedllm/core/wrapper.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"\n    Intercept attribute access.\n\n    If the attribute matches the next in the path, wrap it.\n    Otherwise pass through.\n    \"\"\"\n    attr = getattr(self._original, name)\n\n    if len(self._remaining_path) &gt; 0 and name == self._remaining_path[0]:\n        # Continue wrapping down the path\n        return _create_wrapper_for_path(\n            attr, self._remaining_path[1:], self._scopes, self._executor, self._adapter\n        )\n\n    return attr\n</code></pre>"},{"location":"api/#hooks","title":"Hooks","text":""},{"location":"api/#hookedllm.hooks","title":"<code>hooks</code>","text":"<p>Built-in hook helpers for common use cases.</p>"},{"location":"api/#hookedllm.hooks-classes","title":"Classes","text":""},{"location":"api/#hookedllm.hooks.EvaluationHook","title":"<code>EvaluationHook(evaluator_client: Any, criteria: dict[str, str], model: str = 'gpt-4o-mini', store_in_metadata: bool = True)</code>","text":"<p>Evaluate LLM responses using another LLM.</p> <p>This is an after hook that calls a separate \"evaluator\" LLM to assess the quality of responses based on configurable criteria.</p> Usage <p>from openai import AsyncOpenAI</p> <p>evaluator = AsyncOpenAI()  # Separate client for evaluation criteria = {     \"clarity\": \"Is the response clear and easy to understand?\",     \"accuracy\": \"Is the response factually accurate?\",     \"relevance\": \"Does the response address the user's question?\" }</p> <p>eval_hook = EvaluationHook(evaluator, criteria) hookedllm.scope(\"evaluation\").after(eval_hook)</p> <p>Initialize evaluation hook.</p> <p>Parameters:</p> Name Type Description Default <code>evaluator_client</code> <code>Any</code> <p>OpenAI-compatible client for evaluation calls</p> required <code>criteria</code> <code>dict[str, str]</code> <p>Dict mapping criterion name to description</p> required <code>model</code> <code>str</code> <p>Model to use for evaluation (default: gpt-4o-mini)</p> <code>'gpt-4o-mini'</code> <code>store_in_metadata</code> <code>bool</code> <p>If True, store results in context.metadata</p> <code>True</code> Source code in <code>src/hookedllm/hooks/evaluation.py</code> <pre><code>def __init__(\n    self,\n    evaluator_client: Any,\n    criteria: dict[str, str],\n    model: str = \"gpt-4o-mini\",\n    store_in_metadata: bool = True,\n):\n    \"\"\"\n    Initialize evaluation hook.\n\n    Args:\n        evaluator_client: OpenAI-compatible client for evaluation calls\n        criteria: Dict mapping criterion name to description\n        model: Model to use for evaluation (default: gpt-4o-mini)\n        store_in_metadata: If True, store results in context.metadata\n    \"\"\"\n    self.evaluator = evaluator_client\n    self.criteria = criteria\n    self.model = model\n    self.store_in_metadata = store_in_metadata\n</code></pre>"},{"location":"api/#hookedllm.hooks.EvaluationHook-functions","title":"Functions","text":""},{"location":"api/#hookedllm.hooks.EvaluationHook.__call__","title":"<code>__call__(call_input: CallInput, call_output: CallOutput, context: CallContext) -&gt; None</code>  <code>async</code>","text":"<p>Evaluate the LLM response.</p> <p>Parameters:</p> Name Type Description Default <code>call_input</code> <code>CallInput</code> <p>The original call input</p> required <code>call_output</code> <code>CallOutput</code> <p>The LLM response</p> required <code>context</code> <code>CallContext</code> <p>The call context</p> required Source code in <code>src/hookedllm/hooks/evaluation.py</code> <pre><code>async def __call__(\n    self, call_input: CallInput, call_output: CallOutput, context: CallContext\n) -&gt; None:\n    \"\"\"\n    Evaluate the LLM response.\n\n    Args:\n        call_input: The original call input\n        call_output: The LLM response\n        context: The call context\n    \"\"\"\n    if not call_output.text:\n        # Nothing to evaluate\n        return\n\n    # Extract the original query\n    original_query = self._extract_query(call_input)\n\n    # Build evaluation prompt\n    eval_prompt = self._build_evaluation_prompt(original_query, call_output.text)\n\n    try:\n        # Call evaluator\n        eval_response = await self.evaluator.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": eval_prompt}],\n            temperature=0.0,  # Deterministic evaluation\n        )\n\n        # Extract evaluation result\n        eval_text = eval_response.choices[0].message.content\n\n        # Try to parse as JSON\n        try:\n            eval_result = json.loads(eval_text)\n        except json.JSONDecodeError:\n            # If not JSON, store raw text\n            eval_result = {\"raw_evaluation\": eval_text}\n\n        # Store in context if requested\n        if self.store_in_metadata:\n            context.metadata[\"evaluation\"] = eval_result\n            context.metadata[\"evaluation_model\"] = self.model\n\n    except Exception as e:\n        # Evaluation failed - don't break the main flow\n        if self.store_in_metadata:\n            context.metadata[\"evaluation_error\"] = str(e)\n</code></pre>"},{"location":"api/#hookedllm.hooks.MetricsHook","title":"<code>MetricsHook(stats: dict[str, Any] | None = None)</code>","text":"<p>Track metrics across LLM calls.</p> <p>This is a finally hook that aggregates metrics including: - Total calls - Total tokens used - Error count - Average latency</p> Usage <p>metrics = MetricsHook() hookedllm.finally_(metrics)</p> <p>Initialize metrics hook.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>dict[str, Any] | None</code> <p>Optional existing stats dict to update.    If None, creates a new dict.</p> <code>None</code> Source code in <code>src/hookedllm/hooks/metrics.py</code> <pre><code>def __init__(self, stats: dict[str, Any] | None = None):\n    \"\"\"\n    Initialize metrics hook.\n\n    Args:\n        stats: Optional existing stats dict to update.\n               If None, creates a new dict.\n    \"\"\"\n    if stats is None:\n        self.stats = {\n            \"total_calls\": 0,\n            \"successful_calls\": 0,\n            \"failed_calls\": 0,\n            \"total_tokens\": 0,\n            \"prompt_tokens\": 0,\n            \"completion_tokens\": 0,\n            \"total_latency_ms\": 0.0,\n        }\n    else:\n        self.stats = stats\n</code></pre>"},{"location":"api/#hookedllm.hooks.MetricsHook--later-access-metrics","title":"Later, access metrics","text":"<p>print(metrics.stats)</p>"},{"location":"api/#hookedllm.hooks.MetricsHook-attributes","title":"Attributes","text":""},{"location":"api/#hookedllm.hooks.MetricsHook.average_latency_ms","title":"<code>average_latency_ms: float</code>  <code>property</code>","text":"<p>Calculate average latency.</p>"},{"location":"api/#hookedllm.hooks.MetricsHook.error_rate","title":"<code>error_rate: float</code>  <code>property</code>","text":"<p>Calculate error rate (0.0 to 1.0).</p>"},{"location":"api/#hookedllm.hooks.MetricsHook.success_rate","title":"<code>success_rate: float</code>  <code>property</code>","text":"<p>Calculate success rate (0.0 to 1.0).</p>"},{"location":"api/#hookedllm.hooks.MetricsHook-functions","title":"Functions","text":""},{"location":"api/#hookedllm.hooks.MetricsHook.__call__","title":"<code>__call__(result: CallResult) -&gt; None</code>  <code>async</code>","text":"<p>Update metrics based on call result.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>CallResult</code> <p>The complete call result</p> required Source code in <code>src/hookedllm/hooks/metrics.py</code> <pre><code>async def __call__(self, result: CallResult) -&gt; None:\n    \"\"\"\n    Update metrics based on call result.\n\n    Args:\n        result: The complete call result\n    \"\"\"\n    # Increment total calls\n    self.stats[\"total_calls\"] += 1\n\n    # Track success/failure\n    if result.error is None:\n        self.stats[\"successful_calls\"] += 1\n    else:\n        self.stats[\"failed_calls\"] += 1\n\n    # Track tokens\n    if result.output and result.output.usage:\n        usage = result.output.usage\n        self.stats[\"total_tokens\"] += usage.get(\"total_tokens\", 0)\n        self.stats[\"prompt_tokens\"] += usage.get(\"prompt_tokens\", 0)\n        self.stats[\"completion_tokens\"] += usage.get(\"completion_tokens\", 0)\n\n    # Track latency\n    self.stats[\"total_latency_ms\"] += result.elapsed_ms\n</code></pre>"},{"location":"api/#hookedllm.hooks.MetricsHook.reset","title":"<code>reset() -&gt; None</code>","text":"<p>Reset all metrics to zero.</p> Source code in <code>src/hookedllm/hooks/metrics.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset all metrics to zero.\"\"\"\n    for key in self.stats:\n        self.stats[key] = 0 if isinstance(self.stats[key], int) else 0.0\n</code></pre>"},{"location":"api/#hookedllm.hooks.MetricsHook.summary","title":"<code>summary() -&gt; dict[str, Any]</code>","text":"<p>Get a summary of metrics including calculated values.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with all metrics plus calculated averages</p> Source code in <code>src/hookedllm/hooks/metrics.py</code> <pre><code>def summary(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get a summary of metrics including calculated values.\n\n    Returns:\n        Dict with all metrics plus calculated averages\n    \"\"\"\n    return {\n        **self.stats,\n        \"average_latency_ms\": self.average_latency_ms,\n        \"success_rate\": self.success_rate,\n        \"error_rate\": self.error_rate,\n    }\n</code></pre>"},{"location":"api/#hookedllm.hooks-modules","title":"Modules","text":""},{"location":"api/#hookedllm.hooks.evaluation","title":"<code>evaluation</code>","text":"<p>Built-in evaluation hook helper.</p> <p>Provides a helper for evaluating LLM responses using another LLM.</p>"},{"location":"api/#hookedllm.hooks.evaluation-classes","title":"Classes","text":""},{"location":"api/#hookedllm.hooks.evaluation.EvaluationHook","title":"<code>EvaluationHook(evaluator_client: Any, criteria: dict[str, str], model: str = 'gpt-4o-mini', store_in_metadata: bool = True)</code>","text":"<p>Evaluate LLM responses using another LLM.</p> <p>This is an after hook that calls a separate \"evaluator\" LLM to assess the quality of responses based on configurable criteria.</p> Usage <p>from openai import AsyncOpenAI</p> <p>evaluator = AsyncOpenAI()  # Separate client for evaluation criteria = {     \"clarity\": \"Is the response clear and easy to understand?\",     \"accuracy\": \"Is the response factually accurate?\",     \"relevance\": \"Does the response address the user's question?\" }</p> <p>eval_hook = EvaluationHook(evaluator, criteria) hookedllm.scope(\"evaluation\").after(eval_hook)</p> <p>Initialize evaluation hook.</p> <p>Parameters:</p> Name Type Description Default <code>evaluator_client</code> <code>Any</code> <p>OpenAI-compatible client for evaluation calls</p> required <code>criteria</code> <code>dict[str, str]</code> <p>Dict mapping criterion name to description</p> required <code>model</code> <code>str</code> <p>Model to use for evaluation (default: gpt-4o-mini)</p> <code>'gpt-4o-mini'</code> <code>store_in_metadata</code> <code>bool</code> <p>If True, store results in context.metadata</p> <code>True</code> Source code in <code>src/hookedllm/hooks/evaluation.py</code> <pre><code>def __init__(\n    self,\n    evaluator_client: Any,\n    criteria: dict[str, str],\n    model: str = \"gpt-4o-mini\",\n    store_in_metadata: bool = True,\n):\n    \"\"\"\n    Initialize evaluation hook.\n\n    Args:\n        evaluator_client: OpenAI-compatible client for evaluation calls\n        criteria: Dict mapping criterion name to description\n        model: Model to use for evaluation (default: gpt-4o-mini)\n        store_in_metadata: If True, store results in context.metadata\n    \"\"\"\n    self.evaluator = evaluator_client\n    self.criteria = criteria\n    self.model = model\n    self.store_in_metadata = store_in_metadata\n</code></pre> Functions\u00b6 <code>__call__(call_input: CallInput, call_output: CallOutput, context: CallContext) -&gt; None</code> <code>async</code> \u00b6 <p>Evaluate the LLM response.</p> <p>Parameters:</p> Name Type Description Default <code>call_input</code> <code>CallInput</code> <p>The original call input</p> required <code>call_output</code> <code>CallOutput</code> <p>The LLM response</p> required <code>context</code> <code>CallContext</code> <p>The call context</p> required Source code in <code>src/hookedllm/hooks/evaluation.py</code> <pre><code>async def __call__(\n    self, call_input: CallInput, call_output: CallOutput, context: CallContext\n) -&gt; None:\n    \"\"\"\n    Evaluate the LLM response.\n\n    Args:\n        call_input: The original call input\n        call_output: The LLM response\n        context: The call context\n    \"\"\"\n    if not call_output.text:\n        # Nothing to evaluate\n        return\n\n    # Extract the original query\n    original_query = self._extract_query(call_input)\n\n    # Build evaluation prompt\n    eval_prompt = self._build_evaluation_prompt(original_query, call_output.text)\n\n    try:\n        # Call evaluator\n        eval_response = await self.evaluator.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": eval_prompt}],\n            temperature=0.0,  # Deterministic evaluation\n        )\n\n        # Extract evaluation result\n        eval_text = eval_response.choices[0].message.content\n\n        # Try to parse as JSON\n        try:\n            eval_result = json.loads(eval_text)\n        except json.JSONDecodeError:\n            # If not JSON, store raw text\n            eval_result = {\"raw_evaluation\": eval_text}\n\n        # Store in context if requested\n        if self.store_in_metadata:\n            context.metadata[\"evaluation\"] = eval_result\n            context.metadata[\"evaluation_model\"] = self.model\n\n    except Exception as e:\n        # Evaluation failed - don't break the main flow\n        if self.store_in_metadata:\n            context.metadata[\"evaluation_error\"] = str(e)\n</code></pre>"},{"location":"api/#hookedllm.hooks.metrics","title":"<code>metrics</code>","text":"<p>Built-in metrics tracking hook.</p> <p>Tracks token usage, call counts, and error rates across LLM calls.</p>"},{"location":"api/#hookedllm.hooks.metrics-classes","title":"Classes","text":""},{"location":"api/#hookedllm.hooks.metrics.MetricsHook","title":"<code>MetricsHook(stats: dict[str, Any] | None = None)</code>","text":"<p>Track metrics across LLM calls.</p> <p>This is a finally hook that aggregates metrics including: - Total calls - Total tokens used - Error count - Average latency</p> Usage <p>metrics = MetricsHook() hookedllm.finally_(metrics)</p> <p>Initialize metrics hook.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>dict[str, Any] | None</code> <p>Optional existing stats dict to update.    If None, creates a new dict.</p> <code>None</code> Source code in <code>src/hookedllm/hooks/metrics.py</code> <pre><code>def __init__(self, stats: dict[str, Any] | None = None):\n    \"\"\"\n    Initialize metrics hook.\n\n    Args:\n        stats: Optional existing stats dict to update.\n               If None, creates a new dict.\n    \"\"\"\n    if stats is None:\n        self.stats = {\n            \"total_calls\": 0,\n            \"successful_calls\": 0,\n            \"failed_calls\": 0,\n            \"total_tokens\": 0,\n            \"prompt_tokens\": 0,\n            \"completion_tokens\": 0,\n            \"total_latency_ms\": 0.0,\n        }\n    else:\n        self.stats = stats\n</code></pre> Attributes\u00b6 <code>average_latency_ms: float</code> <code>property</code> \u00b6 <p>Calculate average latency.</p> <code>error_rate: float</code> <code>property</code> \u00b6 <p>Calculate error rate (0.0 to 1.0).</p> <code>success_rate: float</code> <code>property</code> \u00b6 <p>Calculate success rate (0.0 to 1.0).</p> Functions\u00b6 <code>__call__(result: CallResult) -&gt; None</code> <code>async</code> \u00b6 <p>Update metrics based on call result.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>CallResult</code> <p>The complete call result</p> required Source code in <code>src/hookedllm/hooks/metrics.py</code> <pre><code>async def __call__(self, result: CallResult) -&gt; None:\n    \"\"\"\n    Update metrics based on call result.\n\n    Args:\n        result: The complete call result\n    \"\"\"\n    # Increment total calls\n    self.stats[\"total_calls\"] += 1\n\n    # Track success/failure\n    if result.error is None:\n        self.stats[\"successful_calls\"] += 1\n    else:\n        self.stats[\"failed_calls\"] += 1\n\n    # Track tokens\n    if result.output and result.output.usage:\n        usage = result.output.usage\n        self.stats[\"total_tokens\"] += usage.get(\"total_tokens\", 0)\n        self.stats[\"prompt_tokens\"] += usage.get(\"prompt_tokens\", 0)\n        self.stats[\"completion_tokens\"] += usage.get(\"completion_tokens\", 0)\n\n    # Track latency\n    self.stats[\"total_latency_ms\"] += result.elapsed_ms\n</code></pre> <code>reset() -&gt; None</code> \u00b6 <p>Reset all metrics to zero.</p> Source code in <code>src/hookedllm/hooks/metrics.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset all metrics to zero.\"\"\"\n    for key in self.stats:\n        self.stats[key] = 0 if isinstance(self.stats[key], int) else 0.0\n</code></pre> <code>summary() -&gt; dict[str, Any]</code> \u00b6 <p>Get a summary of metrics including calculated values.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with all metrics plus calculated averages</p> Source code in <code>src/hookedllm/hooks/metrics.py</code> <pre><code>def summary(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get a summary of metrics including calculated values.\n\n    Returns:\n        Dict with all metrics plus calculated averages\n    \"\"\"\n    return {\n        **self.stats,\n        \"average_latency_ms\": self.average_latency_ms,\n        \"success_rate\": self.success_rate,\n        \"error_rate\": self.error_rate,\n    }\n</code></pre>"},{"location":"api/#hookedllm.hooks.metrics.MetricsHook--later-access-metrics","title":"Later, access metrics","text":"<p>print(metrics.stats)</p>"},{"location":"api/#configuration","title":"Configuration","text":""},{"location":"api/#hookedllm.config","title":"<code>config</code>","text":"<p>Configuration loading for hookedllm.</p> <p>Provides YAML-based configuration loading (requires pyyaml).</p>"},{"location":"api/#hookedllm.config-classes","title":"Classes","text":""},{"location":"api/#hookedllm.config.HookConfig","title":"<code>HookConfig(name: str, type: Literal['before', 'after', 'error', 'finally'], module: str, function: str | None = None, class_name: str | None = None, when: WhenConfig | None = None, args: dict[str, Any] | None = None)</code>  <code>dataclass</code>","text":"<p>Single hook configuration from YAML.</p>"},{"location":"api/#hookedllm.config.RootConfig","title":"<code>RootConfig(global_hooks: list[HookConfig] | None = None, scopes: dict[str, ScopeConfig] | None = None)</code>  <code>dataclass</code>","text":"<p>Root configuration schema.</p>"},{"location":"api/#hookedllm.config.ScopeConfig","title":"<code>ScopeConfig(hooks: list[HookConfig])</code>  <code>dataclass</code>","text":"<p>Scope configuration with its hooks.</p>"},{"location":"api/#hookedllm.config.WhenConfig","title":"<code>WhenConfig(model: str | None = None, models: list[str] | None = None, tag: str | None = None, tags: list[str] | None = None, metadata: dict[str, Any] | None = None, all_calls: bool = False)</code>  <code>dataclass</code>","text":"<p>Rule configuration from YAML.</p>"},{"location":"api/#hookedllm.config-functions","title":"Functions","text":""},{"location":"api/#hookedllm.config.load_config","title":"<code>load_config(path: str, context: Any | None = None) -&gt; None</code>","text":"<p>Load hooks from a YAML configuration file.</p> <p>Requires pyyaml: pip install hookedllm[config]</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to YAML config file</p> required <code>context</code> <code>Any | None</code> <p>Optional HookedLLMContext to use (default: uses default context)</p> <code>None</code> Example YAML <p>global_hooks:   - name: metrics     type: finally     module: hookedllm.hooks.metrics     class_name: MetricsHook</p> <p>scopes:   evaluation:     hooks:       - name: evaluate         type: after         module: my_app.hooks         function: evaluate_response         when:           model: gpt-4</p> Example usage <p>import hookedllm hookedllm.load_config(\"hooks.yaml\")</p> Source code in <code>src/hookedllm/config/loader.py</code> <pre><code>def load_config(path: str, context: Any | None = None) -&gt; None:\n    \"\"\"\n    Load hooks from a YAML configuration file.\n\n    Requires pyyaml: pip install hookedllm[config]\n\n    Args:\n        path: Path to YAML config file\n        context: Optional HookedLLMContext to use (default: uses default context)\n\n    Example YAML:\n        global_hooks:\n          - name: metrics\n            type: finally\n            module: hookedllm.hooks.metrics\n            class_name: MetricsHook\n\n        scopes:\n          evaluation:\n            hooks:\n              - name: evaluate\n                type: after\n                module: my_app.hooks\n                function: evaluate_response\n                when:\n                  model: gpt-4\n\n    Example usage:\n        import hookedllm\n        hookedllm.load_config(\"hooks.yaml\")\n    \"\"\"\n    try:\n        import yaml\n    except ImportError as e:\n        raise ImportError(\n            \"PyYAML is required for config loading. \" \"Install with: pip install hookedllm[config]\"\n        ) from e\n\n    # Load YAML file\n    config_path = Path(path)\n    if not config_path.exists():\n        raise FileNotFoundError(f\"Config file not found: {path}\")\n\n    with open(config_path) as f:\n        data = yaml.safe_load(f)\n\n    if not data:\n        return  # Empty config\n\n    # Use provided context or default\n    if context is None:\n        import hookedllm\n\n        ctx = hookedllm._default_context\n    else:\n        ctx = context\n\n    # Load global hooks\n    if \"global_hooks\" in data and data[\"global_hooks\"]:\n        for hook_config in data[\"global_hooks\"]:\n            _register_hook_from_config(hook_config, ctx.global_scope(), ctx)\n\n    # Load scoped hooks\n    if \"scopes\" in data and data[\"scopes\"]:\n        for scope_name, scope_data in data[\"scopes\"].items():\n            scope = ctx.scope(scope_name)\n            if \"hooks\" in scope_data:\n                for hook_config in scope_data[\"hooks\"]:\n                    _register_hook_from_config(hook_config, scope, ctx)\n</code></pre>"},{"location":"api/#hookedllm.config-modules","title":"Modules","text":""},{"location":"api/#hookedllm.config.loader","title":"<code>loader</code>","text":"<p>YAML configuration loader for hookedllm.</p> <p>Loads hook configurations from YAML files and registers them.</p>"},{"location":"api/#hookedllm.config.loader-classes","title":"Classes","text":""},{"location":"api/#hookedllm.config.loader-functions","title":"Functions","text":""},{"location":"api/#hookedllm.config.loader.load_config","title":"<code>load_config(path: str, context: Any | None = None) -&gt; None</code>","text":"<p>Load hooks from a YAML configuration file.</p> <p>Requires pyyaml: pip install hookedllm[config]</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to YAML config file</p> required <code>context</code> <code>Any | None</code> <p>Optional HookedLLMContext to use (default: uses default context)</p> <code>None</code> Example YAML <p>global_hooks:   - name: metrics     type: finally     module: hookedllm.hooks.metrics     class_name: MetricsHook</p> <p>scopes:   evaluation:     hooks:       - name: evaluate         type: after         module: my_app.hooks         function: evaluate_response         when:           model: gpt-4</p> Example usage <p>import hookedllm hookedllm.load_config(\"hooks.yaml\")</p> Source code in <code>src/hookedllm/config/loader.py</code> <pre><code>def load_config(path: str, context: Any | None = None) -&gt; None:\n    \"\"\"\n    Load hooks from a YAML configuration file.\n\n    Requires pyyaml: pip install hookedllm[config]\n\n    Args:\n        path: Path to YAML config file\n        context: Optional HookedLLMContext to use (default: uses default context)\n\n    Example YAML:\n        global_hooks:\n          - name: metrics\n            type: finally\n            module: hookedllm.hooks.metrics\n            class_name: MetricsHook\n\n        scopes:\n          evaluation:\n            hooks:\n              - name: evaluate\n                type: after\n                module: my_app.hooks\n                function: evaluate_response\n                when:\n                  model: gpt-4\n\n    Example usage:\n        import hookedllm\n        hookedllm.load_config(\"hooks.yaml\")\n    \"\"\"\n    try:\n        import yaml\n    except ImportError as e:\n        raise ImportError(\n            \"PyYAML is required for config loading. \" \"Install with: pip install hookedllm[config]\"\n        ) from e\n\n    # Load YAML file\n    config_path = Path(path)\n    if not config_path.exists():\n        raise FileNotFoundError(f\"Config file not found: {path}\")\n\n    with open(config_path) as f:\n        data = yaml.safe_load(f)\n\n    if not data:\n        return  # Empty config\n\n    # Use provided context or default\n    if context is None:\n        import hookedllm\n\n        ctx = hookedllm._default_context\n    else:\n        ctx = context\n\n    # Load global hooks\n    if \"global_hooks\" in data and data[\"global_hooks\"]:\n        for hook_config in data[\"global_hooks\"]:\n            _register_hook_from_config(hook_config, ctx.global_scope(), ctx)\n\n    # Load scoped hooks\n    if \"scopes\" in data and data[\"scopes\"]:\n        for scope_name, scope_data in data[\"scopes\"].items():\n            scope = ctx.scope(scope_name)\n            if \"hooks\" in scope_data:\n                for hook_config in scope_data[\"hooks\"]:\n                    _register_hook_from_config(hook_config, scope, ctx)\n</code></pre>"},{"location":"api/#hookedllm.config.schema","title":"<code>schema</code>","text":"<p>Configuration schema for YAML-based hook registration.</p> <p>Defines the structure of YAML configuration files.</p>"},{"location":"api/#hookedllm.config.schema-classes","title":"Classes","text":""},{"location":"api/#hookedllm.config.schema.HookConfig","title":"<code>HookConfig(name: str, type: Literal['before', 'after', 'error', 'finally'], module: str, function: str | None = None, class_name: str | None = None, when: WhenConfig | None = None, args: dict[str, Any] | None = None)</code>  <code>dataclass</code>","text":"<p>Single hook configuration from YAML.</p>"},{"location":"api/#hookedllm.config.schema.RootConfig","title":"<code>RootConfig(global_hooks: list[HookConfig] | None = None, scopes: dict[str, ScopeConfig] | None = None)</code>  <code>dataclass</code>","text":"<p>Root configuration schema.</p>"},{"location":"api/#hookedllm.config.schema.ScopeConfig","title":"<code>ScopeConfig(hooks: list[HookConfig])</code>  <code>dataclass</code>","text":"<p>Scope configuration with its hooks.</p>"},{"location":"api/#hookedllm.config.schema.WhenConfig","title":"<code>WhenConfig(model: str | None = None, models: list[str] | None = None, tag: str | None = None, tags: list[str] | None = None, metadata: dict[str, Any] | None = None, all_calls: bool = False)</code>  <code>dataclass</code>","text":"<p>Rule configuration from YAML.</p>"},{"location":"api/#providers","title":"Providers","text":""},{"location":"api/#hookedllm.providers","title":"<code>providers</code>","text":"<p>Provider adapters for multi-provider support.</p> <p>Each adapter handles provider-specific logic for detecting clients, normalizing input/output, and extracting callable methods.</p>"},{"location":"api/#hookedllm.providers-classes","title":"Classes","text":""},{"location":"api/#hookedllm.providers.ProviderAdapter","title":"<code>ProviderAdapter</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for provider-specific adapters.</p> <p>Each adapter handles: - Detecting if a client belongs to this provider - Normalizing provider-specific input/output formats - Extracting the callable method from the client</p> <p>This follows the Open/Closed Principle: new providers can be added by implementing this protocol without modifying existing code.</p>"},{"location":"api/#hookedllm.providers.ProviderAdapter-functions","title":"Functions","text":""},{"location":"api/#hookedllm.providers.ProviderAdapter.detect","title":"<code>detect(client: Any) -&gt; bool</code>  <code>staticmethod</code>","text":"<p>Detect if a client belongs to this provider.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>The client instance to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if this adapter can handle the client, False otherwise</p> Source code in <code>src/hookedllm/providers/protocol.py</code> <pre><code>@staticmethod\ndef detect(client: Any) -&gt; bool:\n    \"\"\"\n    Detect if a client belongs to this provider.\n\n    Args:\n        client: The client instance to check\n\n    Returns:\n        True if this adapter can handle the client, False otherwise\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.providers.ProviderAdapter.get_callable","title":"<code>get_callable(client: Any) -&gt; Any</code>  <code>staticmethod</code>","text":"<p>Get the callable method from the client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>The client instance</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The callable method (e.g., client.chat.completions.create)</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the callable cannot be found</p> Source code in <code>src/hookedllm/providers/protocol.py</code> <pre><code>@staticmethod\ndef get_callable(client: Any) -&gt; Any:\n    \"\"\"\n    Get the callable method from the client.\n\n    Args:\n        client: The client instance\n\n    Returns:\n        The callable method (e.g., client.chat.completions.create)\n\n    Raises:\n        AttributeError: If the callable cannot be found\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.providers.ProviderAdapter.get_wrapper_path","title":"<code>get_wrapper_path(client: Any) -&gt; list[str]</code>  <code>staticmethod</code>","text":"<p>Get the attribute path to wrap (e.g., [\"chat\", \"completions\"]).</p> <p>This is used by the wrapper to intercept the correct attributes.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>The client instance</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of attribute names to wrap</p> Source code in <code>src/hookedllm/providers/protocol.py</code> <pre><code>@staticmethod\ndef get_wrapper_path(client: Any) -&gt; list[str]:\n    \"\"\"\n    Get the attribute path to wrap (e.g., [\"chat\", \"completions\"]).\n\n    This is used by the wrapper to intercept the correct attributes.\n\n    Args:\n        client: The client instance\n\n    Returns:\n        List of attribute names to wrap\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.providers.ProviderAdapter.normalize_input","title":"<code>normalize_input(provider_name: str, callable_method: Any, *args: Any, **kwargs: Any) -&gt; tuple[CallInput, CallContext]</code>  <code>staticmethod</code>","text":"<p>Normalize provider-specific input to CallInput and CallContext.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Name of the provider (e.g., \"openai\", \"anthropic\")</p> required <code>callable_method</code> <code>Any</code> <p>The callable method being called</p> required <code>*args</code> <code>Any</code> <p>Positional arguments passed to the callable</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments passed to the callable</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[CallInput, CallContext]</code> <p>Tuple of (CallInput, CallContext)</p> Source code in <code>src/hookedllm/providers/protocol.py</code> <pre><code>@staticmethod\ndef normalize_input(\n    provider_name: str, callable_method: Any, *args: Any, **kwargs: Any\n) -&gt; tuple[CallInput, CallContext]:\n    \"\"\"\n    Normalize provider-specific input to CallInput and CallContext.\n\n    Args:\n        provider_name: Name of the provider (e.g., \"openai\", \"anthropic\")\n        callable_method: The callable method being called\n        *args: Positional arguments passed to the callable\n        **kwargs: Keyword arguments passed to the callable\n\n    Returns:\n        Tuple of (CallInput, CallContext)\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.providers.ProviderAdapter.normalize_output","title":"<code>normalize_output(response: Any) -&gt; CallOutput</code>  <code>staticmethod</code>","text":"<p>Normalize provider-specific response to CallOutput.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Any</code> <p>The raw response from the provider SDK</p> required <p>Returns:</p> Type Description <code>CallOutput</code> <p>Normalized CallOutput</p> Source code in <code>src/hookedllm/providers/protocol.py</code> <pre><code>@staticmethod\ndef normalize_output(response: Any) -&gt; CallOutput:\n    \"\"\"\n    Normalize provider-specific response to CallOutput.\n\n    Args:\n        response: The raw response from the provider SDK\n\n    Returns:\n        Normalized CallOutput\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/#hookedllm.providers-modules","title":"Modules","text":""},{"location":"api/#hookedllm.providers.anthropic","title":"<code>anthropic</code>","text":"<p>Anthropic provider adapter.</p> <p>Handles Anthropic SDK-specific logic for detecting clients, normalizing input/output, and extracting callable methods.</p>"},{"location":"api/#hookedllm.providers.anthropic-classes","title":"Classes","text":""},{"location":"api/#hookedllm.providers.anthropic.AnthropicAdapter","title":"<code>AnthropicAdapter</code>","text":"<p>Adapter for Anthropic SDK clients.</p> <p>Supports both AsyncAnthropic and Anthropic clients. Handles the Anthropic-specific API structure: client.messages.create()</p> Functions\u00b6 <code>detect(client: Any) -&gt; bool</code> <code>staticmethod</code> \u00b6 <p>Detect if a client is an Anthropic client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>The client instance to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if client has Anthropic SDK structure, False otherwise</p> Source code in <code>src/hookedllm/providers/anthropic.py</code> <pre><code>@staticmethod\ndef detect(client: Any) -&gt; bool:\n    \"\"\"\n    Detect if a client is an Anthropic client.\n\n    Args:\n        client: The client instance to check\n\n    Returns:\n        True if client has Anthropic SDK structure, False otherwise\n    \"\"\"\n    # Check for Anthropic structure\n    # Must have messages.create and it should be callable\n    try:\n        if hasattr(client, \"messages\") and hasattr(client.messages, \"create\"):\n            # Verify it's actually callable (not just a MagicMock attribute)\n            create_method = getattr(client.messages, \"create\", None)\n            if callable(create_method):\n                # Additional check: if client also has OpenAI structure (chat.completions.create),\n                # we need to prefer Anthropic only if messages.create was explicitly set\n                # For MagicMock, check if chat.completions.create exists - if it does and is callable,\n                # we need to verify which one was set explicitly\n                # Since Anthropic is checked first, if both exist, prefer Anthropic\n                # But we should verify messages.create is the primary structure\n                return True\n    except (AttributeError, TypeError):\n        pass\n\n    return False\n</code></pre> <code>get_callable(client: Any) -&gt; Any</code> <code>staticmethod</code> \u00b6 <p>Get the Anthropic messages.create callable.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>The Anthropic client instance</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The messages.create method</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the callable cannot be found</p> Source code in <code>src/hookedllm/providers/anthropic.py</code> <pre><code>@staticmethod\ndef get_callable(client: Any) -&gt; Any:\n    \"\"\"\n    Get the Anthropic messages.create callable.\n\n    Args:\n        client: The Anthropic client instance\n\n    Returns:\n        The messages.create method\n\n    Raises:\n        AttributeError: If the callable cannot be found\n    \"\"\"\n    return client.messages.create\n</code></pre> <code>get_wrapper_path(client: Any) -&gt; list[str]</code> <code>staticmethod</code> \u00b6 <p>Get the attribute path to wrap for Anthropic.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>[\"messages\"] - the path to intercept</p> Source code in <code>src/hookedllm/providers/anthropic.py</code> <pre><code>@staticmethod\ndef get_wrapper_path(client: Any) -&gt; list[str]:\n    \"\"\"\n    Get the attribute path to wrap for Anthropic.\n\n    Returns:\n        [\"messages\"] - the path to intercept\n    \"\"\"\n    return [\"messages\"]\n</code></pre> <code>normalize_input(provider_name: str, callable_method: Any, *args: Any, **kwargs: Any) -&gt; tuple[CallInput, CallContext]</code> <code>staticmethod</code> \u00b6 <p>Normalize Anthropic input to CallInput and CallContext.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Provider name (should be \"anthropic\")</p> required <code>callable_method</code> <code>Any</code> <p>The callable method (unused for Anthropic)</p> required <code>*args</code> <code>Any</code> <p>Positional arguments (unused for Anthropic)</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments including model, messages, etc.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[CallInput, CallContext]</code> <p>Tuple of (CallInput, CallContext)</p> Source code in <code>src/hookedllm/providers/anthropic.py</code> <pre><code>@staticmethod\ndef normalize_input(\n    provider_name: str, callable_method: Any, *args: Any, **kwargs: Any\n) -&gt; tuple[CallInput, CallContext]:\n    \"\"\"\n    Normalize Anthropic input to CallInput and CallContext.\n\n    Args:\n        provider_name: Provider name (should be \"anthropic\")\n        callable_method: The callable method (unused for Anthropic)\n        *args: Positional arguments (unused for Anthropic)\n        **kwargs: Keyword arguments including model, messages, etc.\n\n    Returns:\n        Tuple of (CallInput, CallContext)\n    \"\"\"\n    model = kwargs.get(\"model\", \"\")\n    messages = kwargs.get(\"messages\", [])\n\n    # Anthropic uses metadata parameter instead of extra_body\n    metadata_dict = kwargs.get(\"metadata\", {})\n    if isinstance(metadata_dict, dict):\n        tags = metadata_dict.pop(\"hookedllm_tags\", [])\n        custom_metadata = metadata_dict.pop(\"hookedllm_metadata\", {})\n    else:\n        tags = []\n        custom_metadata = {}\n\n    # Normalize messages to internal format\n    # Anthropic messages have role and content fields\n    normalized_messages = []\n    for m in messages:\n        role = m.get(\"role\", \"\")\n        # Anthropic content can be a string or list of content blocks\n        content = m.get(\"content\", \"\")\n        if isinstance(content, list) and len(content) &gt; 0:\n            # Extract text from first text block if it's a list\n            first_block = content[0]\n            if isinstance(first_block, dict) and first_block.get(\"type\") == \"text\":\n                content = first_block.get(\"text\", \"\")\n            elif isinstance(first_block, str):\n                content = first_block\n        normalized_messages.append(Message(role=role, content=content))\n\n    # Create normalized input\n    call_input = CallInput(\n        model=model, messages=normalized_messages, params=kwargs, metadata=custom_metadata\n    )\n\n    # Create context\n    context = CallContext(\n        provider=provider_name, model=model, tags=tags, metadata=custom_metadata\n    )\n\n    return call_input, context\n</code></pre> <code>normalize_output(response: Any) -&gt; CallOutput</code> <code>staticmethod</code> \u00b6 <p>Normalize Anthropic response to CallOutput.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Any</code> <p>Anthropic SDK response object</p> required <p>Returns:</p> Type Description <code>CallOutput</code> <p>Normalized CallOutput</p> Source code in <code>src/hookedllm/providers/anthropic.py</code> <pre><code>@staticmethod\ndef normalize_output(response: Any) -&gt; CallOutput:\n    \"\"\"\n    Normalize Anthropic response to CallOutput.\n\n    Args:\n        response: Anthropic SDK response object\n\n    Returns:\n        Normalized CallOutput\n    \"\"\"\n    try:\n        # Extract text from response\n        # Anthropic response has content as a list of content blocks\n        text = None\n        if hasattr(response, \"content\") and isinstance(response.content, list):\n            if len(response.content) &gt; 0:\n                first_content = response.content[0]\n                if isinstance(first_content, dict):\n                    text = first_content.get(\"text\")\n                elif hasattr(first_content, \"text\"):\n                    text = first_content.text\n\n        # Extract usage\n        # Anthropic uses input_tokens and output_tokens\n        usage = None\n        if hasattr(response, \"usage\"):\n            usage_obj = response.usage\n            if hasattr(usage_obj, \"input_tokens\") and hasattr(usage_obj, \"output_tokens\"):\n                # Convert to dict format\n                if hasattr(usage_obj, \"model_dump\"):\n                    usage = usage_obj.model_dump()\n                elif hasattr(usage_obj, \"dict\"):\n                    usage = usage_obj.dict()\n                elif hasattr(usage_obj, \"__dict__\"):\n                    usage = dict(usage_obj.__dict__)\n                else:\n                    # Fallback: create dict manually\n                    usage = {\n                        \"input_tokens\": usage_obj.input_tokens,\n                        \"output_tokens\": usage_obj.output_tokens,\n                        \"total_tokens\": usage_obj.input_tokens + usage_obj.output_tokens,\n                    }\n                # Ensure total_tokens is present\n                if usage and \"total_tokens\" not in usage:\n                    usage[\"total_tokens\"] = usage.get(\"input_tokens\", 0) + usage.get(\n                        \"output_tokens\", 0\n                    )\n\n        # Extract stop_reason (Anthropic's finish_reason)\n        finish_reason = None\n        if hasattr(response, \"stop_reason\"):\n            finish_reason = response.stop_reason\n\n        return CallOutput(text=text, raw=response, usage=usage, finish_reason=finish_reason)\n    except Exception:\n        # If normalization fails, return minimal output with raw response\n        return CallOutput(text=None, raw=response, usage=None, finish_reason=None)\n</code></pre>"},{"location":"api/#hookedllm.providers.openai","title":"<code>openai</code>","text":"<p>OpenAI provider adapter.</p> <p>Handles OpenAI SDK-specific logic for detecting clients, normalizing input/output, and extracting callable methods.</p>"},{"location":"api/#hookedllm.providers.openai-classes","title":"Classes","text":""},{"location":"api/#hookedllm.providers.openai.OpenAIAdapter","title":"<code>OpenAIAdapter</code>","text":"<p>Adapter for OpenAI SDK clients.</p> <p>Supports both AsyncOpenAI and OpenAI clients. Handles the OpenAI-specific API structure: client.chat.completions.create()</p> Functions\u00b6 <code>detect(client: Any) -&gt; bool</code> <code>staticmethod</code> \u00b6 <p>Detect if a client is an OpenAI client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>The client instance to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if client has OpenAI SDK structure, False otherwise</p> Source code in <code>src/hookedllm/providers/openai.py</code> <pre><code>@staticmethod\ndef detect(client: Any) -&gt; bool:\n    \"\"\"\n    Detect if a client is an OpenAI client.\n\n    Args:\n        client: The client instance to check\n\n    Returns:\n        True if client has OpenAI SDK structure, False otherwise\n    \"\"\"\n    # Check for OpenAI structure\n    # Must have chat.completions.create and it should be callable\n    try:\n        # Use getattr with sentinel to avoid MagicMock auto-creation\n        _SENTINEL = object()\n\n        # Check for OpenAI structure\n        chat = getattr(client, \"chat\", _SENTINEL)\n        if chat is _SENTINEL:\n            return False\n\n        completions = getattr(chat, \"completions\", _SENTINEL)\n        if completions is _SENTINEL:\n            return False\n\n        create_method = getattr(completions, \"create\", _SENTINEL)\n        if create_method is _SENTINEL or not callable(create_method):\n            return False\n\n        # Check if it also has Anthropic structure\n        # If both exist, Anthropic should have matched first\n        # But if Anthropic didn't match, it means messages.create wasn't properly set\n        # So we can match OpenAI\n        # However, to avoid false positives with MagicMock, we check:\n        # If messages.create exists and is callable, don't match OpenAI\n        # (Anthropic should have matched if it was properly set)\n        messages = getattr(client, \"messages\", _SENTINEL)\n        if messages is not _SENTINEL:\n            messages_create = getattr(messages, \"create\", _SENTINEL)\n            if messages_create is not _SENTINEL and callable(messages_create):\n                # Both structures exist - since Anthropic is checked first,\n                # if we get here, Anthropic didn't match, which means messages.create\n                # was auto-created by MagicMock. So we should still match OpenAI.\n                # But to be safe, if both are properly callable, prefer Anthropic\n                # Actually, let's be conservative: if both exist, don't match OpenAI\n                return False\n\n        return True\n    except (AttributeError, TypeError):\n        pass\n\n    return False\n</code></pre> <code>get_callable(client: Any) -&gt; Any</code> <code>staticmethod</code> \u00b6 <p>Get the OpenAI completions.create callable.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>The OpenAI client instance</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The chat.completions.create method</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the callable cannot be found</p> Source code in <code>src/hookedllm/providers/openai.py</code> <pre><code>@staticmethod\ndef get_callable(client: Any) -&gt; Any:\n    \"\"\"\n    Get the OpenAI completions.create callable.\n\n    Args:\n        client: The OpenAI client instance\n\n    Returns:\n        The chat.completions.create method\n\n    Raises:\n        AttributeError: If the callable cannot be found\n    \"\"\"\n    return client.chat.completions.create\n</code></pre> <code>get_wrapper_path(client: Any) -&gt; list[str]</code> <code>staticmethod</code> \u00b6 <p>Get the attribute path to wrap for OpenAI.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>[\"chat\", \"completions\"] - the path to intercept</p> Source code in <code>src/hookedllm/providers/openai.py</code> <pre><code>@staticmethod\ndef get_wrapper_path(client: Any) -&gt; list[str]:\n    \"\"\"\n    Get the attribute path to wrap for OpenAI.\n\n    Returns:\n        [\"chat\", \"completions\"] - the path to intercept\n    \"\"\"\n    return [\"chat\", \"completions\"]\n</code></pre> <code>normalize_input(provider_name: str, callable_method: Any, *args: Any, **kwargs: Any) -&gt; tuple[CallInput, CallContext]</code> <code>staticmethod</code> \u00b6 <p>Normalize OpenAI input to CallInput and CallContext.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Provider name (should be \"openai\")</p> required <code>callable_method</code> <code>Any</code> <p>The callable method (unused for OpenAI)</p> required <code>*args</code> <code>Any</code> <p>Positional arguments (unused for OpenAI)</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments including model, messages, etc.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[CallInput, CallContext]</code> <p>Tuple of (CallInput, CallContext)</p> Source code in <code>src/hookedllm/providers/openai.py</code> <pre><code>@staticmethod\ndef normalize_input(\n    provider_name: str, callable_method: Any, *args: Any, **kwargs: Any\n) -&gt; tuple[CallInput, CallContext]:\n    \"\"\"\n    Normalize OpenAI input to CallInput and CallContext.\n\n    Args:\n        provider_name: Provider name (should be \"openai\")\n        callable_method: The callable method (unused for OpenAI)\n        *args: Positional arguments (unused for OpenAI)\n        **kwargs: Keyword arguments including model, messages, etc.\n\n    Returns:\n        Tuple of (CallInput, CallContext)\n    \"\"\"\n    model = kwargs.get(\"model\", \"\")\n    messages = kwargs.get(\"messages\", [])\n\n    # Extract hookedllm-specific params from extra_body\n    extra_body = kwargs.get(\"extra_body\", {})\n    if isinstance(extra_body, dict):\n        tags = extra_body.pop(\"hookedllm_tags\", [])\n        metadata = extra_body.pop(\"hookedllm_metadata\", {})\n    else:\n        tags = []\n        metadata = {}\n\n    # Normalize messages to internal format\n    normalized_messages = [\n        Message(role=m.get(\"role\", \"\"), content=m.get(\"content\", \"\")) for m in messages\n    ]\n\n    # Create normalized input\n    call_input = CallInput(\n        model=model, messages=normalized_messages, params=kwargs, metadata=metadata\n    )\n\n    # Create context\n    context = CallContext(provider=provider_name, model=model, tags=tags, metadata=metadata)\n\n    return call_input, context\n</code></pre> <code>normalize_output(response: Any) -&gt; CallOutput</code> <code>staticmethod</code> \u00b6 <p>Normalize OpenAI response to CallOutput.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Any</code> <p>OpenAI SDK response object</p> required <p>Returns:</p> Type Description <code>CallOutput</code> <p>Normalized CallOutput</p> Source code in <code>src/hookedllm/providers/openai.py</code> <pre><code>@staticmethod\ndef normalize_output(response: Any) -&gt; CallOutput:\n    \"\"\"\n    Normalize OpenAI response to CallOutput.\n\n    Args:\n        response: OpenAI SDK response object\n\n    Returns:\n        Normalized CallOutput\n    \"\"\"\n    try:\n        # Extract text from response\n        text = None\n        if hasattr(response, \"choices\") and len(response.choices) &gt; 0:\n            choice = response.choices[0]\n            if hasattr(choice, \"message\"):\n                text = getattr(choice.message, \"content\", None)\n\n        # Extract usage\n        usage = None\n        if hasattr(response, \"usage\"):\n            # Try to convert to dict\n            usage_obj = response.usage\n            if hasattr(usage_obj, \"model_dump\"):\n                usage = usage_obj.model_dump()\n            elif hasattr(usage_obj, \"dict\"):\n                usage = usage_obj.dict()\n            elif hasattr(usage_obj, \"__dict__\"):\n                usage = dict(usage_obj.__dict__)\n\n        # Extract finish_reason\n        finish_reason = None\n        if hasattr(response, \"choices\") and len(response.choices) &gt; 0:\n            finish_reason = getattr(response.choices[0], \"finish_reason\", None)\n\n        return CallOutput(text=text, raw=response, usage=usage, finish_reason=finish_reason)\n    except Exception:\n        # If normalization fails, return minimal output with raw response\n        return CallOutput(text=None, raw=response, usage=None, finish_reason=None)\n</code></pre>"},{"location":"api/#hookedllm.providers.protocol","title":"<code>protocol</code>","text":"<p>Provider adapter protocol for multi-provider support.</p> <p>Defines the interface that all provider adapters must implement, following the Dependency Inversion Principle.</p>"},{"location":"api/#hookedllm.providers.protocol-classes","title":"Classes","text":""},{"location":"api/#hookedllm.providers.protocol.ProviderAdapter","title":"<code>ProviderAdapter</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for provider-specific adapters.</p> <p>Each adapter handles: - Detecting if a client belongs to this provider - Normalizing provider-specific input/output formats - Extracting the callable method from the client</p> <p>This follows the Open/Closed Principle: new providers can be added by implementing this protocol without modifying existing code.</p> Functions\u00b6 <code>detect(client: Any) -&gt; bool</code> <code>staticmethod</code> \u00b6 <p>Detect if a client belongs to this provider.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>The client instance to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if this adapter can handle the client, False otherwise</p> Source code in <code>src/hookedllm/providers/protocol.py</code> <pre><code>@staticmethod\ndef detect(client: Any) -&gt; bool:\n    \"\"\"\n    Detect if a client belongs to this provider.\n\n    Args:\n        client: The client instance to check\n\n    Returns:\n        True if this adapter can handle the client, False otherwise\n    \"\"\"\n    ...\n</code></pre> <code>get_callable(client: Any) -&gt; Any</code> <code>staticmethod</code> \u00b6 <p>Get the callable method from the client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>The client instance</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The callable method (e.g., client.chat.completions.create)</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the callable cannot be found</p> Source code in <code>src/hookedllm/providers/protocol.py</code> <pre><code>@staticmethod\ndef get_callable(client: Any) -&gt; Any:\n    \"\"\"\n    Get the callable method from the client.\n\n    Args:\n        client: The client instance\n\n    Returns:\n        The callable method (e.g., client.chat.completions.create)\n\n    Raises:\n        AttributeError: If the callable cannot be found\n    \"\"\"\n    ...\n</code></pre> <code>get_wrapper_path(client: Any) -&gt; list[str]</code> <code>staticmethod</code> \u00b6 <p>Get the attribute path to wrap (e.g., [\"chat\", \"completions\"]).</p> <p>This is used by the wrapper to intercept the correct attributes.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Any</code> <p>The client instance</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of attribute names to wrap</p> Source code in <code>src/hookedllm/providers/protocol.py</code> <pre><code>@staticmethod\ndef get_wrapper_path(client: Any) -&gt; list[str]:\n    \"\"\"\n    Get the attribute path to wrap (e.g., [\"chat\", \"completions\"]).\n\n    This is used by the wrapper to intercept the correct attributes.\n\n    Args:\n        client: The client instance\n\n    Returns:\n        List of attribute names to wrap\n    \"\"\"\n    ...\n</code></pre> <code>normalize_input(provider_name: str, callable_method: Any, *args: Any, **kwargs: Any) -&gt; tuple[CallInput, CallContext]</code> <code>staticmethod</code> \u00b6 <p>Normalize provider-specific input to CallInput and CallContext.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Name of the provider (e.g., \"openai\", \"anthropic\")</p> required <code>callable_method</code> <code>Any</code> <p>The callable method being called</p> required <code>*args</code> <code>Any</code> <p>Positional arguments passed to the callable</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments passed to the callable</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[CallInput, CallContext]</code> <p>Tuple of (CallInput, CallContext)</p> Source code in <code>src/hookedllm/providers/protocol.py</code> <pre><code>@staticmethod\ndef normalize_input(\n    provider_name: str, callable_method: Any, *args: Any, **kwargs: Any\n) -&gt; tuple[CallInput, CallContext]:\n    \"\"\"\n    Normalize provider-specific input to CallInput and CallContext.\n\n    Args:\n        provider_name: Name of the provider (e.g., \"openai\", \"anthropic\")\n        callable_method: The callable method being called\n        *args: Positional arguments passed to the callable\n        **kwargs: Keyword arguments passed to the callable\n\n    Returns:\n        Tuple of (CallInput, CallContext)\n    \"\"\"\n    ...\n</code></pre> <code>normalize_output(response: Any) -&gt; CallOutput</code> <code>staticmethod</code> \u00b6 <p>Normalize provider-specific response to CallOutput.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Any</code> <p>The raw response from the provider SDK</p> required <p>Returns:</p> Type Description <code>CallOutput</code> <p>Normalized CallOutput</p> Source code in <code>src/hookedllm/providers/protocol.py</code> <pre><code>@staticmethod\ndef normalize_output(response: Any) -&gt; CallOutput:\n    \"\"\"\n    Normalize provider-specific response to CallOutput.\n\n    Args:\n        response: The raw response from the provider SDK\n\n    Returns:\n        Normalized CallOutput\n    \"\"\"\n    ...\n</code></pre>"},{"location":"examples/advanced/","title":"Advanced Examples","text":"<p>This example demonstrates advanced HookedLLM patterns including evaluation, metrics, and custom error handling.</p>"},{"location":"examples/advanced/#evaluation-hook","title":"Evaluation Hook","text":"<pre><code>import hookedllm\nfrom openai import AsyncOpenAI\n\nasync def evaluate_response(call_input, call_output, context):\n    \"\"\"Evaluate LLM responses for quality.\"\"\"\n    # Build evaluation prompt\n    eval_prompt = f\"\"\"\n    Evaluate this response for clarity and accuracy:\n\n    Query: {call_input.messages[-1].content}\n    Response: {call_output.text}\n\n    Return JSON: {{\"clarity\": 0-1, \"accuracy\": 0-1}}\n    \"\"\"\n\n    # Use separate evaluator client (no hooks to avoid recursion)\n    evaluator = AsyncOpenAI()\n    eval_result = await evaluator.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": eval_prompt}]\n    )\n\n    # Store evaluation in metadata\n    context.metadata[\"evaluation\"] = eval_result.choices[0].message.content\n\n# Register to evaluation scope\nhookedllm.scope(\"evaluation\").after(evaluate_response)\n\n# Use with evaluation scope\neval_client = hookedllm.wrap(AsyncOpenAI(), scope=\"evaluation\")\n</code></pre>"},{"location":"examples/advanced/#metrics-collection","title":"Metrics Collection","text":"<pre><code>import hookedllm\nfrom collections import defaultdict\n\nmetrics = defaultdict(int)\n\nasync def track_metrics(result):\n    \"\"\"Track aggregated metrics.\"\"\"\n    metrics[\"calls\"] += 1\n\n    if result.error:\n        metrics[\"errors\"] += 1\n\n    if result.output and result.output.usage:\n        metrics[\"tokens\"] += result.output.usage.get(\"total_tokens\", 0)\n        metrics[\"prompt_tokens\"] += result.output.usage.get(\"prompt_tokens\", 0)\n        metrics[\"completion_tokens\"] += result.output.usage.get(\"completion_tokens\", 0)\n\n    metrics[\"total_time_ms\"] += result.elapsed_ms\n\n# Register globally\nhookedllm.finally_(track_metrics)\n\n# Metrics accumulate across all calls\n</code></pre>"},{"location":"examples/advanced/#tags-and-metadata","title":"Tags and Metadata","text":"<pre><code>import hookedllm\nfrom openai import AsyncOpenAI\n\n# Hook that uses tags\nasync def logger_hook(call_input, call_output, context):\n    if \"monitoring\" in context.tags:\n        logger.info(f\"Monitored call: {context.call_id}\")\n\nhookedllm.after(logger_hook)\n\n# Use tags in calls\nclient = hookedllm.wrap(AsyncOpenAI())\n\nresponse = await client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    extra_body={\n        \"hookedllm_tags\": [\"monitoring\", \"critical\"],\n        \"hookedllm_metadata\": {\n            \"user_id\": \"abc123\",\n            \"user_tier\": \"premium\"\n        }\n    }\n)\n</code></pre>"},{"location":"examples/advanced/#custom-error-handling","title":"Custom Error Handling","text":"<pre><code>import hookedllm\nfrom openai import AsyncOpenAI, RateLimitError\n\ndef my_error_handler(error, context):\n    \"\"\"Custom handling for hook errors.\"\"\"\n    logger.error(f\"Hook failed in {context}: {error}\")\n\n# Create custom executor with error handler\nfrom hookedllm.core import DefaultHookExecutor\n\nexecutor = DefaultHookExecutor(\n    error_handler=my_error_handler,\n    logger=my_logger\n)\n\n# Create context with custom executor\nctx = hookedllm.create_context(executor=executor)\nclient = ctx.wrap(AsyncOpenAI())\n</code></pre>"},{"location":"examples/advanced/#conditional-evaluation","title":"Conditional Evaluation","text":"<pre><code>import hookedllm\n\n# Only evaluate expensive models\nhookedllm.scope(\"evaluation\").after(\n    expensive_evaluation,\n    when=hookedllm.when.model(\"gpt-4\")\n)\n\n# Quick evaluation for cheaper models\nhookedllm.scope(\"evaluation\").after(\n    quick_evaluation,\n    when=hookedllm.when.model(\"gpt-4o-mini\")\n)\n\n# Only evaluate for tagged calls\nhookedllm.scope(\"evaluation\").after(\n    evaluation_hook,\n    when=hookedllm.when.tag(\"monitoring\")\n)\n</code></pre>"},{"location":"examples/advanced/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nimport hookedllm\nfrom openai import AsyncOpenAI\nfrom collections import defaultdict\n\n# Metrics tracking\nmetrics = defaultdict(int)\n\nasync def track_metrics(result):\n    metrics[\"calls\"] += 1\n    if result.output and result.output.usage:\n        metrics[\"tokens\"] += result.output.usage.get(\"total_tokens\", 0)\n\nasync def evaluate_response(call_input, call_output, context):\n    # Evaluation logic here\n    context.metadata[\"eval_score\"] = 0.9\n\n# Register hooks\nhookedllm.finally_(track_metrics)\nhookedllm.scope(\"evaluation\").after(evaluate_response)\n\n# Create client\nclient = hookedllm.wrap(AsyncOpenAI(), scope=\"evaluation\")\n\n# Use client\nasync def main():\n    response = await client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n    )\n    print(f\"Response: {response.choices[0].message.content}\")\n    print(f\"Metrics: {dict(metrics)}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/advanced/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Dependency Injection for testing</li> <li>See Testing Guide for test patterns</li> <li>Check out Architecture for design details</li> </ul>"},{"location":"examples/basic-usage/","title":"Basic Usage Example","text":"<p>Complete examples demonstrating HookedLLM fundamentals. For a quick introduction, see the Quick Start Guide.</p> <p>This example shows a complete application setup combining multiple concepts: hook registration, scopes, rules, and all hook types.</p>"},{"location":"examples/basic-usage/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nimport hookedllm\nfrom openai import AsyncOpenAI\n\n# Define hooks\nasync def track_metrics(result):\n    \"\"\"Track metrics in finally hook.\"\"\"\n    if result.output:\n        print(f\"[METRICS] Call {result.context.call_id} took {result.elapsed_ms:.2f}ms\")\n\nasync def evaluate_response(call_input, call_output, context):\n    \"\"\"Evaluate response quality.\"\"\"\n    print(f\"[EVAL] Evaluating response for: {call_input.model}\")\n    context.metadata[\"eval_score\"] = 0.9\n\nasync def log_call(call_input, call_output, context):\n    \"\"\"Log every LLM call.\"\"\"\n    print(f\"[LOG] Model: {call_input.model}\")\n\n# Register hooks\nhookedllm.finally_(track_metrics)  # Global hook\nhookedllm.scope(\"evaluation\").after(evaluate_response)\nhookedllm.scope(\"evaluation\").after(log_call)\n\n# Create client with evaluation scope\nclient = hookedllm.wrap(AsyncOpenAI(), scope=\"evaluation\")\n\n# Use the client\nasync def main():\n    response = await client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n    )\n    print(response.choices[0].message.content)\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Scopes for isolating hooks</li> <li>Explore Rules for conditional execution</li> <li>See Advanced Examples for more patterns</li> </ul>"},{"location":"examples/scopes/","title":"Scopes Example","text":"<p>Real-world examples demonstrating scope usage patterns. For basic scope concepts, see the Scopes guide.</p>"},{"location":"examples/scopes/#real-world-example","title":"Real-World Example","text":"<pre><code>import hookedllm\nfrom openai import AsyncOpenAI\n\n# Development scope - verbose logging\nhookedllm.scope(\"development\").after(\n    lambda i, o, c: print(f\"[DEV] {c.call_id}: {o.text[:50]}\")\n)\n\n# Testing scope - mock responses\nhookedllm.scope(\"testing\").before(\n    lambda i, c: setattr(c, \"skip_call\", True)\n)\n\n# Default scope - minimal logging\nhookedllm.scope(\"default\").finally_(\n    lambda r: logger.info(f\"Call {r.context.call_id} completed\")\n)\n\n# Create clients for different environments\ndev_client = hookedllm.wrap(AsyncOpenAI(), scope=\"development\")\ntest_client = hookedllm.wrap(AsyncOpenAI(), scope=\"testing\")\ndefault_client = hookedllm.wrap(AsyncOpenAI(), scope=\"default\")\n</code></pre>"},{"location":"examples/scopes/#next-steps","title":"Next Steps","text":"<ul> <li>Learn more about Scopes</li> <li>See Advanced Examples for more patterns</li> </ul>"},{"location":"examples/travel-assistant/","title":"Travel Assistant Example","text":"<p>Minimal example: Adding logging, metrics, and evaluation to a travel assistant app.</p>"},{"location":"examples/travel-assistant/#application","title":"Application","text":"<pre><code># app.py\nfrom openai import AsyncOpenAI\nimport hookedllm\nimport os\n\n# Setup client\nclient = hookedllm.wrap(\n    AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\")),\n    scope=\"default\"\n)\n\nasync def travel_assistant(query: str) -&gt; str:\n    \"\"\"Simple travel assistant that calls LLM.\"\"\"\n    response = await client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful travel assistant.\"},\n            {\"role\": \"user\", \"content\": query}\n        ]\n    )\n    return response.choices[0].message.content\n</code></pre>"},{"location":"examples/travel-assistant/#setup-logging-metrics-evaluation","title":"Setup: Logging, Metrics, Evaluation","text":"<pre><code># hooks.py\nimport logging\nfrom prometheus_client import Counter, Histogram\nimport hookedllm\n\n# Prometheus metrics\nllm_calls_total = Counter(\"llm_calls_total\", \"Total LLM calls\", [\"model\", \"scope\"])\nllm_call_duration = Histogram(\"llm_call_duration_seconds\", \"LLM call duration\", [\"model\"])\nllm_tokens_total = Counter(\"llm_tokens_total\", \"Total tokens\", [\"model\", \"type\"])\n\n# Structured logging (for Loki)\nlogger = logging.getLogger(__name__)\n\n# Logging hook\nasync def log_hook(call_input, call_output, context):\n    logger.info(\"llm_call\", extra={\n        \"call_id\": context.call_id,\n        \"model\": call_input.model,\n        \"tokens\": call_output.usage.get(\"total_tokens\", 0) if call_output.usage else 0,\n        \"duration_ms\": context.elapsed_ms,\n        \"query\": call_input.messages[-1][\"content\"][:100],  # First 100 chars\n    })\n\n# Metrics hook\nasync def metrics_hook(result):\n    model = result.input.model\n    llm_calls_total.labels(model=model, scope=\"default\").inc()\n\n    if result.output:\n        llm_call_duration.labels(model=model).observe(result.elapsed_ms / 1000.0)\n        if result.output.usage:\n            llm_tokens_total.labels(model=model, type=\"prompt\").inc(\n                result.output.usage.get(\"prompt_tokens\", 0)\n            )\n            llm_tokens_total.labels(model=model, type=\"completion\").inc(\n                result.output.usage.get(\"completion_tokens\", 0)\n            )\n\n# Evaluation hook\nasync def eval_hook(call_input, call_output, context):\n    \"\"\"Evaluate response quality.\"\"\"\n    response_text = call_output.text.lower()\n\n    # Simple checks\n    has_location = any(word in response_text for word in [\"hotel\", \"flight\", \"restaurant\", \"attraction\"])\n    has_action = any(word in response_text for word in [\"book\", \"reserve\", \"find\", \"recommend\"])\n\n    # Store evaluation in metadata\n    context.metadata[\"eval_score\"] = 1.0 if (has_location and has_action) else 0.5\n    context.metadata[\"eval_has_location\"] = has_location\n    context.metadata[\"eval_has_action\"] = has_action\n\n    logger.info(\"evaluation\", extra={\n        \"call_id\": context.call_id,\n        \"score\": context.metadata[\"eval_score\"],\n        \"has_location\": has_location,\n        \"has_action\": has_action,\n    })\n\n# Register hooks\nhookedllm.scope(\"default\").after(log_hook)\nhookedllm.scope(\"default\").after(eval_hook)\nhookedllm.finally_(metrics_hook)\n</code></pre>"},{"location":"examples/travel-assistant/#complete-setup","title":"Complete Setup","text":"<pre><code># main.py\nimport logging\nfrom prometheus_client import start_http_server\nimport asyncio\nfrom app import travel_assistant\n\n# Configure logging for Loki\nlogging.basicConfig(\n    level=logging.INFO,\n    format='{\"timestamp\": \"%(asctime)s\", \"level\": \"%(levelname)s\", \"message\": \"%(message)s\", %(extra)s}',\n    handlers=[\n        logging.StreamHandler(),  # Or use Loki handler\n    ]\n)\n\n# Start Prometheus metrics server\nstart_http_server(8000)  # Metrics available at http://localhost:8000/metrics\n\n# Import hooks to register them\nimport hooks\n\nasync def main():\n    result = await travel_assistant(\"Find hotels in Paris\")\n    print(result)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/travel-assistant/#docker-compose-optional","title":"Docker Compose (Optional)","text":"<pre><code># docker-compose.yml\nversion: '3'\nservices:\n  prometheus:\n    image: prom/prometheus\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n\n  grafana:\n    image: grafana/grafana\n    ports:\n      - \"3000:3000\"\n    environment:\n      - GF_SECURITY_ADMIN_PASSWORD=admin\n\n  loki:\n    image: grafana/loki\n    ports:\n      - \"3100:3100\"\n\n  app:\n    build: .\n    ports:\n      - \"8000:8000\"  # Prometheus metrics\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n</code></pre>"},{"location":"examples/travel-assistant/#grafana-dashboards","title":"Grafana Dashboards","text":"<p>Metrics Dashboard (Prometheus): - <code>llm_calls_total</code> - Total calls over time - <code>llm_call_duration_seconds</code> - P50, P95, P99 latencies - <code>llm_tokens_total</code> - Token usage by type</p> <p>Logs Dashboard (Loki): - Query: <code>{job=\"travel-assistant\"} |= \"llm_call\"</code> - Filter by model, duration, tokens</p> <p>Evaluation Dashboard: - Query: <code>{job=\"travel-assistant\"} |= \"evaluation\"</code> - Track eval scores over time</p>"},{"location":"examples/travel-assistant/#usage","title":"Usage","text":"<pre><code># Run app\npython main.py\n\n# View metrics\ncurl http://localhost:8000/metrics\n\n# Query logs (Loki)\ncurl -G \"http://localhost:3100/loki/api/v1/query_range\" \\\n  --data-urlencode 'query={job=\"travel-assistant\"}' \\\n  --data-urlencode 'start=1h'\n</code></pre> <p>This setup provides: - \u2705 Logging: All LLM calls logged to Loki - \u2705 Metrics: Prometheus metrics for monitoring - \u2705 Evaluation: Response quality checks - \u2705 Observability: Full visibility into LLM usage</p>"},{"location":"guides/architecture/","title":"Architecture Guide","text":"<p>HookedLLM follows SOLID principles with full dependency injection support.</p>"},{"location":"guides/architecture/#design-principles","title":"Design Principles","text":"<ul> <li>SRP: Each component has a single responsibility</li> <li>OCP: Open for extension, closed for modification</li> <li>LSP: Implementations are interchangeable</li> <li>ISP: Focused, minimal interfaces</li> <li>DIP: Depend on abstractions, not concretions</li> </ul>"},{"location":"guides/architecture/#component-overview","title":"Component Overview","text":"<pre><code>HookedLLMContext (DI Container)\n\u251c\u2500\u2500 Registry (Protocol) \u2192 InMemoryRegistry\n\u2514\u2500\u2500 Executor (Protocol) \u2192 DefaultExecutor\n</code></pre>"},{"location":"guides/architecture/#execution-model","title":"Execution Model","text":"<p>Hooks execute sequentially (not in parallel). Each hook is awaited before the next starts.</p> <p>Async benefits: Non-blocking I/O, concurrent LLM calls, efficient resource usage.</p>"},{"location":"guides/architecture/#scope-isolation","title":"Scope Isolation","text":"<ul> <li>Each scope maintains its own hook list</li> <li>Clients opt into scopes</li> <li>Global hooks run for all clients</li> <li>Scopes created on-demand</li> </ul>"},{"location":"guides/architecture/#error-handling","title":"Error Handling","text":"<p>Hook failures never break LLM calls. Exceptions are caught and logged.</p>"},{"location":"guides/architecture/#thread-safety","title":"Thread Safety","text":"<ul> <li>Registry: NOT thread-safe - register hooks at startup</li> <li>Hook execution: Safe for concurrent calls</li> <li>Each call has its own context</li> </ul> <p>See lifecycle.md for details.</p>"},{"location":"guides/architecture/#extension-points","title":"Extension Points","text":"<ul> <li>Custom executors (error handling, logging)</li> <li>Custom registries (persistent storage, thread-safe)</li> <li>Custom rules (conditional execution)</li> </ul> <p>See dependency-injection.md for customization.</p>"},{"location":"guides/dependency-injection/","title":"Dependency Injection Guide","text":"<p>HookedLLM uses dependency injection for testability and customization.</p>"},{"location":"guides/dependency-injection/#basic-usage","title":"Basic Usage","text":"<p>Default context uses sensible defaults:</p> <pre><code>import hookedllm\n\nclient = hookedllm.wrap(AsyncOpenAI())\n</code></pre>"},{"location":"guides/dependency-injection/#custom-context","title":"Custom Context","text":"<p>Create a custom context with injected dependencies:</p> <pre><code>from hookedllm.core import InMemoryScopeRegistry, DefaultHookExecutor\n\nregistry = InMemoryScopeRegistry()\nexecutor = DefaultHookExecutor()\n\nctx = hookedllm.create_context(\n    registry=registry,\n    executor=executor\n)\n\nclient = ctx.wrap(AsyncOpenAI())\n</code></pre>"},{"location":"guides/dependency-injection/#custom-executor","title":"Custom Executor","text":"<p>Customize error handling and logging:</p> <pre><code>from hookedllm.core import DefaultHookExecutor\nimport logging\n\ndef my_error_handler(error, context):\n    logger.error(f\"Hook failed: {error}\")\n\nexecutor = DefaultHookExecutor(\n    error_handler=my_error_handler,\n    logger=logging.getLogger(\"myapp\")\n)\n\nctx = hookedllm.create_context(executor=executor)\n</code></pre>"},{"location":"guides/dependency-injection/#testing-with-mocks","title":"Testing with Mocks","text":"<pre><code>from unittest.mock import Mock\n\nmock_executor = Mock(spec=hookedllm.HookExecutor)\nctx = hookedllm.create_context(executor=mock_executor)\n\nctx.scope(\"test\").after(my_hook)\nclient = ctx.wrap(FakeClient(), scope=\"test\")\n\n# Assert\nassert mock_executor.execute_after.called\n</code></pre> <p>See testing.md for more patterns.</p>"},{"location":"guides/error-handling/","title":"Error Handling Guide","text":"<p>Error handling patterns for HookedLLM.</p>"},{"location":"guides/error-handling/#error-model","title":"Error Model","text":"<p>Hook failures never break your LLM calls. Each hook's errors are isolated.</p> <pre><code>async def buggy_hook(call_input, call_output, context):\n    raise ValueError(\"Hook failed!\")\n\nhookedllm.after(buggy_hook)\n\n# The LLM call still succeeds even if the hook fails\nresponse = await client.chat.completions.create(...)\n</code></pre>"},{"location":"guides/error-handling/#error-handling-patterns","title":"Error Handling Patterns","text":""},{"location":"guides/error-handling/#error-logging","title":"Error Logging","text":"<p>Log errors with context:</p> <pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\nasync def error_handler(call_input, error, context):\n    logger.error(\n        \"llm_call_failed\",\n        extra={\n            \"call_id\": context.call_id,\n            \"model\": call_input.model,\n            \"error_type\": type(error).__name__,\n            \"error_message\": str(error),\n        },\n        exc_info=True,\n    )\n\nhookedllm.error(error_handler)\n</code></pre>"},{"location":"guides/error-handling/#error-classification","title":"Error Classification","text":"<p>Handle different error types:</p> <pre><code>async def error_handler(call_input, error, context):\n    error_type = type(error).__name__\n\n    if error_type == \"RateLimitError\":\n        logger.warning(f\"Rate limit: {context.call_id}\")\n        await schedule_retry(context.call_id, backoff=60)\n    elif error_type == \"AuthenticationError\":\n        logger.critical(f\"Auth error: {context.call_id}\")\n        await send_alert(\"Authentication failed\")\n</code></pre>"},{"location":"guides/error-handling/#rule-evaluation-errors","title":"Rule Evaluation Errors","text":"<p>Rules that fail are skipped (hook doesn't execute):</p> <pre><code># Rule that might fail\ndef safe_rule(call_input, ctx):\n    try:\n        return ctx.metadata.get(\"key\") == \"value\"\n    except Exception:\n        return False  # Don't match on error\n\nhookedllm.after(hook, when=safe_rule)\n</code></pre> <p>See examples/advanced.md for more patterns.</p>"},{"location":"guides/lifecycle/","title":"Lifecycle &amp; Concurrency Guide","text":"<p>Hook registration, memory management, and thread safety.</p>"},{"location":"guides/lifecycle/#hook-registration","title":"Hook Registration","text":"<p>Register hooks at application startup (before concurrent access):</p> <pre><code># app/startup.py\nimport hookedllm\n\ndef initialize_hooks():\n    hookedllm.scope(\"default\").after(default_logger)\n    hookedllm.scope(\"default\").error(error_handler)\n    hookedllm.finally_(metrics_collector)\n\ninitialize_hooks()  # Call during app startup\n</code></pre> <p>Thread Safety: The default registry is NOT thread-safe. Register hooks before concurrent access.</p>"},{"location":"guides/lifecycle/#hook-unregistration","title":"Hook Unregistration","text":"<p>HookedLLM doesn't provide built-in hook unregistration. Use conditional execution or separate contexts:</p> <pre><code># Conditional execution\nENABLE_EVALUATION = os.getenv(\"ENABLE_EVALUATION\", \"false\") == \"true\"\nif ENABLE_EVALUATION:\n    hookedllm.scope(\"evaluation\").after(evaluator)\n\n# Or use separate contexts\nevaluation_ctx = hookedllm.create_context()\ndefault_ctx = hookedllm.create_context()\n</code></pre>"},{"location":"guides/lifecycle/#concurrency","title":"Concurrency","text":"<p>Hook execution is safe for concurrent LLM calls - each call has its own context.</p> <pre><code># \u2705 Safe: Multiple concurrent calls\nasync def make_call(client):\n    return await client.chat.completions.create(...)\n\nawait asyncio.gather(\n    make_call(client1),\n    make_call(client2),\n    make_call(client3),\n)\n</code></pre> <p>Shared state: Use thread-safe structures if hooks access shared data:</p> <pre><code>import asyncio\n\ncounter = asyncio.Lock()\ncount = 0\n\nasync def safe_hook(call_input, call_output, context):\n    async with counter:\n        global count\n        count += 1\n</code></pre>"},{"location":"guides/lifecycle/#memory-management","title":"Memory Management","text":"<p>Best practices: - Don't store large objects in hooks or contexts - Use bounded storage for accumulating data - Clean up resources in finally hooks</p> <pre><code># \u2705 Good: Bounded storage\nfrom collections import deque\ncall_history = deque(maxlen=1000)\n\nasync def hook(result):\n    call_history.append(result)  # Bounded size\n</code></pre>"},{"location":"guides/lifecycle/#scope-lifecycle","title":"Scope Lifecycle","text":"<p>Scopes are created on-demand and stored in memory (don't persist across restarts).</p> <pre><code># Scope created when first accessed\nhookedllm.scope(\"default\").after(hook)\n</code></pre> <p>See examples/advanced.md for more patterns.</p>"},{"location":"guides/observability/","title":"Observability Guide","text":"<p>Integrating HookedLLM with monitoring, logging, and tracing tools.</p>"},{"location":"guides/observability/#structured-logging","title":"Structured Logging","text":"<p>Use structured logging instead of print statements:</p> <pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\nasync def logger_hook(call_input, call_output, context):\n    logger.info(\"llm_call_completed\", extra={\n        \"call_id\": context.call_id,\n        \"model\": call_input.model,\n        \"tokens\": call_output.usage.get(\"total_tokens\", 0),\n        \"duration_ms\": context.elapsed_ms,\n    })\n\nhookedllm.after(logger_hook)\n</code></pre>"},{"location":"guides/observability/#metrics-collection","title":"Metrics Collection","text":""},{"location":"guides/observability/#prometheus","title":"Prometheus","text":"<pre><code>from prometheus_client import Counter, Histogram\n\nllm_calls_total = Counter(\"llm_calls_total\", \"Total LLM calls\", [\"model\"])\nllm_call_duration = Histogram(\"llm_call_duration_seconds\", \"LLM call duration\", [\"model\"])\n\nasync def metrics_hook(result):\n    model = result.input.model\n    llm_calls_total.labels(model=model).inc()\n    if result.output:\n        llm_call_duration.labels(model=model).observe(result.elapsed_ms / 1000.0)\n\nhookedllm.finally_(metrics_hook)\n</code></pre>"},{"location":"guides/observability/#distributed-tracing","title":"Distributed Tracing","text":""},{"location":"guides/observability/#opentelemetry","title":"OpenTelemetry","text":"<pre><code>from opentelemetry import trace\n\ntracer = trace.get_tracer(__name__)\n\nasync def tracing_hook(call_input, call_output, context):\n    with tracer.start_as_current_span(\"llm_call\") as span:\n        span.set_attribute(\"llm.model\", call_input.model)\n        span.set_attribute(\"llm.tokens\", call_output.usage.get(\"total_tokens\", 0))\n        span.set_attribute(\"llm.duration_ms\", context.elapsed_ms)\n\nhookedllm.after(tracing_hook)\n</code></pre>"},{"location":"guides/observability/#error-tracking","title":"Error Tracking","text":""},{"location":"guides/observability/#sentry","title":"Sentry","text":"<pre><code>import sentry_sdk\n\nasync def error_tracking_hook(call_input, error, context):\n    sentry_sdk.capture_exception(\n        error,\n        contexts={\n            \"llm\": {\n                \"call_id\": context.call_id,\n                \"model\": call_input.model,\n            }\n        }\n    )\n\nhookedllm.error(error_tracking_hook)\n</code></pre> <p>See examples/advanced.md and advanced-setup.md for complete examples.</p>"},{"location":"guides/performance/","title":"Performance Guide","text":"<p>Performance characteristics and optimization strategies for HookedLLM.</p>"},{"location":"guides/performance/#execution-model","title":"Execution Model","text":"<p>Hooks execute sequentially (not in parallel). Each hook's execution time adds to total latency.</p> <pre><code>hookedllm.before(hook1)  # Runs first\nhookedllm.before(hook2)  # Runs second (waits for hook1)\n</code></pre> <p>Total latency = LLM call time + sum of all hook execution times</p> <p>Async benefits: non-blocking I/O, concurrent LLM calls, efficient resource usage.</p>"},{"location":"guides/performance/#performance-overhead","title":"Performance Overhead","text":"<ul> <li>Wrapper overhead: ~0.1ms per call</li> <li>Scope lookup: ~0.05ms per scope</li> <li>Rule evaluation: ~0.01-0.1ms per rule</li> </ul>"},{"location":"guides/performance/#rule-performance","title":"Rule Performance","text":"<p>Rules are evaluated synchronously before hook execution. They are not cached - evaluated for every call.</p> <pre><code># \u2705 Fast: Simple rule\nhookedllm.after(hook, when=hookedllm.when.model(\"gpt-4\"))\n\n# \u26a0\ufe0f Moderate: Composed rule\nhookedllm.after(\n    hook,\n    when=hookedllm.when.model(\"gpt-4\") &amp; hookedllm.when.tag(\"monitoring\")\n)\n\n# \u274c Slow: Complex lambda\nhookedllm.after(\n    hook,\n    when=lambda call_input, ctx: expensive_computation(ctx)\n)\n</code></pre> <p>Best practices: - Keep rules simple - Use built-in rules when possible - Cache expensive computations in rule functions - Handle rule errors (failed rules skip the hook)</p> <pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=1000)\ndef expensive_check(model: str, tier: str) -&gt; bool:\n    return complex_check(model, tier)\n\nhookedllm.after(\n    hook,\n    when=lambda call_input, ctx: expensive_check(\n        call_input.model,\n        ctx.metadata.get(\"tier\", \"\")\n    )\n)\n</code></pre>"},{"location":"guides/performance/#optimization-strategies","title":"Optimization Strategies","text":""},{"location":"guides/performance/#keep-hooks-fast","title":"Keep Hooks Fast","text":"<p>Hooks should complete quickly (&lt;10ms ideally):</p> <pre><code># \u2705 Good: Fast, non-blocking\nasync def fast_logging_hook(call_input, call_output, context):\n    logger.info(\"llm_call\", extra={\n        \"call_id\": context.call_id,\n        \"model\": call_input.model,\n    })\n\n# \u274c Bad: Slow, blocking\nasync def slow_hook(call_input, call_output, context):\n    await database.save_call_details(call_input, call_output)\n    await analytics.track(call_output)\n</code></pre>"},{"location":"guides/performance/#use-background-tasks","title":"Use Background Tasks","text":"<p>Offload heavy operations:</p> <pre><code>import asyncio\nfrom queue import Queue\n\nbackground_queue = Queue()\n\nasync def metrics_hook(result):\n    \"\"\"Fast hook that queues work for background processing.\"\"\"\n    background_queue.put({\n        \"call_id\": result.context.call_id,\n        \"tokens\": result.output.usage.get(\"total_tokens\", 0),\n    })\n\n# Background worker processes the queue\nasync def background_worker():\n    while True:\n        item = await background_queue.get()\n        await process_metrics(item)  # Slow operation, doesn't block calls\n</code></pre>"},{"location":"guides/performance/#minimize-hook-count","title":"Minimize Hook Count","text":"<p>Fewer hooks = lower latency:</p> <pre><code># \u274c Bad: Many small hooks\nhookedllm.after(log_model)\nhookedllm.after(log_tokens)\nhookedllm.after(log_duration)\n\n# \u2705 Good: One comprehensive hook\nasync def comprehensive_logging(call_input, call_output, context):\n    logger.info(\"llm_call\", extra={\n        \"model\": call_input.model,\n        \"tokens\": call_output.usage.get(\"total_tokens\", 0),\n        \"duration_ms\": context.elapsed_ms,\n    })\n</code></pre>"},{"location":"guides/performance/#use-scopes","title":"Use Scopes","text":"<p>Scopes prevent hooks from running when not needed:</p> <pre><code># Evaluation hooks only run for evaluation clients\nhookedllm.scope(\"evaluation\").after(expensive_evaluation)\n\n# Default scope hooks only run for default clients\nhookedllm.scope(\"default\").after(default_logging)\n</code></pre>"},{"location":"guides/performance/#monitoring","title":"Monitoring","text":"<p>Track hook execution time:</p> <pre><code>import time\n\nasync def monitored_hook(call_input, call_output, context):\n    start = time.perf_counter()\n    try:\n        # Your hook logic here\n        await do_work(call_input, call_output, context)\n    finally:\n        duration = (time.perf_counter() - start) * 1000\n        if duration &gt; 10:\n            logger.warning(f\"Slow hook: {duration:.2f}ms\")\n</code></pre>"},{"location":"guides/performance/#performance-targets","title":"Performance Targets","text":"<ul> <li>Before hooks: &lt;5ms total</li> <li>After hooks: &lt;10ms total</li> <li>Finally hooks: &lt;5ms total</li> <li>Rule evaluation: &lt;0.1ms per rule</li> </ul> <p>See examples/advanced.md for more patterns.</p>"},{"location":"guides/testing/","title":"Testing Guide","text":"<p>HookedLLM is fully testable through dependency injection.</p>"},{"location":"guides/testing/#testing-hooks","title":"Testing Hooks","text":"<p>Test hooks in isolation:</p> <pre><code>import pytest\nfrom hookedllm.core import CallInput, CallOutput, CallContext\n\nasync def test_log_hook():\n    call_input = CallInput(model=\"gpt-4\", messages=[{\"role\": \"user\", \"content\": \"Hello\"}])\n    call_output = CallOutput(text=\"Hi there!\")\n    context = CallContext()\n\n    await log_hook(call_input, call_output, context)\n\n    assert context.metadata.get(\"logged\") == True\n</code></pre>"},{"location":"guides/testing/#testing-hook-execution","title":"Testing Hook Execution","text":"<p>Use mocks to test execution:</p> <pre><code>from unittest.mock import Mock, AsyncMock\n\nmock_executor = Mock(spec=hookedllm.HookExecutor)\nmock_executor.execute_after = AsyncMock()\n\nctx = hookedllm.create_context(executor=mock_executor)\nctx.scope(\"test\").after(my_hook)\nclient = ctx.wrap(FakeClient(), scope=\"test\")\n\nawait client.chat.completions.create(...)\n\nassert mock_executor.execute_after.called\n</code></pre>"},{"location":"guides/testing/#testing-rules","title":"Testing Rules","text":"<p>Test conditional rules:</p> <pre><code>hookedllm.after(\n    my_hook,\n    when=hookedllm.when.model(\"gpt-4\")\n)\n\n# Test with matching/non-matching models\n</code></pre>"},{"location":"guides/testing/#testing-error-handling","title":"Testing Error Handling","text":"<pre><code>error_triggered = False\n\nasync def error_hook(call_input, error, context):\n    nonlocal error_triggered\n    error_triggered = True\n\nhookedllm.error(error_hook)\nclient = hookedllm.wrap(FailingClient())\n\nwith pytest.raises(Exception):\n    await client.chat.completions.create(...)\n\nassert error_triggered\n</code></pre> <p>See dependency-injection.md for DI patterns.</p>"}]}